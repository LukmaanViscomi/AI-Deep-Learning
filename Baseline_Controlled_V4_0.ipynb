{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "1Zd_m3UEZoTZJxdjc2dTaimGuiL066QAf",
      "authorship_tag": "ABX9TyPYZ2l1NVs8nZePU2ZHz8iz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LukmaanViscomi/AI-Deep-Learning/blob/main/Baseline_Controlled_V4_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 1: Setup and Dependencies"
      ],
      "metadata": {
        "id": "gyd77wiWl4e7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Section 1: Setup and Dependencies\n",
        "!apt-get install p7zip-full # Install 7-Zip\n",
        "!pip install patool # Install the patool library which provides the patoolib module\n",
        "import zipfile\n",
        "import os\n",
        "import patoolib # Now you can import patoolib\n",
        "\n",
        "# Path to the uploaded zip file\n",
        "zip_file_path = 'dataset2 (1).zip'\n",
        "extracted_folder_path = './dataset2'  # Use a relative path for the extraction directory\n",
        "\n",
        "# Extract the zip file using patool\n",
        "patoolib.extract_archive(zip_file_path, outdir=extracted_folder_path)\n",
        "\n",
        "# List the contents of the extracted folder\n",
        "extracted_files = os.listdir(extracted_folder_path)\n",
        "print(extracted_files)\n",
        "\n"
      ],
      "metadata": {
        "id": "tZMl4NRlv_ds",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46ffae3c-9fd1-4be3-d20e-a881b4cb6a60"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "p7zip-full is already the newest version (16.02+dfsg-8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "Requirement already satisfied: patool in /usr/local/lib/python3.10/dist-packages (2.3.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO patool: Extracting dataset2 (1).zip ...\n",
            "INFO:patool:Extracting dataset2 (1).zip ...\n",
            "INFO patool: ... creating output directory `./dataset2'.\n",
            "INFO:patool:... creating output directory `./dataset2'.\n",
            "INFO patool: running /usr/bin/7z x -o./dataset2 -- \"dataset2 (1).zip\"\n",
            "INFO:patool:running /usr/bin/7z x -o./dataset2 -- \"dataset2 (1).zip\"\n",
            "INFO patool:     with input=''\n",
            "INFO:patool:    with input=''\n",
            "INFO patool: ... dataset2 (1).zip extracted to `./dataset2'.\n",
            "INFO:patool:... dataset2 (1).zip extracted to `./dataset2'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['triple_mnist']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 2 : Inspecting the Original Dataset"
      ],
      "metadata": {
        "id": "63eY2Qdwl9Ue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Section 2 : Inspecting the Original Dataset\n",
        "import os\n",
        "\n",
        "# Path to the triple_mnist directory\n",
        "base_dir = 'dataset2/triple_mnist'  # Replace with the actual path to the triple_mnist folder\n",
        "\n",
        "# Folders to check\n",
        "folders = ['train', 'test', 'val']\n",
        "\n",
        "# Function to count classes and images in a given directory\n",
        "def count_classes_and_images(directory):\n",
        "    class_count = 0\n",
        "    total_images = 0\n",
        "\n",
        "    for class_name in os.listdir(directory):\n",
        "        class_dir = os.path.join(directory, class_name)\n",
        "        if os.path.isdir(class_dir):\n",
        "            class_count += 1\n",
        "            image_count = len(os.listdir(class_dir))\n",
        "            total_images += image_count\n",
        "\n",
        "    return class_count, total_images\n",
        "\n",
        "# Iterate through each folder (train, test, val) and print the summary\n",
        "for folder in folders:\n",
        "    folder_path = os.path.join(base_dir, folder)\n",
        "    class_count, total_images = count_classes_and_images(folder_path)\n",
        "    print(f\"{folder.capitalize()} folder: {class_count} classes, {total_images} images\")\n"
      ],
      "metadata": {
        "id": "zpjwjGbSAIvI",
        "outputId": "6851393e-cb5f-4ff7-a455-648c7664b2cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train folder: 640 classes, 64000 images\n",
            "Test folder: 200 classes, 20000 images\n",
            "Val folder: 160 classes, 16000 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 3:  Paths to Your Full Dataset"
      ],
      "metadata": {
        "id": "_sxrV-d3mBuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Section 3: Paths to Your Full Dataset\n",
        "from pathlib import Path\n",
        "\n",
        "# Paths to original directories\n",
        "full_train_dir = Path('dataset2/triple_mnist/train')\n",
        "full_val_dir = Path('dataset2/triple_mnist/val')\n",
        "full_test_dir = Path('dataset2/triple_mnist/test')\n",
        "\n",
        "# Paths to subset directories\n",
        "subset_base_dir = Path('subset/triple_mnist')\n",
        "subset_train_dir = subset_base_dir / 'train'\n",
        "subset_val_dir = subset_base_dir / 'val'\n",
        "subset_test_dir = subset_base_dir / 'test'\n",
        "\n",
        "# Ensure subset directories exist\n",
        "subset_train_dir.mkdir(parents=True, exist_ok=True)\n",
        "subset_val_dir.mkdir(parents=True, exist_ok=True)\n",
        "subset_test_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Paths set for full and subset datasets.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "w5-981_hQSDN",
        "outputId": "8687d6c7-9a43-49b3-98c1-8d6797dd8e42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paths set for full and subset datasets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 4: Create a Small Subset of Your Data"
      ],
      "metadata": {
        "id": "E2bruFbVmcWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Section 4: Create a Small Subset of Your Data\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Function to create a subset of the dataset\n",
        "def create_subset(original_dir, subset_dir, classes, num_images_per_class):\n",
        "    if not os.path.exists(subset_dir):\n",
        "        os.makedirs(subset_dir)\n",
        "    for class_name in classes:\n",
        "        class_dir = os.path.join(original_dir, class_name)\n",
        "        subset_class_dir = os.path.join(subset_dir, class_name)\n",
        "        if not os.path.exists(subset_class_dir):\n",
        "            os.makedirs(subset_class_dir)\n",
        "        images = os.listdir(class_dir)\n",
        "        random.shuffle(images)\n",
        "        for img in images[:num_images_per_class]:\n",
        "            shutil.copy(os.path.join(class_dir, img), os.path.join(subset_class_dir, img))\n",
        "\n",
        "# Get the intersection of class names in train, val, and test directories\n",
        "train_classes = set(os.listdir(full_train_dir))\n",
        "val_classes = set(os.listdir(full_val_dir))\n",
        "test_classes = set(os.listdir(full_test_dir))\n",
        "\n",
        "# Since classes are unique, we'll pick a fixed number from each split\n",
        "classes_to_use_train = sorted(train_classes)[:50]  # First 50 classes from train\n",
        "classes_to_use_val = sorted(val_classes)[:50]  # First 50 classes from val\n",
        "classes_to_use_test = sorted(test_classes)[:50]  # First 50 classes from test\n",
        "\n",
        "num_images_per_class = 500  # Increase to 500 images per class\n",
        "\n",
        "# Create subsets independently for train, val, and test\n",
        "create_subset(\n",
        "    original_dir=full_train_dir,\n",
        "    subset_dir=subset_train_dir,\n",
        "    classes=classes_to_use_train,\n",
        "    num_images_per_class=num_images_per_class\n",
        ")\n",
        "\n",
        "create_subset(\n",
        "    original_dir=full_val_dir,\n",
        "    subset_dir=subset_val_dir,\n",
        "    classes=classes_to_use_val,\n",
        "    num_images_per_class=num_images_per_class\n",
        ")\n",
        "\n",
        "create_subset(\n",
        "    original_dir=full_test_dir,\n",
        "    subset_dir=subset_test_dir,\n",
        "    classes=classes_to_use_test,\n",
        "    num_images_per_class=num_images_per_class\n",
        ")\n",
        "\n",
        "print(\"Subset creation complete.\")\n"
      ],
      "metadata": {
        "id": "p98rbJA7CNn-",
        "outputId": "98a9b598-504b-4d20-9361-b68462fbc60d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset creation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 5: Imports and Initial Setup"
      ],
      "metadata": {
        "id": "EOQOrkSem5lO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Section 5: Imports and Initial Setup\n",
        "!pip install keras-tuner\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import keras_tuner as kt\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "-PIvGymr-WP_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "108d4346-68e9-4e05-fe83-5aaabcca8aa7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (3.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.32.3)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (3.11.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.12.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.7.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras->keras-tuner) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n",
            "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 6: Image Data Generators for the Subset (Updated for Data Augmentation)\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Dzn5PI6r4vt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 6: Image data generators for the subset (Updated for More Aggressive Data Augmentation)\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.3,\n",
        "    height_shift_range=0.3,\n",
        "    shear_range=0.3,\n",
        "    zoom_range=0.3,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,  # Added vertical flip\n",
        "    brightness_range=[0.8, 1.2],  # Added brightness adjustment\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n"
      ],
      "metadata": {
        "id": "ha5W2LD0fqc9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "old version"
      ],
      "metadata": {
        "id": "08pOywbJ9b4y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 7: Data Generators for the Subset\n"
      ],
      "metadata": {
        "id": "U2K6AfJW41iK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Section 7: Data Generators for the Subset\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    subset_train_dir,\n",
        "    target_size=(84, 84),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    subset_val_dir,\n",
        "    target_size=(84, 84),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    subset_test_dir,\n",
        "    target_size=(84, 84),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2GT0Rly5Hsf",
        "outputId": "23414dee-91e6-4e14-ec58-7e2a7de834f0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5000 images belonging to 50 classes.\n",
            "Found 5000 images belonging to 50 classes.\n",
            "Found 5000 images belonging to 50 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 8: Hyperparameter Tuning Function with Batch Size, Batch Normalization, and Regularization"
      ],
      "metadata": {
        "id": "TbyY5ONznKD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Section 8: Hyperparameter Tuning Function with Batch Size, Batch Normalization, and Regularization\n",
        "from tensorflow.keras.layers import BatchNormalization  # Add this import\n",
        "\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "\n",
        "    # Print the batch size being used for this trial\n",
        "    batch_size = hp.Int('batch_size', 16, 64, step=16)\n",
        "    print(f\"Batch size for this trial: {batch_size}\")\n",
        "\n",
        "    # First convolutional layer\n",
        "    model.add(Conv2D(hp.Int('conv_1_filter', 32, 256, step=32), (3, 3), activation='relu', input_shape=(84, 84, 3)))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Second convolutional layer\n",
        "    model.add(Conv2D(hp.Int('conv_2_filter', 64, 512, step=64), (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Third convolutional layer (new)\n",
        "    model.add(Conv2D(hp.Int('conv_3_filter', 128, 512, step=64), (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # Dense layer with regularization\n",
        "    model.add(Dense(hp.Int('dense_units', 64, 512, step=64), activation='relu',\n",
        "                    kernel_regularizer=tf.keras.regularizers.l2(hp.Float('l2_regularization', 1e-4, 1e-2, sampling='LOG'))))\n",
        "    model.add(Dropout(hp.Float('dropout_rate', 0.2, 0.5, step=0.1)))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(Dense(train_generator.num_classes, activation='softmax'))\n",
        "\n",
        "    # Compile the model with a tunable learning rate\n",
        "    model.compile(optimizer=Adam(learning_rate=hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG')),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "CmnMPwiVOaZZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 9: Early Stopping Callback"
      ],
      "metadata": {
        "id": "fT97PfFFpNZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Section 9: Callback for Early Stopping (Updated for longer patience)\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,  # Increased patience for better fine-tuning\n",
        "    restore_best_weights=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "E-AGVk0VgOJ9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 10: Learning Rate Scheduler (New)"
      ],
      "metadata": {
        "id": "3rAbY1dFpjFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Section 10: Learning Rate Scheduler\n",
        "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.1,\n",
        "    patience=3,\n",
        "    min_lr=1e-6,\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "nIXAWvCUpfsE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 11: Tuner Setup (Including Batch Size Tuning)"
      ],
      "metadata": {
        "id": "czJZHV3RoJez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Section 11: Tuner Setup (Corrected with Batch Size Tuning)\n",
        "tuner = kt.Hyperband(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_epochs=10,\n",
        "    factor=3,\n",
        "    directory='new_output',\n",
        "    project_name='digit_tuning_subset'\n",
        ")\n",
        "\n",
        "# Start the hyperparameter search\n",
        "tuner.search(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[early_stopping, lr_scheduler]\n",
        ")\n",
        "\n",
        "# Log the hyperparameters, including batch size\n",
        "tuner.search_space_summary()\n"
      ],
      "metadata": {
        "id": "T1JTVQR1N9sY",
        "outputId": "3a0badb4-c6ee-4c5d-dc8f-9c6891905d59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size for this trial: 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Search: Running Trial #1\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "48                |48                |batch_size\n",
            "96                |96                |conv_1_filter\n",
            "448               |448               |conv_2_filter\n",
            "448               |448               |conv_3_filter\n",
            "128               |128               |dense_units\n",
            "0.0062534         |0.0062534         |l2_regularization\n",
            "0.2               |0.2               |dropout_rate\n",
            "0.00013322        |0.00013322        |learning_rate\n",
            "2                 |2                 |tuner/epochs\n",
            "0                 |0                 |tuner/initial_epoch\n",
            "2                 |2                 |tuner/bracket\n",
            "0                 |0                 |tuner/round\n",
            "\n",
            "Batch size for this trial: 48\n",
            "Epoch 1/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1162s\u001b[0m 7s/step - accuracy: 0.0189 - loss: 6.5763 - val_accuracy: 0.0212 - val_loss: 5.4981 - learning_rate: 1.3322e-04\n",
            "Epoch 2/2\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.0135 - loss: 6.1274"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 12: Train Model with Best Hyperparameters\n",
        "\n"
      ],
      "metadata": {
        "id": "M6XmwGSz5VrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Section 12: Train Model with Best Hyperparameters\n",
        "print(\"Compiling and training the model with the best hyperparameters...\")\n",
        "\n",
        "# Build the best model from the hyperparameter search\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "best_model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Explicitly compile the model to ensure metrics are tracked\n",
        "best_model.compile(\n",
        "    optimizer=Adam(learning_rate=best_hps.get('learning_rate')),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model with the best hyperparameters, using the early stopping and learning rate scheduler callbacks\n",
        "history = best_model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[early_stopping, lr_scheduler]\n",
        ")\n",
        "\n",
        "# After training, print out the history keys to check for 'val_accuracy'\n",
        "print(\"Available keys in history.history after training:\", history.history.keys())\n",
        "\n",
        "# Evaluate the model on the subset test data\n",
        "test_loss, test_acc = best_model.evaluate(test_generator)\n",
        "print(f\"Test accuracy on subset: {test_acc}\")\n"
      ],
      "metadata": {
        "id": "AkoGDBLriJ4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 13: Cross-Validation Loop (Optional)\n"
      ],
      "metadata": {
        "id": "Uz002UT1qWNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Section 13: Cross-Validation Loop (Optional)\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "### Section 14. Cross-Validation Loop (Optional)\n",
        "kf = KFold(n_splits=5)\n",
        "cv_scores = []\n",
        "\n",
        "for train_idx, val_idx in kf.split(train_data):\n",
        "    train_data_fold = train_data[train_idx]\n",
        "    val_data_fold = val_data[val_idx]\n",
        "\n",
        "    history = best_model.fit(train_data_fold, epochs=10, validation_data=val_data_fold,\n",
        "                             callbacks=[early_stopping, lr_scheduler])\n",
        "    score = best_model.evaluate(val_data_fold)\n",
        "    cv_scores.append(score)\n",
        "\n",
        "print(\"Cross-validation scores:\", cv_scores)\n"
      ],
      "metadata": {
        "id": "7ioC8505qUaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 14: Get the Best Hyperparameters"
      ],
      "metadata": {
        "id": "VfDRHbXu5TlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Section 14: Get the Best Hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "# Print the best hyperparameters, including batch size\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(f\"Batch size: {best_hps.get('batch_size')}\")\n",
        "print(f\"conv_1_filter: {best_hps.get('conv_1_filter')}\")\n",
        "print(f\"conv_2_filter: {best_hps.get('conv_2_filter')}\")\n",
        "print(f\"conv_3_filter: {best_hps.get('conv_3_filter')}\")\n",
        "print(f\"dense_units: {best_hps.get('dense_units')}\")\n",
        "print(f\"l2_regularization: {best_hps.get('l2_regularization')}\")\n",
        "print(f\"dropout_rate: {best_hps.get('dropout_rate')}\")\n",
        "print(f\"learning_rate: {best_hps.get('learning_rate')}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "5xP27-sf5UY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 15: Save the results in a CSV file"
      ],
      "metadata": {
        "id": "wziPOH5z5c_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Section 15: Save the results in a CSV file\n",
        "results = []\n",
        "for trial in tuner.oracle.get_best_trials(num_trials=10):\n",
        "    trial_summary = {\n",
        "        'trial_number': trial.trial_id,\n",
        "        'conv_1_filter': trial.hyperparameters.get('conv_1_filter'),\n",
        "        'conv_2_filter': trial.hyperparameters.get('conv_2_filter'),\n",
        "        'dense_units': trial.hyperparameters.get('dense_units'),\n",
        "        'dropout_rate': trial.hyperparameters.get('dropout_rate'),\n",
        "        'learning_rate': trial.hyperparameters.get('learning_rate'),\n",
        "        'batch_size': trial.hyperparameters.values['batch_size'] if 'batch_size' in trial.hyperparameters.values else 32,\n",
        "        'accuracy': trial.metrics.get_last_value('accuracy'),\n",
        "        'loss': trial.metrics.get_last_value('loss'),\n",
        "        'val_accuracy': trial.metrics.get_last_value('val_accuracy'),\n",
        "        'val_loss': trial.metrics.get_last_value('val_loss'),\n",
        "        'test_accuracy': test_acc,\n",
        "        'test_loss': test_loss,\n",
        "    }\n",
        "    results.append(trial_summary)\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv('new_output/trial_results.csv', index=False)\n"
      ],
      "metadata": {
        "id": "YLKyTSwl5gKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 16: Train on Full Dataset (Updated)"
      ],
      "metadata": {
        "id": "IwgtpneX5hwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Section 16: Train on Full Dataset (Updated)\n",
        "\n",
        "# Use the best hyperparameters for the full dataset\n",
        "batch_size = best_hps.values.get('batch_size', 32)  # Properly retrieve the batch size with a default fallback\n",
        "\n",
        "train_generator_full = train_datagen.flow_from_directory(\n",
        "    full_train_dir,\n",
        "    target_size=(84, 84),\n",
        "    batch_size=batch_size,  # Use the resolved batch_size\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator_full = val_datagen.flow_from_directory(\n",
        "    full_val_dir,\n",
        "    target_size=(84, 84),\n",
        "    batch_size=batch_size,  # Use the resolved batch_size\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator_full = test_datagen.flow_from_directory(\n",
        "    full_test_dir,\n",
        "    target_size=(84, 84),\n",
        "    batch_size=batch_size,  # Use the resolved batch_size\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Build and train the model with the best hyperparameters on the full dataset\n",
        "best_model_full = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Modify the final layer to match the number of classes in the full dataset\n",
        "best_model_full.pop()  # Remove the old final layer\n",
        "best_model_full.add(Dense(train_generator_full.num_classes, activation='softmax'))  # Add a new layer with correct output size\n",
        "\n",
        "best_model_full.compile(optimizer=Adam(learning_rate=best_hps.get('learning_rate')),\n",
        "                        loss='categorical_crossentropy',\n",
        "                        metrics=['accuracy'])\n",
        "\n",
        "history_full = best_model_full.fit(\n",
        "    train_generator_full,\n",
        "    epochs=20,\n",
        "    validation_data=val_generator_full,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluate the model on the full test data\n",
        "test_loss_full, test_acc_full = best_model_full.evaluate(test_generator_full)\n",
        "print(f\"Test accuracy on full dataset: {test_acc_full}\")\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_full.history['accuracy'], label='accuracy')\n",
        "plt.plot(history_full.history['val_accuracy'], label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_full.history['loss'], label='loss')\n",
        "plt.plot(history_full.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Displaying the winning hyperparameters\n",
        "best_hyperparameters_df = pd.DataFrame([{\n",
        "    'conv_1_filter': best_hps.get('conv_1_filter'),\n",
        "    'conv_2_filter': best_hps.get('conv_2_filter'),\n",
        "    'dense_units': best_hps.get('dense_units'),\n",
        "    'dropout_rate': best_hps.get('dropout_rate'),\n",
        "    'learning_rate': best_hps.get('learning_rate'),\n",
        "    'batch_size': batch_size  # Use the resolved batch size\n",
        "}])\n",
        "print(best_hyperparameters_df)\n",
        "best_hyperparameters_df.to_csv('new_output/best_hyperparameters.csv', index=False)\n"
      ],
      "metadata": {
        "id": "jpgW1EJ2tdLj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}