{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1j7fS6SGPeaB8uf2Nyp2jnW9GYcszfKq8",
      "authorship_tag": "ABX9TyPbpQATQkdsqPpxjvx42YiK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LukmaanViscomi/AI-Deep-Learning/blob/main/_Baseline_Controlled_V3_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PHASE-1 - UNZIP THE DATA"
      ],
      "metadata": {
        "id": "8jBACT8WZpU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install p7zip-full # Install 7-Zip\n",
        "!pip install patool # Install the patool library which provides the patoolib module\n",
        "import zipfile\n",
        "import os\n",
        "import patoolib # Now you can import patoolib\n",
        "\n",
        "# Path to the uploaded zip file\n",
        "zip_file_path = 'dataset2 (1).zip'\n",
        "extracted_folder_path = './dataset2'  # Use a relative path for the extraction directory\n",
        "\n",
        "# Extract the zip file using patool\n",
        "patoolib.extract_archive(zip_file_path, outdir=extracted_folder_path)\n",
        "\n",
        "# List the contents of the extracted folder\n",
        "extracted_files = os.listdir(extracted_folder_path)\n",
        "print(extracted_files)"
      ],
      "metadata": {
        "id": "tZMl4NRlv_ds",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beccec72-5cb8-46e4-efbc-3c0f0cec5842"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "p7zip-full is already the newest version (16.02+dfsg-8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "Collecting patool\n",
            "  Downloading patool-2.3.0-py2.py3-none-any.whl.metadata (4.0 kB)\n",
            "Downloading patool-2.3.0-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: patool\n",
            "Successfully installed patool-2.3.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO patool: Extracting dataset2 (1).zip ...\n",
            "INFO:patool:Extracting dataset2 (1).zip ...\n",
            "INFO patool: ... creating output directory `./dataset2'.\n",
            "INFO:patool:... creating output directory `./dataset2'.\n",
            "INFO patool: running /usr/bin/7z x -o./dataset2 -- \"dataset2 (1).zip\"\n",
            "INFO:patool:running /usr/bin/7z x -o./dataset2 -- \"dataset2 (1).zip\"\n",
            "INFO patool:     with input=''\n",
            "INFO:patool:    with input=''\n",
            "INFO patool: ... dataset2 (1).zip extracted to `./dataset2'.\n",
            "INFO:patool:... dataset2 (1).zip extracted to `./dataset2'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['triple_mnist']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PHASE-2 - REDISTRIBUTE THE DATASET FOR TRAIN-VAL-TEST"
      ],
      "metadata": {
        "id": "IebDDjncZ1E3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import random\n",
        "\n",
        "# Paths to original directories\n",
        "original_base_dir = Path('dataset2/triple_mnist')\n",
        "original_train_dir = original_base_dir / 'train'\n",
        "original_val_dir = original_base_dir / 'val'\n",
        "original_test_dir = original_base_dir / 'test'\n",
        "\n",
        "# Path to the new dataset directory\n",
        "new_base_dir = Path('dataset-c/triple_mnist')\n",
        "new_train_dir = new_base_dir / 'train'\n",
        "new_val_dir = new_base_dir / 'val'\n",
        "new_test_dir = new_base_dir / 'test'\n",
        "\n",
        "# Ensure the new directories exist\n",
        "new_train_dir.mkdir(parents=True, exist_ok=True)\n",
        "new_val_dir.mkdir(parents=True, exist_ok=True)\n",
        "new_test_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Function to split and copy files\n",
        "def split_and_copy_files(src_dir, new_train_dir, new_val_dir, new_test_dir, train_ratio=0.6, val_ratio=0.2, test_ratio=0.2):\n",
        "    if not src_dir.exists():\n",
        "        return\n",
        "\n",
        "    classes = sorted(os.listdir(src_dir))\n",
        "\n",
        "    for cls in classes:\n",
        "        cls_path = src_dir / cls\n",
        "        if cls_path.is_dir():\n",
        "            images = list(cls_path.glob('*'))\n",
        "            random.shuffle(images)\n",
        "\n",
        "            num_train = int(len(images) * train_ratio)\n",
        "            num_val = int(len(images) * val_ratio)\n",
        "\n",
        "            train_images = images[:num_train]\n",
        "            val_images = images[num_train:num_train+num_val]\n",
        "            test_images = images[num_train+num_val:]\n",
        "\n",
        "            cls_train_dir = new_train_dir / cls\n",
        "            cls_val_dir = new_val_dir / cls\n",
        "            cls_test_dir = new_test_dir / cls\n",
        "\n",
        "            cls_train_dir.mkdir(parents=True, exist_ok=True)\n",
        "            cls_val_dir.mkdir(parents=True, exist_ok=True)\n",
        "            cls_test_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            for img in train_images:\n",
        "                shutil.copy(str(img), str(cls_train_dir / img.name))\n",
        "            for img in val_images:\n",
        "                shutil.copy(str(img), str(cls_val_dir / img.name))\n",
        "            for img in test_images:\n",
        "                shutil.copy(str(img), str(cls_test_dir / img.name))\n",
        "\n",
        "# Split and copy files from original train, val, and test directories\n",
        "split_and_copy_files(original_train_dir, new_train_dir, new_val_dir, new_test_dir)\n",
        "split_and_copy_files(original_val_dir, new_train_dir, new_val_dir, new_test_dir)\n",
        "split_and_copy_files(original_test_dir, new_train_dir, new_val_dir, new_test_dir)\n",
        "\n",
        "print(\"Files split and copied successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFDSyQu_wCfZ",
        "outputId": "dd94ac70-4a58-4a3f-cb59-b7f8df3f6178"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files split and copied successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VERISON 3.0 - LETS GET CRACKING!!!"
      ],
      "metadata": {
        "id": "zdWEHHwb4NGM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 1: Imports and Initial Setup"
      ],
      "metadata": {
        "id": "fHciR29g-WpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import keras_tuner as kt\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PIvGymr-WP_",
        "outputId": "40f75e55-a534-4a0f-c984-857afd05b278"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (3.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.32.3)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (3.11.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.12.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.7.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras->keras-tuner) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n",
            "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 2: Function to Create a Small Subset of the Dataset"
      ],
      "metadata": {
        "id": "D_QZFJKY-I-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Section 2. Function to Create a Small Subset of the Dataset\n",
        "def create_subset(original_dir, subset_dir, classes, num_images_per_class):\n",
        "    if not os.path.exists(subset_dir):\n",
        "        os.makedirs(subset_dir)\n",
        "    for class_name in classes:\n",
        "        class_dir = os.path.join(original_dir, class_name)\n",
        "        subset_class_dir = os.path.join(subset_dir, class_name)\n",
        "        if not os.path.exists(subset_class_dir):\n",
        "            os.makedirs(subset_class_dir)\n",
        "        images = os.listdir(class_dir)\n",
        "        random.shuffle(images)\n",
        "        for img in images[:num_images_per_class]:\n",
        "            shutil.copy(os.path.join(class_dir, img), os.path.join(subset_class_dir, img))\n",
        "\n"
      ],
      "metadata": {
        "id": "WhQUj6IO-NP0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 3: Paths to Your Full and Subset Datasets\n"
      ],
      "metadata": {
        "id": "YM_Dsw_x4q1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### 2. Paths to your full dataset\n",
        "full_train_dir = 'dataset-c/triple_mnist/train'\n",
        "full_val_dir = 'dataset-c/triple_mnist/val'\n",
        "full_test_dir = 'dataset-c/triple_mnist/test'\n",
        "\n",
        "### Section 4. Paths to Your Subset Dataset\n",
        "subset_train_dir = 'subset/triple_mnist/train'\n",
        "subset_val_dir = 'subset/triple_mnist/val'\n",
        "subset_test_dir = 'subset/triple_mnist/test'"
      ],
      "metadata": {
        "id": "MhOspdUM-IW1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 4: Create a Small Subset of Your Data\n",
        "\n"
      ],
      "metadata": {
        "id": "R61X9uAL4tkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Create a small subset of your data\n",
        "classes_to_use = [str(i).zfill(3) for i in range(50)]  # Use the first 50 classes as an example\n",
        "num_images_per_class = 500  # Increase to 500 images per class\n",
        "create_subset(full_train_dir, subset_train_dir, classes_to_use, num_images_per_class)\n",
        "create_subset(full_val_dir, subset_val_dir, classes_to_use, num_images_per_class)\n",
        "create_subset(full_test_dir, subset_test_dir, classes_to_use, num_images_per_class)\n"
      ],
      "metadata": {
        "id": "dCp0Jk_65GuP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 5. Image data generators for the subset (Updated for Data Augmentation)\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Dzn5PI6r4vt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Section 5. Image data generators for the subset (Updated for Data Augmentation)\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n"
      ],
      "metadata": {
        "id": "ha5W2LD0fqc9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "old version"
      ],
      "metadata": {
        "id": "08pOywbJ9b4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ### 5. Image data generators for the subset\n",
        "# train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "# val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "# test_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "metadata": {
        "id": "LRIurcmomsI5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 6: Data Generators for the Subset\n"
      ],
      "metadata": {
        "id": "U2K6AfJW41iK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Section 6. Data Generators for the Subset\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    subset_train_dir,\n",
        "    target_size=(84, 84),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    subset_val_dir,\n",
        "    target_size=(84, 84),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    subset_test_dir,\n",
        "    target_size=(84, 84),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2GT0Rly5Hsf",
        "outputId": "61ef2f2a-e3a7-4cc6-b6e8-2bfe31aaf46f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3000 images belonging to 50 classes.\n",
            "Found 1000 images belonging to 50 classes.\n",
            "Found 1000 images belonging to 50 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 7: Transfer Learning (Optional)"
      ],
      "metadata": {
        "id": "9mRLwLzc41Ue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Section 7. Transfer Learning (Optional)\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(84, 84, 3))\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False  # Freeze layers to use as feature extractor\n",
        "\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(train_generator.num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDIOsf_chN_1",
        "outputId": "5435341f-14c0-4d0e-a755-f99013ef68b9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 8: Hyperparameter Tuning Function with Batch Size, Batch Normalization, and Regularization"
      ],
      "metadata": {
        "id": "VsmQa6JIow9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Version 2 is generally more robust due to the inclusion of batch normalization. It can lead to more stable training and better generalizatio"
      ],
      "metadata": {
        "id": "g8wp1vtrNVFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.layers import BatchNormalization\n",
        "# from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# ### Section 8. Hyperparameter Tuning Function with Regularization and Batch Normalization\n",
        "# def build_model(hp):\n",
        "#     model = Sequential()\n",
        "#     model.add(Conv2D(hp.Int('conv_1_filter', 32, 128, step=32), (3, 3), activation='relu',\n",
        "#                      input_shape=(84, 84, 3), kernel_regularizer=l2(0.001)))\n",
        "#     model.add(BatchNormalization())  # Added Batch Normalization\n",
        "#     model.add(MaxPooling2D((2, 2)))\n",
        "#     model.add(Conv2D(hp.Int('conv_2_filter', 32, 128, step=32), (3, 3), activation='relu', kernel_regularizer=l2(0.001)))\n",
        "#     model.add(BatchNormalization())  # Added Batch Normalization\n",
        "#     model.add(MaxPooling2D((2, 2)))\n",
        "#     model.add(Flatten())\n",
        "#     model.add(Dense(hp.Int('dense_units', 32, 128, step=32), activation='relu', kernel_regularizer=l2(0.001)))\n",
        "#     model.add(Dropout(hp.Float('dropout_rate', 0.2, 0.5, step=0.1)))\n",
        "#     model.add(Dense(train_generator.num_classes, activation='softmax'))\n",
        "\n",
        "#     model.compile(optimizer=Adam(learning_rate=hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG')),\n",
        "#                   loss='categorical_crossentropy',\n",
        "#                   metrics=['accuracy'])\n",
        "\n",
        "#     return model\n"
      ],
      "metadata": {
        "id": "BirWC0CDf2Qu"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Version 1 might be preferred if you want to simplify the model and focus more on controlling overfitting through regularization alone."
      ],
      "metadata": {
        "id": "KMJeHsk6NOpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Section 8. Hyperparameter Tuning Function with Batch Size, Batch Normalization, and Regularization\n",
        "# def build_model(hp):\n",
        "#     model = Sequential()\n",
        "\n",
        "#     # Print the batch size being used for this trial\n",
        "#     #print(f\"Batch size for this trial: {hp.get('batch_size')}\")\n",
        "\n",
        "#     # First convolutional layer\n",
        "#     model.add(Conv2D(hp.Int('conv_1_filter', 32, 128, step=32), (3, 3), activation='relu', input_shape=(84, 84, 3)))\n",
        "#     model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "#     # Second convolutional layer\n",
        "#     model.add(Conv2D(hp.Int('conv_2_filter', 32, 128, step=32), (3, 3), activation='relu'))\n",
        "#     model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "#     model.add(Flatten())\n",
        "\n",
        "#     # Dense layer with regularization\n",
        "#     model.add(Dense(hp.Int('dense_units', 32, 128, step=32), activation='relu',\n",
        "#                     kernel_regularizer=tf.keras.regularizers.l2(hp.Float('l2_regularization', 1e-4, 1e-2, sampling='LOG'))))\n",
        "\n",
        "#     model.add(Dropout(hp.Float('dropout_rate', 0.2, 0.5, step=0.1)))\n",
        "\n",
        "#     # Output layer\n",
        "#     model.add(Dense(train_generator.num_classes, activation='softmax'))\n",
        "\n",
        "#     # Compile the model with a tunable learning rate\n",
        "#     model.compile(optimizer=Adam(learning_rate=hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG')),\n",
        "#                   loss='categorical_crossentropy',\n",
        "#                   metrics=['accuracy'])\n",
        "\n",
        "#     return model\n",
        "\n"
      ],
      "metadata": {
        "id": "PgkpW1xzTHeW"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "new version to fix patch size issues"
      ],
      "metadata": {
        "id": "Wk9Yu931OW7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 8. Hyperparameter Tuning Function with Batch Size, Batch Normalization, and Regularization\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "\n",
        "    # Print the batch size being used for this trial\n",
        "    batch_size = hp.Int('batch_size', 16, 64, step=16)\n",
        "    print(f\"Batch size for this trial: {batch_size}\")\n",
        "\n",
        "    # First convolutional layer\n",
        "    model.add(Conv2D(hp.Int('conv_1_filter', 32, 128, step=32), (3, 3), activation='relu', input_shape=(84, 84, 3)))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "    # Second convolutional layer\n",
        "    model.add(Conv2D(hp.Int('conv_2_filter', 32, 128, step=32), (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # Dense layer with regularization\n",
        "    model.add(Dense(hp.Int('dense_units', 32, 128, step=32), activation='relu',\n",
        "                    kernel_regularizer=tf.keras.regularizers.l2(hp.Float('l2_regularization', 1e-4, 1e-2, sampling='LOG'))))\n",
        "\n",
        "    model.add(Dropout(hp.Float('dropout_rate', 0.2, 0.5, step=0.1)))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(Dense(train_generator.num_classes, activation='softmax'))\n",
        "\n",
        "    # Compile the model with a tunable learning rate\n",
        "    model.compile(optimizer=Adam(learning_rate=hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG')),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "CmnMPwiVOaZZ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 9: Early Stopping Callback"
      ],
      "metadata": {
        "id": "fT97PfFFpNZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Section 9. Callback for Early Stopping (Updated for longer patience)\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,  # Increased patience for better fine-tuning\n",
        "    restore_best_weights=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "E-AGVk0VgOJ9"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 10: Tuner Setup (Corrected with Batch Size Tuning)\n",
        "\n"
      ],
      "metadata": {
        "id": "XTf8kPeh5Ivo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Newest Version is probably the most straightforward while maintaining modularity. It ensures that batch size tuning is included without adding unnecessary complexity."
      ],
      "metadata": {
        "id": "hhuJaPBrALQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Section 10. Tuner Setup (Including Batch Size Tuning)\n",
        "# tuner = kt.Hyperband(\n",
        "#     build_model,\n",
        "#     objective='val_accuracy',\n",
        "#     max_epochs=10,\n",
        "#     factor=3,\n",
        "#     directory='new_output',\n",
        "#     project_name='digit_tuning_subset'\n",
        "# )\n",
        "\n",
        "# # Adding batch size to the hyperparameter search space\n",
        "# hp = kt.HyperParameters()\n",
        "# hp.Int('batch_size', 16, 64, step=16)\n",
        "\n",
        "# # Include batch size in the search space and ensure it's passed correctly\n",
        "# tuner.search(\n",
        "#     train_generator,\n",
        "#     epochs=10,\n",
        "#     validation_data=val_generator,\n",
        "#     callbacks=[early_stopping],\n",
        "#     hyperparameters=hp\n",
        "# )\n",
        "\n",
        "# # Log the hyperparameters, including batch size\n",
        "# tuner.search_space_summary()\n",
        "\n"
      ],
      "metadata": {
        "id": "DrY70F6O-wvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Version 1 adds an extra step by retrieving the batch_size manually,"
      ],
      "metadata": {
        "id": "cHuLR4gMABn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Section 10. Tuner Setup (Including Batch Size Tuning)\n",
        "# tuner = kt.Hyperband(\n",
        "#     build_model,\n",
        "#     objective='val_accuracy',\n",
        "#     max_epochs=10,\n",
        "#     factor=3,\n",
        "#     directory='new_output',\n",
        "#     project_name='digit_tuning_subset'\n",
        "# )\n",
        "\n",
        "# # Adding batch size to the hyperparameter search space\n",
        "# hp = kt.HyperParameters()\n",
        "# hp.Int('batch_size', 16, 64, step=16)\n",
        "\n",
        "# # Log the hyperparameters, including batch size\n",
        "# tuner.search_space_summary()\n",
        "\n",
        "# # Start the hyperparameter search\n",
        "# tuner.search(\n",
        "#     train_generator,\n",
        "#     epochs=10,\n",
        "#     validation_data=val_generator,\n",
        "#     callbacks=[early_stopping],\n",
        "#     # Pass the batch size as part of the search space\n",
        "#     batch_size=hp.get('batch_size')\n",
        "# )\n"
      ],
      "metadata": {
        "id": "mVm-UiBL5Q1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Version 2 is the most concise but lacks the flexibility and might be less clear when scaling or adding more hyperparameters."
      ],
      "metadata": {
        "id": "-c79lLbAAF5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Section 10. Tuner Setup\n",
        "# tuner = kt.Hyperband(\n",
        "#     build_model,\n",
        "#     objective='val_accuracy',\n",
        "#     max_epochs=10,\n",
        "#     factor=3,\n",
        "#     directory='new_output',\n",
        "#     project_name='digit_tuning_subset'\n",
        "# )\n",
        "\n",
        "# # Adding batch size to the hyperparameter search space\n",
        "# tuner.search_space_summary()\n",
        "# tuner.search(\n",
        "#     train_generator,\n",
        "#     epochs=10,\n",
        "#     validation_data=val_generator,\n",
        "#     callbacks=[early_stopping],\n",
        "#     # Specify batch size here to be tuned\n",
        "#     batch_size=kt.HyperParameters().Int('batch_size', 16, 64, step=16)\n",
        "# )\n"
      ],
      "metadata": {
        "id": "3bkESWjlhGSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "new version"
      ],
      "metadata": {
        "id": "rWabOseqN74g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 10. Tuner Setup (Including Batch Size Tuning)\n",
        "tuner = kt.Hyperband(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_epochs=10,\n",
        "    factor=3,\n",
        "    directory='new_output',\n",
        "    project_name='digit_tuning_subset'\n",
        ")\n",
        "\n",
        "# Start the hyperparameter search\n",
        "tuner.search(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Log the hyperparameters, including batch size\n",
        "tuner.search_space_summary()\n"
      ],
      "metadata": {
        "id": "T1JTVQR1N9sY",
        "outputId": "0116010e-0326-41d6-ddb2-8ab1cd11cf92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 15 Complete [00h 00m 35s]\n",
            "val_accuracy: 0.019999999552965164\n",
            "\n",
            "Best val_accuracy So Far: 0.04100000113248825\n",
            "Total elapsed time: 00h 07m 01s\n",
            "\n",
            "Search: Running Trial #16\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "96                |32                |conv_1_filter\n",
            "32                |128               |conv_2_filter\n",
            "96                |96                |dense_units\n",
            "0.3               |0.2               |dropout_rate\n",
            "0.0035583         |0.00020196        |learning_rate\n",
            "16                |48                |batch_size\n",
            "0.0019894         |0.00015774        |l2_regularization\n",
            "4                 |4                 |tuner/epochs\n",
            "0                 |2                 |tuner/initial_epoch\n",
            "1                 |2                 |tuner/bracket\n",
            "0                 |1                 |tuner/round\n",
            "\n",
            "Batch size for this trial: 16\n",
            "Epoch 1/4\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 103ms/step - accuracy: 0.0192 - loss: 4.1663 - val_accuracy: 0.0200 - val_loss: 3.9208\n",
            "Epoch 2/4\n",
            "\u001b[1m60/94\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - accuracy: 0.0121 - loss: 3.9182"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 11: Learning Rate Scheduler (New)"
      ],
      "metadata": {
        "id": "3rAbY1dFpjFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Section 11. Learning Rate Scheduler\n",
        "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.1,\n",
        "    patience=3,\n",
        "    min_lr=1e-6,\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "nIXAWvCUpfsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 12: Perform the Hyperparameter Search on the Subset"
      ],
      "metadata": {
        "id": "NiTy4w-M5PoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Section 12. Perform the Hyperparameter Search on the Subset\n",
        "tuner.search(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[early_stopping, lr_scheduler]  # Include both callbacks here\n",
        ")\n"
      ],
      "metadata": {
        "id": "lZA5Q8Jv5RWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 13: Train Model with Best Hyperparameters\n",
        "\n"
      ],
      "metadata": {
        "id": "M6XmwGSz5VrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Section 13. Train Model with Best Hyperparameters\n",
        "print(\"Compiling and training the model with the best hyperparameters...\")\n",
        "\n",
        "# Build the best model from the hyperparameter search\n",
        "best_model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Explicitly compile the model to ensure metrics are tracked\n",
        "best_model.compile(\n",
        "    optimizer=Adam(learning_rate=best_hps.get('learning_rate')),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model with the best hyperparameters, using the early stopping and learning rate scheduler callbacks\n",
        "history = best_model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[early_stopping, lr_scheduler]\n",
        ")\n",
        "\n",
        "# After training, print out the history keys to check for 'val_accuracy'\n",
        "print(\"Available keys in history.history after training:\", history.history.keys())\n",
        "\n",
        "# Evaluate the model on the subset test data\n",
        "test_loss, test_acc = best_model.evaluate(test_generator)\n",
        "print(f\"Test accuracy on subset: {test_acc}\")\n"
      ],
      "metadata": {
        "id": "AkoGDBLriJ4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 14: Cross-Validation Loop (Optional)\n"
      ],
      "metadata": {
        "id": "Uz002UT1qWNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Section 14: Cross-Validation Loop (Optional)\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "### Section 14. Cross-Validation Loop (Optional)\n",
        "kf = KFold(n_splits=5)\n",
        "cv_scores = []\n",
        "\n",
        "for train_idx, val_idx in kf.split(train_data):\n",
        "    train_data_fold = train_data[train_idx]\n",
        "    val_data_fold = val_data[val_idx]\n",
        "\n",
        "    history = best_model.fit(train_data_fold, epochs=10, validation_data=val_data_fold,\n",
        "                             callbacks=[early_stopping, lr_scheduler])\n",
        "    score = best_model.evaluate(val_data_fold)\n",
        "    cv_scores.append(score)\n",
        "\n",
        "print(\"Cross-validation scores:\", cv_scores)\n"
      ],
      "metadata": {
        "id": "7ioC8505qUaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 15: Get the Best Hyperparameters"
      ],
      "metadata": {
        "id": "VfDRHbXu5TlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Section 15. Get the Best Hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "# Print the winning hyperparameters on screen\n",
        "print(\"Winning Hyperparameters:\")\n",
        "print(f\"conv_1_filter: {best_hps.get('conv_1_filter')}\")\n",
        "print(f\"conv_2_filter: {best_hps.get('conv_2_filter')}\")\n",
        "print(f\"dense_units: {best_hps.get('dense_units')}\")\n",
        "print(f\"dropout_rate: {best_hps.get('dropout_rate')}\")\n",
        "print(f\"learning_rate: {best_hps.get('learning_rate')}\")\n",
        "print(f\"batch_size: {best_hps.values.get('batch_size', 32)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "5xP27-sf5UY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 16. Save the results in a CSV file"
      ],
      "metadata": {
        "id": "wziPOH5z5c_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Section 16. Save the results in a CSV file\n",
        "results = []\n",
        "for trial in tuner.oracle.get_best_trials(num_trials=10):\n",
        "    trial_summary = {\n",
        "        'trial_number': trial.trial_id,\n",
        "        'conv_1_filter': trial.hyperparameters.get('conv_1_filter'),\n",
        "        'conv_2_filter': trial.hyperparameters.get('conv_2_filter'),\n",
        "        'dense_units': trial.hyperparameters.get('dense_units'),\n",
        "        'dropout_rate': trial.hyperparameters.get('dropout_rate'),\n",
        "        'learning_rate': trial.hyperparameters.get('learning_rate'),\n",
        "        'batch_size': trial.hyperparameters.values['batch_size'] if 'batch_size' in trial.hyperparameters.values else 32,\n",
        "        'accuracy': trial.metrics.get_last_value('accuracy'),\n",
        "        'loss': trial.metrics.get_last_value('loss'),\n",
        "        'val_accuracy': trial.metrics.get_last_value('val_accuracy'),\n",
        "        'val_loss': trial.metrics.get_last_value('val_loss'),\n",
        "        'test_accuracy': test_acc,\n",
        "        'test_loss': test_loss,\n",
        "    }\n",
        "    results.append(trial_summary)\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv('new_output/trial_results.csv', index=False)\n"
      ],
      "metadata": {
        "id": "YLKyTSwl5gKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 17: Train on Full Dataset (Updated)"
      ],
      "metadata": {
        "id": "IwgtpneX5hwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Section 17. Train on Full Dataset Without Validation Threshold\n",
        "\n",
        "# Use the best hyperparameters for the full dataset\n",
        "batch_size = best_hps.values.get('batch_size', 32)  # Properly retrieve the batch size with a default fallback\n",
        "\n",
        "train_generator_full = train_datagen.flow_from_directory(\n",
        "    full_train_dir,\n",
        "    target_size=(84, 84),\n",
        "    batch_size=batch_size,  # Use the resolved batch_size\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator_full = val_datagen.flow_from_directory(\n",
        "    full_val_dir,\n",
        "    target_size=(84, 84),\n",
        "    batch_size=batch_size,  # Use the resolved batch_size\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator_full = test_datagen.flow_from_directory(\n",
        "    full_test_dir,\n",
        "    target_size=(84, 84),\n",
        "    batch_size=batch_size,  # Use the resolved batch_size\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Build and train the model with the best hyperparameters on the full dataset\n",
        "best_model_full = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Modify the final layer to match the number of classes in the full dataset\n",
        "best_model_full.pop()  # Remove the old final layer\n",
        "best_model_full.add(Dense(train_generator_full.num_classes, activation='softmax'))  # Add a new layer with correct output size\n",
        "\n",
        "best_model_full.compile(optimizer=Adam(learning_rate=best_hps.get('learning_rate')),\n",
        "                        loss='categorical_crossentropy',\n",
        "                        metrics=['accuracy'])\n",
        "\n",
        "history_full = best_model_full.fit(\n",
        "    train_generator_full,\n",
        "    epochs=20,\n",
        "    validation_data=val_generator_full,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluate the model on the full test data\n",
        "test_loss_full, test_acc_full = best_model_full.evaluate(test_generator_full)\n",
        "print(f\"Test accuracy on full dataset: {test_acc_full}\")\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_full.history['accuracy'], label='accuracy')\n",
        "plt.plot(history_full.history['val_accuracy'], label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_full.history['loss'], label='loss')\n",
        "plt.plot(history_full.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Displaying the winning hyperparameters\n",
        "best_hyperparameters_df = pd.DataFrame([{\n",
        "    'conv_1_filter': best_hps.get('conv_1_filter'),\n",
        "    'conv_2_filter': best_hps.get('conv_2_filter'),\n",
        "    'dense_units': best_hps.get('dense_units'),\n",
        "    'dropout_rate': best_hps.get('dropout_rate'),\n",
        "    'learning_rate': best_hps.get('learning_rate'),\n",
        "    'batch_size': batch_size  # Use the resolved batch size\n",
        "}])\n",
        "print(best_hyperparameters_df)\n",
        "best_hyperparameters_df.to_csv('new_output/best_hyperparameters.csv', index=False)\n"
      ],
      "metadata": {
        "id": "jpgW1EJ2tdLj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}