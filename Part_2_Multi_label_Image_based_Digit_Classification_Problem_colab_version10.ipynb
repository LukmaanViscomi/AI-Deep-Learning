{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1RrjnI9Y3uK-CBmTzYoSq2UNhLanmCjy5",
      "authorship_tag": "ABX9TyMwLg89DF4MqAGoj7gCgAEP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LukmaanViscomi/AI-Deep-Learning/blob/main/Part_2_Multi_label_Image_based_Digit_Classification_Problem_colab_version10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thsi code creates an upload button to copy the zip file from your local directory"
      ],
      "metadata": {
        "id": "zmAYWy_X-yry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        },
        "id": "o-WUlF8zTC-3",
        "outputId": "27770543-f54a-4c4b-f5e2-b5df0ebfaec1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8471368a-fd83-43c8-b5cf-00a242d19609\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8471368a-fd83-43c8-b5cf-00a242d19609\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving dataset2 (1).zip to dataset2 (1).zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code checks he MD5 hash of the fie to make sure there was no corruption in transit."
      ],
      "metadata": {
        "id": "ZrA8Oc0l-ne6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "\n",
        "# Function to calculate MD5 checksum\n",
        "def calculate_md5(file_path):\n",
        "    hash_md5 = hashlib.md5()\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
        "            hash_md5.update(chunk)\n",
        "    return hash_md5.hexdigest()\n",
        "\n",
        "# Get the uploaded file name\n",
        "uploaded_file_name = list(uploaded.keys())[0]\n",
        "\n",
        "# Calculate the MD5 checksum\n",
        "uploaded_file_md5 = calculate_md5(uploaded_file_name)\n",
        "print(f\"MD5 checksum of the uploaded file: {uploaded_file_md5}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJZkdnAtS-mw",
        "outputId": "3b61e957-1948-4a4a-a553-6a1ec04b5fee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MD5 checksum of the uploaded file: bcc80758eb75688e00498d65e1204258\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code unzips the dataset2 file and unpacks it locally"
      ],
      "metadata": {
        "id": "LrEfiQU--hJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install p7zip-full # Install 7-Zip\n",
        "!pip install patool # Install the patool library which provides the patoolib module\n",
        "import zipfile\n",
        "import os\n",
        "import patoolib # Now you can import patoolib\n",
        "\n",
        "# Path to the uploaded zip file\n",
        "zip_file_path = 'dataset2 (1).zip'\n",
        "extracted_folder_path = './dataset2'  # Use a relative path for the extraction directory\n",
        "\n",
        "# Extract the zip file using patool\n",
        "patoolib.extract_archive(zip_file_path, outdir=extracted_folder_path)\n",
        "\n",
        "# List the contents of the extracted folder\n",
        "extracted_files = os.listdir(extracted_folder_path)\n",
        "print(extracted_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_k-0oLLS86m",
        "outputId": "fba4ae98-491a-4855-9575-87c7d5978252"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "p7zip-full is already the newest version (16.02+dfsg-8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "Collecting patool\n",
            "  Downloading patool-2.3.0-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: patool\n",
            "Successfully installed patool-2.3.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO patool: Extracting dataset2 (1).zip ...\n",
            "INFO:patool:Extracting dataset2 (1).zip ...\n",
            "INFO patool: ... creating output directory `./dataset2'.\n",
            "INFO:patool:... creating output directory `./dataset2'.\n",
            "INFO patool: running /usr/bin/7z x -o./dataset2 -- \"dataset2 (1).zip\"\n",
            "INFO:patool:running /usr/bin/7z x -o./dataset2 -- \"dataset2 (1).zip\"\n",
            "INFO patool:     with input=''\n",
            "INFO:patool:    with input=''\n",
            "INFO patool: ... dataset2 (1).zip extracted to `./dataset2'.\n",
            "INFO:patool:... dataset2 (1).zip extracted to `./dataset2'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['triple_mnist']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below basically takes all the images classes which are 1000 (there are 100 varietion of the same image in each class folder). For each class it 60% into the Training folder, 20% int the Valuation folder and 20% into the Test folder."
      ],
      "metadata": {
        "id": "XWtS5vnk92cp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQWYhOD-SrNy",
        "outputId": "d7de03f2-c71f-46c0-b439-cd6bf0caab8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files split and copied successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import random\n",
        "\n",
        "# Paths to original directories\n",
        "original_base_dir = Path('dataset2/triple_mnist')\n",
        "original_train_dir = original_base_dir / 'train'\n",
        "original_val_dir = original_base_dir / 'val'\n",
        "original_test_dir = original_base_dir / 'test'\n",
        "\n",
        "# Path to the new dataset directory\n",
        "new_base_dir = Path('dataset-c/triple_mnist')\n",
        "new_train_dir = new_base_dir / 'train'\n",
        "new_val_dir = new_base_dir / 'val'\n",
        "new_test_dir = new_base_dir / 'test'\n",
        "\n",
        "# Ensure the new directories exist\n",
        "new_train_dir.mkdir(parents=True, exist_ok=True)\n",
        "new_val_dir.mkdir(parents=True, exist_ok=True)\n",
        "new_test_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Function to split and copy files\n",
        "def split_and_copy_files(src_dir, new_train_dir, new_val_dir, new_test_dir, train_ratio=0.6, val_ratio=0.2, test_ratio=0.2):\n",
        "    if not src_dir.exists():\n",
        "        return\n",
        "\n",
        "    classes = sorted(os.listdir(src_dir))\n",
        "\n",
        "    for cls in classes:\n",
        "        cls_path = src_dir / cls\n",
        "        if cls_path.is_dir():\n",
        "            images = list(cls_path.glob('*'))\n",
        "            random.shuffle(images)\n",
        "\n",
        "            num_train = int(len(images) * train_ratio)\n",
        "            num_val = int(len(images) * val_ratio)\n",
        "\n",
        "            train_images = images[:num_train]\n",
        "            val_images = images[num_train:num_train+num_val]\n",
        "            test_images = images[num_train+num_val:]\n",
        "\n",
        "            cls_train_dir = new_train_dir / cls\n",
        "            cls_val_dir = new_val_dir / cls\n",
        "            cls_test_dir = new_test_dir / cls\n",
        "\n",
        "            cls_train_dir.mkdir(parents=True, exist_ok=True)\n",
        "            cls_val_dir.mkdir(parents=True, exist_ok=True)\n",
        "            cls_test_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            for img in train_images:\n",
        "                shutil.copy(str(img), str(cls_train_dir / img.name))\n",
        "            for img in val_images:\n",
        "                shutil.copy(str(img), str(cls_val_dir / img.name))\n",
        "            for img in test_images:\n",
        "                shutil.copy(str(img), str(cls_test_dir / img.name))\n",
        "\n",
        "# Split and copy files from original train, val, and test directories\n",
        "split_and_copy_files(original_train_dir, new_train_dir, new_val_dir, new_test_dir)\n",
        "split_and_copy_files(original_val_dir, new_train_dir, new_val_dir, new_test_dir)\n",
        "split_and_copy_files(original_test_dir, new_train_dir, new_val_dir, new_test_dir)\n",
        "\n",
        "print(\"Files split and copied successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is working but not fully developed"
      ],
      "metadata": {
        "id": "9OIAuivv9wM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import keras_tuner as kt\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Function to create a small subset of the dataset\n",
        "def create_subset(original_dir, subset_dir, classes, num_images_per_class):\n",
        "    if not os.path.exists(subset_dir):\n",
        "        os.makedirs(subset_dir)\n",
        "    for class_name in classes:\n",
        "        class_dir = os.path.join(original_dir, class_name)\n",
        "        subset_class_dir = os.path.join(subset_dir, class_name)\n",
        "        if not os.path.exists(subset_class_dir):\n",
        "            os.makedirs(subset_class_dir)\n",
        "        images = os.listdir(class_dir)\n",
        "        random.shuffle(images)\n",
        "        for img in images[:num_images_per_class]:\n",
        "            shutil.copy(os.path.join(class_dir, img), os.path.join(subset_class_dir, img))\n",
        "\n",
        "# Paths to your full dataset\n",
        "full_train_dir = 'dataset-c/triple_mnist/train'\n",
        "full_val_dir = 'dataset-c/triple_mnist/val'\n",
        "full_test_dir = 'dataset-c/triple_mnist/test'\n",
        "\n",
        "# Paths to your subset dataset\n",
        "subset_train_dir = 'subset/triple_mnist/train'\n",
        "subset_val_dir = 'subset/triple_mnist/val'\n",
        "subset_test_dir = 'subset/triple_mnist/test'\n",
        "\n",
        "# Create a small subset of your data\n",
        "classes_to_use = ['000', '001', '002']  # Example classes\n",
        "num_images_per_class = 50  # Example number of images per class\n",
        "create_subset(full_train_dir, subset_train_dir, classes_to_use, num_images_per_class)\n",
        "create_subset(full_val_dir, subset_val_dir, classes_to_use, num_images_per_class)\n",
        "create_subset(full_test_dir, subset_test_dir, classes_to_use, num_images_per_class)\n",
        "\n",
        "# Image data generators for the subset\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Data generators for the subset\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    subset_train_dir,\n",
        "    target_size=(84, 84),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    subset_val_dir,\n",
        "    target_size=(84, 84),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    subset_test_dir,\n",
        "    target_size=(84, 84),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Hyperparameter tuning function\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(hp.Int('conv_1_filter', 32, 128, step=32), (3, 3), activation='relu', input_shape=(84, 84, 3)))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Conv2D(hp.Int('conv_2_filter', 32, 128, step=32), (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(hp.Int('dense_units', 32, 128, step=32), activation='relu'))\n",
        "    model.add(Dropout(hp.Float('dropout_rate', 0.2, 0.5, step=0.1)))\n",
        "    model.add(Dense(train_generator.num_classes, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG')),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Tuner setup\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,  # Limit the number of trials for faster tuning\n",
        "    executions_per_trial=1,  # Number of models to build and fit for each trial\n",
        "    directory='output',\n",
        "    project_name='digit_tuning_subset'\n",
        ")\n",
        "\n",
        "# Callback for early stopping\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Perform the hyperparameter search on the subset\n",
        "tuner.search(\n",
        "    train_generator,\n",
        "    epochs=10,  # You can adjust the number of epochs\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "# Build and train the model with the best hyperparameters on the subset\n",
        "best_model = tuner.hypermodel.build(best_hps)\n",
        "history = best_model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluate the model on the subset test data\n",
        "test_loss, test_acc = best_model.evaluate(test_generator)\n",
        "print(f\"Test accuracy on subset: {test_acc}\")\n",
        "\n",
        "# Use the best hyperparameters for the full dataset\n",
        "train_generator_full = train_datagen.flow_from_directory(\n",
        "    full_train_dir,\n",
        "    target_size=(84, 84),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator_full = val_datagen.flow_from_directory(\n",
        "    full_val_dir,\n",
        "    target_size=(84, 84),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator_full = test_datagen.flow_from_directory(\n",
        "    full_test_dir,\n",
        "    target_size=(84, 84),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "\n",
        "# Build and train the model with the best hyperparameters on the full dataset\n",
        "best_model_full = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Modify the final layer to match the number of classes in the full dataset\n",
        "best_model_full.pop()  # Remove the old final layer\n",
        "best_model_full.add(Dense(train_generator_full.num_classes, activation='softmax'))  # Add a new layer with correct output size\n",
        "\n",
        "best_model_full.compile(optimizer=Adam(learning_rate=best_hps.get('learning_rate')),\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "history_full = best_model_full.fit(\n",
        "    train_generator_full,\n",
        "    epochs=20,\n",
        "    validation_data=val_generator_full,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "\n",
        "# Evaluate the model on the full test data\n",
        "test_loss_full, test_acc_full = best_model_full.evaluate(test_generator_full)\n",
        "print(f\"Test accuracy on full dataset: {test_acc_full}\")\n",
        "\n",
        "# Plot training history\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_full.history['accuracy'], label='accuracy')\n",
        "plt.plot(history_full.history['val_accuracy'], label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_full.history['loss'], label='loss')\n",
        "plt.plot(history_full.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zfA3CYCVXE27",
        "outputId": "d6f32890-8053-4856-ae31-2efdce1dbd21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.10/dist-packages (1.4.7)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.7.4)\n",
            "Found 171 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Reloading Tuner from output/digit_tuning_subset/tuner0.json\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 8s 1s/step - loss: 1.3795 - accuracy: 0.3801 - val_loss: 1.0826 - val_accuracy: 0.5667\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 5s 904ms/step - loss: 1.0035 - accuracy: 0.5614 - val_loss: 0.8739 - val_accuracy: 0.6000\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 7s 1s/step - loss: 0.6139 - accuracy: 0.6959 - val_loss: 0.7347 - val_accuracy: 0.7167\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 6s 898ms/step - loss: 0.4714 - accuracy: 0.7778 - val_loss: 0.6087 - val_accuracy: 0.7333\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 6s 1s/step - loss: 0.3655 - accuracy: 0.8538 - val_loss: 0.6724 - val_accuracy: 0.7667\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 5s 906ms/step - loss: 0.1787 - accuracy: 0.9123 - val_loss: 0.8525 - val_accuracy: 0.7000\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 7s 1s/step - loss: 0.1137 - accuracy: 0.9591 - val_loss: 1.4069 - val_accuracy: 0.7333\n",
            "2/2 [==============================] - 1s 230ms/step - loss: 0.9247 - accuracy: 0.6333\n",
            "Test accuracy on subset: 0.6333333253860474\n",
            "Found 60000 images belonging to 1000 classes.\n",
            "Found 20000 images belonging to 1000 classes.\n",
            "Found 20000 images belonging to 1000 classes.\n",
            "Epoch 1/20\n",
            "1875/1875 [==============================] - 2208s 1s/step - loss: 6.9233 - accuracy: 5.6667e-04 - val_loss: 6.9094 - val_accuracy: 0.0010\n",
            "Epoch 2/20\n",
            "1875/1875 [==============================] - 2198s 1s/step - loss: 6.9201 - accuracy: 4.5000e-04 - val_loss: 6.9094 - val_accuracy: 0.0010\n",
            "Epoch 3/20\n",
            "1875/1875 [==============================] - 2196s 1s/step - loss: 6.9201 - accuracy: 6.0000e-04 - val_loss: 6.9094 - val_accuracy: 0.0010\n",
            "Epoch 4/20\n",
            "1875/1875 [==============================] - 2180s 1s/step - loss: 6.9202 - accuracy: 7.8333e-04 - val_loss: 6.9095 - val_accuracy: 0.0010\n",
            "625/625 [==============================] - 196s 314ms/step - loss: 6.9094 - accuracy: 0.0010\n",
            "Test accuracy on full dataset: 0.0010000000474974513\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FhdQjc6vZhVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# These model has batch size **included** in the hyperparmeter trials"
      ],
      "metadata": {
        "id": "fkZNFOyJZ0jQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "this code is not working - hyperparameters not rotating"
      ],
      "metadata": {
        "id": "J5lRI8WFZ0PF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Keras Tuner\n",
        "!pip install keras-tuner --upgrade\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import keras_tuner as kt\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to create a small subset of the dataset\n",
        "def create_subset(original_dir, subset_dir, classes, num_images_per_class):\n",
        "    if not os.path.exists(subset_dir):\n",
        "        os.makedirs(subset_dir)\n",
        "    for class_name in classes:\n",
        "        class_dir = os.path.join(original_dir, class_name)\n",
        "        subset_class_dir = os.path.join(subset_dir, class_name)\n",
        "        if not os.path.exists(subset_class_dir):\n",
        "            os.makedirs(subset_class_dir)\n",
        "        images = os.listdir(class_dir)\n",
        "        random.shuffle(images)\n",
        "        for img in images[:num_images_per_class]:\n",
        "            shutil.copy(os.path.join(class_dir, img), os.path.join(subset_class_dir, img))\n",
        "\n",
        "# Paths to your full dataset\n",
        "full_train_dir = 'dataset-c/triple_mnist/train'\n",
        "full_val_dir = 'dataset-c/triple_mnist/val'\n",
        "full_test_dir = 'dataset-c/triple_mnist/test'\n",
        "\n",
        "# Paths to your subset dataset\n",
        "subset_train_dir = 'subset/triple_mnist/train'\n",
        "subset_val_dir = 'subset/triple_mnist/val'\n",
        "subset_test_dir = 'subset/triple_mnist/test'\n",
        "\n",
        "# Create a small subset of your data\n",
        "classes_to_use = ['000', '001', '002']  # Example classes\n",
        "num_images_per_class = 50  # Example number of images per class\n",
        "create_subset(full_train_dir, subset_train_dir, classes_to_use, num_images_per_class)\n",
        "create_subset(full_val_dir, subset_val_dir, classes_to_use, num_images_per_class)\n",
        "create_subset(full_test_dir, subset_test_dir, classes_to_use, num_images_per_class)\n",
        "\n",
        "# Data augmentation for training\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Data generators for the subset\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Hyperparameter tuning function\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(hp.Int('conv_1_filter', 32, 128, step=32), (3, 3), activation='relu', input_shape=(84, 84, 3)))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Conv2D(hp.Int('conv_2_filter', 32, 128, step=32), (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(hp.Int('dense_units', 32, 128, step=32), activation='relu'))\n",
        "    model.add(Dropout(hp.Float('dropout_rate', 0.2, 0.5, step=0.1)))\n",
        "    model.add(Dense(3, activation='softmax'))  # For subset data\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG')),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Callback for early stopping\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Define the tuner using RandomSearch\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=1,\n",
        "    directory='output',\n",
        "    project_name='digit_tuning_subset'\n",
        ")\n",
        "\n",
        "best_acc = 0\n",
        "while best_acc < 0.80:  # Run until accuracy of 80% is achieved\n",
        "    # Perform the hyperparameter search on the subset\n",
        "    tuner.search(\n",
        "        train_datagen.flow_from_directory(\n",
        "            subset_train_dir,\n",
        "            target_size=(84, 84),\n",
        "            batch_size=32,\n",
        "            class_mode='categorical'\n",
        "        ),\n",
        "        epochs=10,\n",
        "        validation_data=val_datagen.flow_from_directory(\n",
        "            subset_val_dir,\n",
        "            target_size=(84, 84),\n",
        "            batch_size=32,\n",
        "            class_mode='categorical'\n",
        "        ),\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    # Retrieve the best hyperparameters and print them\n",
        "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "    # Build and train the model with the best hyperparameters on the subset\n",
        "    best_model = tuner.hypermodel.build(best_hps)\n",
        "    history = best_model.fit(\n",
        "        train_datagen.flow_from_directory(\n",
        "            subset_train_dir,\n",
        "            target_size=(84, 84),\n",
        "            batch_size=best_hps.get('batch_size'),\n",
        "            class_mode='categorical'\n",
        "        ),\n",
        "        epochs=10,\n",
        "        validation_data=val_datagen.flow_from_directory(\n",
        "            subset_val_dir,\n",
        "            target_size=(84, 84),\n",
        "            batch_size=best_hps.get('batch_size'),\n",
        "            class_mode='categorical'\n",
        "        ),\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    # Evaluate the model on the subset test data\n",
        "    test_loss, test_acc = best_model.evaluate(\n",
        "        test_datagen.flow_from_directory(\n",
        "            subset_test_dir,\n",
        "            target_size=(84, 84),\n",
        "            batch_size=best_hps.get('batch_size'),\n",
        "            class_mode='categorical'\n",
        "        )\n",
        "    )\n",
        "    print(f\"Test accuracy on subset: {test_acc}\")\n",
        "    best_acc = test_acc\n",
        "\n",
        "    # Print the best hyperparameters\n",
        "    print(\"Best Hyperparameters:\")\n",
        "    for hp, value in best_hps.values.items():\n",
        "        print(f\"{hp}: {value}\")\n",
        "\n",
        "# Plot training history for the subset\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Use the best hyperparameters for the full dataset\n",
        "train_generator_full = train_datagen.flow_from_directory(\n",
        "    full_train_dir,\n",
        "    target_size=(84, 84),\n",
        "    batch_size=best_hps.get('batch_size'),\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator_full = val_datagen.flow_from_directory(\n",
        "    full_val_dir,\n",
        "    target_size=(84, 84),\n",
        "    batch_size=best_hps.get('batch_size'),\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator_full = test_datagen.flow_from_directory(\n",
        "    full_test_dir,\n",
        "    target_size=(84, 84),\n",
        "    batch_size=best_hps.get('batch_size'),\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Build and train the model with the best hyperparameters on the full dataset\n",
        "best_model_full = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Modify the final layer to match the number of classes in the full dataset\n",
        "best_model_full.pop()  # Remove the old final layer\n",
        "best_model_full.add(Dense(train_generator_full.num_classes, activation='softmax'))  # Add a new layer with correct output size\n",
        "\n",
        "best_model_full.compile(optimizer=Adam(learning_rate=best_hps.get('learning_rate')),\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "history_full = best_model_full.fit(\n",
        "    train_generator_full,\n",
        "    epochs=20,\n",
        "    validation_data=val_generator_full,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluate the model on the full test data\n",
        "test_loss_full, test_acc_full = best_model_full.evaluate(test_generator_full)\n",
        "print(f\"Test accuracy on full dataset: {test_acc_full}\")\n",
        "\n",
        "# Plot training history for the full dataset\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_full.history['accuracy'], label='accuracy')\n",
        "plt.plot(history_full.history['val_accuracy'], label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_full.history['loss'], label='loss')\n",
        "plt.plot(history_full.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZbavJ1pVOLsh",
        "outputId": "d34548e0-82a6-49a6-8ceb-2f2b3895e488"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.10/dist-packages (1.4.7)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.7.4)\n",
            "Reloading Tuner from output/digit_tuning_subset/tuner0.json\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 5s 4s/step - loss: 1.5503 - accuracy: 0.2944 - val_loss: 1.1795 - val_accuracy: 0.3333\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1111 - accuracy: 0.3667 - val_loss: 1.0960 - val_accuracy: 0.3833\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0950 - accuracy: 0.3889 - val_loss: 1.0965 - val_accuracy: 0.3167\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0970 - accuracy: 0.3278 - val_loss: 1.0962 - val_accuracy: 0.3167\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 934ms/step - loss: 1.0966 - accuracy: 0.3111 - val_loss: 1.0947 - val_accuracy: 0.3167\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 961ms/step - loss: 1.0939 - accuracy: 0.3833 - val_loss: 1.0895 - val_accuracy: 0.3500\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0891 - accuracy: 0.4000 - val_loss: 1.0822 - val_accuracy: 0.3833\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0924 - accuracy: 0.3500 - val_loss: 1.0786 - val_accuracy: 0.3667\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0889 - accuracy: 0.3056 - val_loss: 1.0823 - val_accuracy: 0.3000\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 3s 970ms/step - loss: 1.0847 - accuracy: 0.3389 - val_loss: 1.0852 - val_accuracy: 0.3000\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 320ms/step - loss: 1.0882 - accuracy: 0.3667\n",
            "Test accuracy on subset: 0.36666667461395264\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 6s 3s/step - loss: 1.4461 - accuracy: 0.2944 - val_loss: 1.1951 - val_accuracy: 0.2833\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1330 - accuracy: 0.3333 - val_loss: 1.1031 - val_accuracy: 0.2833\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0982 - accuracy: 0.3778 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 1s/step - loss: 1.0986 - accuracy: 0.3556 - val_loss: 1.0981 - val_accuracy: 0.3667\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0985 - accuracy: 0.3833 - val_loss: 1.0973 - val_accuracy: 0.4500\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0978 - accuracy: 0.3778 - val_loss: 1.0966 - val_accuracy: 0.4667\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 3s 945ms/step - loss: 1.0955 - accuracy: 0.3667 - val_loss: 1.0952 - val_accuracy: 0.3667\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0948 - accuracy: 0.3667 - val_loss: 1.0934 - val_accuracy: 0.3500\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 5s 3s/step - loss: 1.0906 - accuracy: 0.3611 - val_loss: 1.0920 - val_accuracy: 0.3833\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0986 - accuracy: 0.3167 - val_loss: 1.0895 - val_accuracy: 0.4167\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 498ms/step - loss: 1.0971 - accuracy: 0.2667\n",
            "Test accuracy on subset: 0.2666666805744171\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 5s 1s/step - loss: 1.1916 - accuracy: 0.3333 - val_loss: 1.1343 - val_accuracy: 0.3500\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1329 - accuracy: 0.2944 - val_loss: 1.0989 - val_accuracy: 0.3500\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 973ms/step - loss: 1.0987 - accuracy: 0.3333 - val_loss: 1.0987 - val_accuracy: 0.3667\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0990 - accuracy: 0.3556 - val_loss: 1.0982 - val_accuracy: 0.3833\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0987 - accuracy: 0.3500 - val_loss: 1.0957 - val_accuracy: 0.4167\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 899ms/step - loss: 1.0958 - accuracy: 0.3444 - val_loss: 1.0912 - val_accuracy: 0.3333\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0863 - accuracy: 0.4167 - val_loss: 1.0825 - val_accuracy: 0.3167\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0836 - accuracy: 0.3889 - val_loss: 1.0899 - val_accuracy: 0.3167\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 3s 985ms/step - loss: 1.0997 - accuracy: 0.3556 - val_loss: 1.0847 - val_accuracy: 0.4167\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 3s 997ms/step - loss: 1.0766 - accuracy: 0.3944 - val_loss: 1.0944 - val_accuracy: 0.4333\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 294ms/step - loss: 1.0851 - accuracy: 0.3833\n",
            "Test accuracy on subset: 0.38333332538604736\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 6s 3s/step - loss: 2.1132 - accuracy: 0.3333 - val_loss: 1.0965 - val_accuracy: 0.4333\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1109 - accuracy: 0.3167 - val_loss: 1.1011 - val_accuracy: 0.3333\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0985 - accuracy: 0.3389 - val_loss: 1.0987 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0987 - accuracy: 0.3333 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 466ms/step - loss: 1.0941 - accuracy: 0.3833\n",
            "Test accuracy on subset: 0.38333332538604736\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 5s 1s/step - loss: 1.4386 - accuracy: 0.2944 - val_loss: 1.1535 - val_accuracy: 0.3500\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 943ms/step - loss: 1.1680 - accuracy: 0.3111 - val_loss: 1.1022 - val_accuracy: 0.3167\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0946 - accuracy: 0.3944 - val_loss: 1.0988 - val_accuracy: 0.3167\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0983 - accuracy: 0.3500 - val_loss: 1.0985 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0986 - accuracy: 0.3500 - val_loss: 1.0976 - val_accuracy: 0.3167\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0987 - accuracy: 0.3222 - val_loss: 1.0968 - val_accuracy: 0.3333\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 3s 926ms/step - loss: 1.0975 - accuracy: 0.3667 - val_loss: 1.0954 - val_accuracy: 0.3333\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0912 - accuracy: 0.3889 - val_loss: 1.0916 - val_accuracy: 0.3500\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 3s 944ms/step - loss: 1.0939 - accuracy: 0.3778 - val_loss: 1.0880 - val_accuracy: 0.3667\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 5s 3s/step - loss: 1.0855 - accuracy: 0.3444 - val_loss: 1.0860 - val_accuracy: 0.4167\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 278ms/step - loss: 1.0961 - accuracy: 0.3667\n",
            "Test accuracy on subset: 0.36666667461395264\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.3362 - accuracy: 0.2833 - val_loss: 1.1206 - val_accuracy: 0.3333\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 924ms/step - loss: 1.1068 - accuracy: 0.4111 - val_loss: 1.0964 - val_accuracy: 0.3500\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 5s 3s/step - loss: 1.0982 - accuracy: 0.3667 - val_loss: 1.0977 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0971 - accuracy: 0.3556 - val_loss: 1.0970 - val_accuracy: 0.3500\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0967 - accuracy: 0.3722 - val_loss: 1.0934 - val_accuracy: 0.4667\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0980 - accuracy: 0.3056 - val_loss: 1.0920 - val_accuracy: 0.4333\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 5s 3s/step - loss: 1.0980 - accuracy: 0.3389 - val_loss: 1.0918 - val_accuracy: 0.4333\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0935 - accuracy: 0.3333 - val_loss: 1.0910 - val_accuracy: 0.4667\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0919 - accuracy: 0.3556 - val_loss: 1.0881 - val_accuracy: 0.4000\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0876 - accuracy: 0.3833 - val_loss: 1.0848 - val_accuracy: 0.3500\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 290ms/step - loss: 1.0725 - accuracy: 0.3667\n",
            "Test accuracy on subset: 0.36666667461395264\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 2.1710 - accuracy: 0.3444 - val_loss: 1.1144 - val_accuracy: 0.3000\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 944ms/step - loss: 1.1084 - accuracy: 0.3111 - val_loss: 1.0999 - val_accuracy: 0.2833\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0968 - accuracy: 0.3556 - val_loss: 1.0954 - val_accuracy: 0.3667\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1020 - accuracy: 0.3500 - val_loss: 1.0933 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 939ms/step - loss: 1.0936 - accuracy: 0.3611 - val_loss: 1.0899 - val_accuracy: 0.3500\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 936ms/step - loss: 1.0866 - accuracy: 0.3667 - val_loss: 1.0861 - val_accuracy: 0.3000\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0977 - accuracy: 0.3278 - val_loss: 1.0873 - val_accuracy: 0.3167\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 3s 929ms/step - loss: 1.0996 - accuracy: 0.3833 - val_loss: 1.0843 - val_accuracy: 0.3500\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 3s 942ms/step - loss: 1.0870 - accuracy: 0.3778 - val_loss: 1.0893 - val_accuracy: 0.3500\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 3s 936ms/step - loss: 1.0932 - accuracy: 0.3667 - val_loss: 1.0932 - val_accuracy: 0.4333\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 275ms/step - loss: 1.0854 - accuracy: 0.4667\n",
            "Test accuracy on subset: 0.46666666865348816\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.3235 - accuracy: 0.3389 - val_loss: 1.1058 - val_accuracy: 0.3667\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 941ms/step - loss: 1.1298 - accuracy: 0.3611 - val_loss: 1.0938 - val_accuracy: 0.3500\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0961 - accuracy: 0.3722 - val_loss: 1.1012 - val_accuracy: 0.3167\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0953 - accuracy: 0.3500 - val_loss: 1.1041 - val_accuracy: 0.3833\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 944ms/step - loss: 1.0873 - accuracy: 0.4111 - val_loss: 1.1061 - val_accuracy: 0.2667\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 275ms/step - loss: 1.0967 - accuracy: 0.3667\n",
            "Test accuracy on subset: 0.36666667461395264\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.2667 - accuracy: 0.3167 - val_loss: 1.1460 - val_accuracy: 0.3167\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1097 - accuracy: 0.3333 - val_loss: 1.0936 - val_accuracy: 0.4000\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 991ms/step - loss: 1.0951 - accuracy: 0.3389 - val_loss: 1.0981 - val_accuracy: 0.3167\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 983ms/step - loss: 1.0984 - accuracy: 0.3278 - val_loss: 1.0986 - val_accuracy: 0.3833\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0980 - accuracy: 0.3722 - val_loss: 1.0983 - val_accuracy: 0.3667\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 1s 500ms/step - loss: 1.0808 - accuracy: 0.4000\n",
            "Test accuracy on subset: 0.4000000059604645\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 1s/step - loss: 1.1371 - accuracy: 0.3444 - val_loss: 1.0797 - val_accuracy: 0.4000\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0995 - accuracy: 0.3889 - val_loss: 1.0897 - val_accuracy: 0.4500\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0923 - accuracy: 0.4444 - val_loss: 1.0860 - val_accuracy: 0.3500\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0988 - accuracy: 0.3778 - val_loss: 1.0845 - val_accuracy: 0.4167\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 1.1127 - accuracy: 0.3333\n",
            "Test accuracy on subset: 0.3333333432674408\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.2654 - accuracy: 0.3333 - val_loss: 1.2515 - val_accuracy: 0.3500\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.1977 - accuracy: 0.3611 - val_loss: 1.0996 - val_accuracy: 0.3333\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 924ms/step - loss: 1.0998 - accuracy: 0.3167 - val_loss: 1.0978 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0995 - accuracy: 0.2889 - val_loss: 1.0964 - val_accuracy: 0.3833\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 949ms/step - loss: 1.0980 - accuracy: 0.4278 - val_loss: 1.0954 - val_accuracy: 0.4833\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.0991 - accuracy: 0.3111 - val_loss: 1.0957 - val_accuracy: 0.4500\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 3s 961ms/step - loss: 1.0969 - accuracy: 0.3556 - val_loss: 1.0960 - val_accuracy: 0.4333\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0946 - accuracy: 0.3833 - val_loss: 1.0964 - val_accuracy: 0.3667\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 277ms/step - loss: 1.0964 - accuracy: 0.3500\n",
            "Test accuracy on subset: 0.3499999940395355\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 6s 1s/step - loss: 1.2582 - accuracy: 0.3389 - val_loss: 1.2038 - val_accuracy: 0.3500\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1380 - accuracy: 0.3056 - val_loss: 1.1005 - val_accuracy: 0.3333\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0972 - accuracy: 0.3444 - val_loss: 1.0988 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0987 - accuracy: 0.3333 - val_loss: 1.0985 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.0989 - accuracy: 0.3333 - val_loss: 1.0984 - val_accuracy: 0.3333\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0986 - accuracy: 0.3333 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 3s 1s/step - loss: 1.0985 - accuracy: 0.3333 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0989 - accuracy: 0.3333 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 1.0986 - accuracy: 0.3333\n",
            "Test accuracy on subset: 0.3333333432674408\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.4185 - accuracy: 0.3222 - val_loss: 1.4328 - val_accuracy: 0.3500\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.1924 - accuracy: 0.3556 - val_loss: 1.1030 - val_accuracy: 0.3167\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 969ms/step - loss: 1.0991 - accuracy: 0.3278 - val_loss: 1.0985 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 963ms/step - loss: 1.0986 - accuracy: 0.3000 - val_loss: 1.0985 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0991 - accuracy: 0.2889 - val_loss: 1.0984 - val_accuracy: 0.3333\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 5s 3s/step - loss: 1.0985 - accuracy: 0.3278 - val_loss: 1.0984 - val_accuracy: 0.3333\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 3s 949ms/step - loss: 1.0985 - accuracy: 0.3278 - val_loss: 1.0981 - val_accuracy: 0.3167\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0990 - accuracy: 0.3056 - val_loss: 1.0983 - val_accuracy: 0.3333\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0990 - accuracy: 0.2667 - val_loss: 1.0979 - val_accuracy: 0.3167\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0976 - accuracy: 0.3556 - val_loss: 1.0973 - val_accuracy: 0.3167\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 277ms/step - loss: 1.0984 - accuracy: 0.2500\n",
            "Test accuracy on subset: 0.25\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 1s/step - loss: 1.2193 - accuracy: 0.2889 - val_loss: 1.1921 - val_accuracy: 0.3167\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 1s/step - loss: 1.1152 - accuracy: 0.3778 - val_loss: 1.1013 - val_accuracy: 0.3333\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0982 - accuracy: 0.3278 - val_loss: 1.0992 - val_accuracy: 0.2667\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0993 - accuracy: 0.3500 - val_loss: 1.1003 - val_accuracy: 0.3167\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0987 - accuracy: 0.3667 - val_loss: 1.1025 - val_accuracy: 0.2667\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0977 - accuracy: 0.3778 - val_loss: 1.1036 - val_accuracy: 0.3000\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 1.0992 - accuracy: 0.3333\n",
            "Test accuracy on subset: 0.3333333432674408\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 6s 1s/step - loss: 1.2243 - accuracy: 0.2667 - val_loss: 1.1021 - val_accuracy: 0.3333\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 972ms/step - loss: 1.1091 - accuracy: 0.3389 - val_loss: 1.0973 - val_accuracy: 0.3667\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0983 - accuracy: 0.3667 - val_loss: 1.0985 - val_accuracy: 0.3167\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0980 - accuracy: 0.3722 - val_loss: 1.0975 - val_accuracy: 0.3667\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0978 - accuracy: 0.3667 - val_loss: 1.0964 - val_accuracy: 0.3667\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0976 - accuracy: 0.3833 - val_loss: 1.0957 - val_accuracy: 0.4000\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0920 - accuracy: 0.3556 - val_loss: 1.0940 - val_accuracy: 0.3000\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.1017 - accuracy: 0.3278 - val_loss: 1.0921 - val_accuracy: 0.3167\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0959 - accuracy: 0.3778 - val_loss: 1.0959 - val_accuracy: 0.3333\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0895 - accuracy: 0.3611 - val_loss: 1.1000 - val_accuracy: 0.3500\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 278ms/step - loss: 1.0963 - accuracy: 0.3833\n",
            "Test accuracy on subset: 0.38333332538604736\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 6s 3s/step - loss: 1.2290 - accuracy: 0.2833 - val_loss: 1.1135 - val_accuracy: 0.4167\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1065 - accuracy: 0.3444 - val_loss: 1.0956 - val_accuracy: 0.4167\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 961ms/step - loss: 1.0977 - accuracy: 0.3222 - val_loss: 1.0985 - val_accuracy: 0.3500\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0986 - accuracy: 0.3278 - val_loss: 1.0985 - val_accuracy: 0.3667\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0975 - accuracy: 0.3667 - val_loss: 1.0985 - val_accuracy: 0.3333\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 1.0982 - accuracy: 0.3333\n",
            "Test accuracy on subset: 0.3333333432674408\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.5101 - accuracy: 0.3111 - val_loss: 1.2906 - val_accuracy: 0.3667\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.1419 - accuracy: 0.3556 - val_loss: 1.1017 - val_accuracy: 0.3167\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 4s 938ms/step - loss: 1.1003 - accuracy: 0.3000 - val_loss: 1.0987 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 4s 1s/step - loss: 1.0987 - accuracy: 0.3556 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 4s 1s/step - loss: 1.0989 - accuracy: 0.3056 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 923ms/step - loss: 1.0987 - accuracy: 0.3444 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0989 - accuracy: 0.3444 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 280ms/step - loss: 1.0986 - accuracy: 0.3500\n",
            "Test accuracy on subset: 0.3499999940395355\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 5s 3s/step - loss: 2.1140 - accuracy: 0.3000 - val_loss: 1.0945 - val_accuracy: 0.3167\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 944ms/step - loss: 1.1112 - accuracy: 0.2944 - val_loss: 1.0961 - val_accuracy: 0.3667\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0978 - accuracy: 0.4000 - val_loss: 1.0984 - val_accuracy: 0.4000\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0985 - accuracy: 0.3611 - val_loss: 1.0986 - val_accuracy: 0.2833\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 277ms/step - loss: 1.0865 - accuracy: 0.3167\n",
            "Test accuracy on subset: 0.3166666626930237\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.2405 - accuracy: 0.3000 - val_loss: 1.1696 - val_accuracy: 0.3000\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1163 - accuracy: 0.3944 - val_loss: 1.1024 - val_accuracy: 0.3833\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0991 - accuracy: 0.3389 - val_loss: 1.0995 - val_accuracy: 0.2000\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0988 - accuracy: 0.3278 - val_loss: 1.0988 - val_accuracy: 0.2167\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 944ms/step - loss: 1.0978 - accuracy: 0.3500 - val_loss: 1.0987 - val_accuracy: 0.3333\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0990 - accuracy: 0.3167 - val_loss: 1.0988 - val_accuracy: 0.2667\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0995 - accuracy: 0.3056 - val_loss: 1.0988 - val_accuracy: 0.3333\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0986 - accuracy: 0.3333 - val_loss: 1.0987 - val_accuracy: 0.3333\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0988 - accuracy: 0.3333 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0992 - accuracy: 0.3333 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 1.0990 - accuracy: 0.3333\n",
            "Test accuracy on subset: 0.3333333432674408\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 1s/step - loss: 1.1408 - accuracy: 0.2833 - val_loss: 1.0963 - val_accuracy: 0.3667\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0928 - accuracy: 0.3722 - val_loss: 1.0980 - val_accuracy: 0.3500\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0994 - accuracy: 0.3389 - val_loss: 1.0982 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0986 - accuracy: 0.3278 - val_loss: 1.0960 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0984 - accuracy: 0.3500 - val_loss: 1.0941 - val_accuracy: 0.3500\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0967 - accuracy: 0.3500 - val_loss: 1.0899 - val_accuracy: 0.3833\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 3s 949ms/step - loss: 1.0927 - accuracy: 0.3778 - val_loss: 1.0888 - val_accuracy: 0.3667\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0799 - accuracy: 0.3944 - val_loss: 1.0745 - val_accuracy: 0.4167\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0508 - accuracy: 0.4222 - val_loss: 1.0560 - val_accuracy: 0.4833\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 3s 959ms/step - loss: 1.0844 - accuracy: 0.4444 - val_loss: 1.0409 - val_accuracy: 0.5167\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 279ms/step - loss: 1.0663 - accuracy: 0.3833\n",
            "Test accuracy on subset: 0.38333332538604736\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.2779 - accuracy: 0.3444 - val_loss: 1.0968 - val_accuracy: 0.3500\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1017 - accuracy: 0.3944 - val_loss: 1.1037 - val_accuracy: 0.4167\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 935ms/step - loss: 1.0924 - accuracy: 0.3778 - val_loss: 1.1012 - val_accuracy: 0.3667\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0997 - accuracy: 0.3722 - val_loss: 1.0945 - val_accuracy: 0.4000\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 928ms/step - loss: 1.0939 - accuracy: 0.3944 - val_loss: 1.0959 - val_accuracy: 0.3500\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 935ms/step - loss: 1.0964 - accuracy: 0.3889 - val_loss: 1.0974 - val_accuracy: 0.3500\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 3s 975ms/step - loss: 1.0907 - accuracy: 0.4278 - val_loss: 1.0993 - val_accuracy: 0.3500\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 1.0971 - accuracy: 0.3333\n",
            "Test accuracy on subset: 0.3333333432674408\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.9564 - accuracy: 0.3278 - val_loss: 1.2677 - val_accuracy: 0.3667\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 958ms/step - loss: 1.3326 - accuracy: 0.2889 - val_loss: 1.0976 - val_accuracy: 0.3500\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0988 - accuracy: 0.3556 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0989 - accuracy: 0.2944 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 981ms/step - loss: 1.0986 - accuracy: 0.3389 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 1.0973 - accuracy: 0.4500\n",
            "Test accuracy on subset: 0.44999998807907104\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 1s/step - loss: 1.1893 - accuracy: 0.2833 - val_loss: 1.1842 - val_accuracy: 0.3667\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.1051 - accuracy: 0.3333 - val_loss: 1.0990 - val_accuracy: 0.3500\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 995ms/step - loss: 1.0991 - accuracy: 0.3389 - val_loss: 1.0987 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0973 - accuracy: 0.3389 - val_loss: 1.0994 - val_accuracy: 0.3000\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1005 - accuracy: 0.3444 - val_loss: 1.1023 - val_accuracy: 0.3167\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1008 - accuracy: 0.3611 - val_loss: 1.1001 - val_accuracy: 0.2833\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 279ms/step - loss: 1.0982 - accuracy: 0.3333\n",
            "Test accuracy on subset: 0.3333333432674408\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 1s/step - loss: 1.1113 - accuracy: 0.3056 - val_loss: 1.2530 - val_accuracy: 0.3000\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.1386 - accuracy: 0.2833 - val_loss: 1.1108 - val_accuracy: 0.2667\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 4s 949ms/step - loss: 1.0864 - accuracy: 0.3944 - val_loss: 1.1026 - val_accuracy: 0.3167\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0986 - accuracy: 0.3056 - val_loss: 1.0999 - val_accuracy: 0.3667\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0922 - accuracy: 0.3944 - val_loss: 1.1014 - val_accuracy: 0.3667\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0879 - accuracy: 0.3389 - val_loss: 1.1078 - val_accuracy: 0.3833\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 3s 969ms/step - loss: 1.0801 - accuracy: 0.4222 - val_loss: 1.1259 - val_accuracy: 0.3667\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 272ms/step - loss: 1.0917 - accuracy: 0.3833\n",
            "Test accuracy on subset: 0.38333332538604736\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.6250 - accuracy: 0.2944 - val_loss: 1.2028 - val_accuracy: 0.3000\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 5s 3s/step - loss: 1.1139 - accuracy: 0.3167 - val_loss: 1.0961 - val_accuracy: 0.2833\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 971ms/step - loss: 1.0976 - accuracy: 0.3500 - val_loss: 1.0982 - val_accuracy: 0.4000\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0985 - accuracy: 0.3500 - val_loss: 1.0983 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0985 - accuracy: 0.3722 - val_loss: 1.0990 - val_accuracy: 0.4000\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 375ms/step - loss: 1.0970 - accuracy: 0.3167\n",
            "Test accuracy on subset: 0.3166666626930237\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 1s/step - loss: 1.1304 - accuracy: 0.3167 - val_loss: 1.0862 - val_accuracy: 0.4333\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 969ms/step - loss: 1.1084 - accuracy: 0.3278 - val_loss: 1.0970 - val_accuracy: 0.3500\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0987 - accuracy: 0.3333 - val_loss: 1.0969 - val_accuracy: 0.3667\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 4s 1s/step - loss: 1.0979 - accuracy: 0.3222 - val_loss: 1.0955 - val_accuracy: 0.4000\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 1.0753 - accuracy: 0.4000\n",
            "Test accuracy on subset: 0.4000000059604645\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.9251 - accuracy: 0.3056 - val_loss: 1.1446 - val_accuracy: 0.3667\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1276 - accuracy: 0.3444 - val_loss: 1.1000 - val_accuracy: 0.3000\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 5s 1s/step - loss: 1.0978 - accuracy: 0.3778 - val_loss: 1.0987 - val_accuracy: 0.3500\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0986 - accuracy: 0.3444 - val_loss: 1.0987 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 963ms/step - loss: 1.0986 - accuracy: 0.3333 - val_loss: 1.0985 - val_accuracy: 0.3333\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 5s 3s/step - loss: 1.0984 - accuracy: 0.3389 - val_loss: 1.0974 - val_accuracy: 0.3500\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 4s 1s/step - loss: 1.0982 - accuracy: 0.3222 - val_loss: 1.0924 - val_accuracy: 0.3833\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 3s 980ms/step - loss: 1.0897 - accuracy: 0.4444 - val_loss: 1.0826 - val_accuracy: 0.3333\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0931 - accuracy: 0.3556 - val_loss: 1.0818 - val_accuracy: 0.3833\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0878 - accuracy: 0.3833 - val_loss: 1.0927 - val_accuracy: 0.3667\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 1.0845 - accuracy: 0.3333\n",
            "Test accuracy on subset: 0.3333333432674408\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 1s/step - loss: 1.1062 - accuracy: 0.3667 - val_loss: 1.1593 - val_accuracy: 0.3333\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1105 - accuracy: 0.3667 - val_loss: 1.1012 - val_accuracy: 0.3500\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0993 - accuracy: 0.3500 - val_loss: 1.0949 - val_accuracy: 0.4500\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 935ms/step - loss: 1.0954 - accuracy: 0.3778 - val_loss: 1.0926 - val_accuracy: 0.3833\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0896 - accuracy: 0.4278 - val_loss: 1.0989 - val_accuracy: 0.2833\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 4s 961ms/step - loss: 1.1138 - accuracy: 0.3778 - val_loss: 1.0936 - val_accuracy: 0.3000\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 3s 1s/step - loss: 1.0825 - accuracy: 0.3833 - val_loss: 1.0966 - val_accuracy: 0.3000\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 1.1048 - accuracy: 0.3000\n",
            "Test accuracy on subset: 0.30000001192092896\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 1s/step - loss: 1.1328 - accuracy: 0.3556 - val_loss: 1.1260 - val_accuracy: 0.4167\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 5s 1s/step - loss: 1.1240 - accuracy: 0.3611 - val_loss: 1.0970 - val_accuracy: 0.3833\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0932 - accuracy: 0.4222 - val_loss: 1.0961 - val_accuracy: 0.3833\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 930ms/step - loss: 1.0968 - accuracy: 0.3611 - val_loss: 1.0944 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.1003 - accuracy: 0.3444 - val_loss: 1.0936 - val_accuracy: 0.3333\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 1s/step - loss: 1.0924 - accuracy: 0.3444 - val_loss: 1.0971 - val_accuracy: 0.3500\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0776 - accuracy: 0.3944 - val_loss: 1.1051 - val_accuracy: 0.3333\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0621 - accuracy: 0.4833 - val_loss: 1.1042 - val_accuracy: 0.3500\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 315ms/step - loss: 1.0711 - accuracy: 0.4167\n",
            "Test accuracy on subset: 0.4166666567325592\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 2.5718 - accuracy: 0.3000 - val_loss: 1.3770 - val_accuracy: 0.3333\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 1s/step - loss: 1.2069 - accuracy: 0.3556 - val_loss: 1.1103 - val_accuracy: 0.3000\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.1039 - accuracy: 0.3667 - val_loss: 1.0996 - val_accuracy: 0.2500\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0983 - accuracy: 0.3389 - val_loss: 1.0989 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0988 - accuracy: 0.3333 - val_loss: 1.0988 - val_accuracy: 0.3333\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 1s/step - loss: 1.0983 - accuracy: 0.3333 - val_loss: 1.0988 - val_accuracy: 0.3333\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 5s 1s/step - loss: 1.0987 - accuracy: 0.3333 - val_loss: 1.0989 - val_accuracy: 0.3333\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0985 - accuracy: 0.3333 - val_loss: 1.0990 - val_accuracy: 0.3333\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 1.0985 - accuracy: 0.3333\n",
            "Test accuracy on subset: 0.3333333432674408\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.7798 - accuracy: 0.3444 - val_loss: 1.3582 - val_accuracy: 0.3500\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.1903 - accuracy: 0.3556 - val_loss: 1.1009 - val_accuracy: 0.3333\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1003 - accuracy: 0.3389 - val_loss: 1.0985 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 1s/step - loss: 1.0983 - accuracy: 0.3389 - val_loss: 1.0974 - val_accuracy: 0.3500\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 5s 4s/step - loss: 1.0988 - accuracy: 0.3611 - val_loss: 1.0974 - val_accuracy: 0.4167\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0973 - accuracy: 0.3722 - val_loss: 1.0963 - val_accuracy: 0.3333\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 3s 1s/step - loss: 1.0955 - accuracy: 0.3611 - val_loss: 1.0931 - val_accuracy: 0.3333\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.0949 - accuracy: 0.4167 - val_loss: 1.0883 - val_accuracy: 0.3333\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0850 - accuracy: 0.3667 - val_loss: 1.0826 - val_accuracy: 0.3667\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 3s 1s/step - loss: 1.0928 - accuracy: 0.3667 - val_loss: 1.0784 - val_accuracy: 0.4000\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 312ms/step - loss: 1.0767 - accuracy: 0.3667\n",
            "Test accuracy on subset: 0.36666667461395264\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 7s 2s/step - loss: 1.4064 - accuracy: 0.3556 - val_loss: 1.1899 - val_accuracy: 0.3000\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 1s/step - loss: 1.1365 - accuracy: 0.3111 - val_loss: 1.1003 - val_accuracy: 0.2500\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 1s/step - loss: 1.0969 - accuracy: 0.3611 - val_loss: 1.0986 - val_accuracy: 0.2833\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 5s 4s/step - loss: 1.0986 - accuracy: 0.3167 - val_loss: 1.0985 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0984 - accuracy: 0.3333 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0984 - accuracy: 0.3444 - val_loss: 1.0987 - val_accuracy: 0.3333\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 5s 3s/step - loss: 1.0982 - accuracy: 0.3333 - val_loss: 1.1000 - val_accuracy: 0.3500\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 1.0983 - accuracy: 0.3333\n",
            "Test accuracy on subset: 0.3333333432674408\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 1s/step - loss: 1.2364 - accuracy: 0.3611 - val_loss: 1.2007 - val_accuracy: 0.3000\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1231 - accuracy: 0.3222 - val_loss: 1.0995 - val_accuracy: 0.3167\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0978 - accuracy: 0.3444 - val_loss: 1.0989 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0981 - accuracy: 0.3389 - val_loss: 1.0988 - val_accuracy: 0.3167\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 1s/step - loss: 1.0991 - accuracy: 0.3222 - val_loss: 1.0989 - val_accuracy: 0.3000\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 5s 3s/step - loss: 1.0967 - accuracy: 0.3333 - val_loss: 1.1006 - val_accuracy: 0.3000\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0997 - accuracy: 0.3000 - val_loss: 1.0992 - val_accuracy: 0.2833\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 297ms/step - loss: 1.0982 - accuracy: 0.3500\n",
            "Test accuracy on subset: 0.3499999940395355\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 7s 2s/step - loss: 1.3691 - accuracy: 0.2722 - val_loss: 1.2319 - val_accuracy: 0.3333\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 1s/step - loss: 1.1613 - accuracy: 0.3167 - val_loss: 1.1006 - val_accuracy: 0.3500\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 4s 1s/step - loss: 1.0963 - accuracy: 0.3722 - val_loss: 1.0980 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 5s 3s/step - loss: 1.0988 - accuracy: 0.3333 - val_loss: 1.0976 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 1s/step - loss: 1.0982 - accuracy: 0.3389 - val_loss: 1.0978 - val_accuracy: 0.3167\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 1s/step - loss: 1.0972 - accuracy: 0.3556 - val_loss: 1.0973 - val_accuracy: 0.3500\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0973 - accuracy: 0.3722 - val_loss: 1.0938 - val_accuracy: 0.3667\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0958 - accuracy: 0.3556 - val_loss: 1.0921 - val_accuracy: 0.4000\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 3s 962ms/step - loss: 1.0972 - accuracy: 0.3778 - val_loss: 1.0896 - val_accuracy: 0.4000\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0878 - accuracy: 0.3389 - val_loss: 1.0825 - val_accuracy: 0.3833\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 459ms/step - loss: 1.0952 - accuracy: 0.3667\n",
            "Test accuracy on subset: 0.36666667461395264\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.5279 - accuracy: 0.3278 - val_loss: 1.2248 - val_accuracy: 0.3333\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1510 - accuracy: 0.3889 - val_loss: 1.1013 - val_accuracy: 0.3167\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0987 - accuracy: 0.3667 - val_loss: 1.0988 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.0987 - accuracy: 0.3278 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 962ms/step - loss: 1.0988 - accuracy: 0.3333 - val_loss: 1.0987 - val_accuracy: 0.3333\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0987 - accuracy: 0.3333 - val_loss: 1.0988 - val_accuracy: 0.3333\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0989 - accuracy: 0.3333 - val_loss: 1.0988 - val_accuracy: 0.3333\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 491ms/step - loss: 1.0986 - accuracy: 0.3333\n",
            "Test accuracy on subset: 0.3333333432674408\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 1s/step - loss: 1.1308 - accuracy: 0.4056 - val_loss: 1.1635 - val_accuracy: 0.4000\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1528 - accuracy: 0.3444 - val_loss: 1.0990 - val_accuracy: 0.3333\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0986 - accuracy: 0.3611 - val_loss: 1.0985 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0985 - accuracy: 0.3444 - val_loss: 1.0985 - val_accuracy: 0.3500\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0986 - accuracy: 0.3611 - val_loss: 1.0986 - val_accuracy: 0.3667\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0985 - accuracy: 0.3333 - val_loss: 1.0987 - val_accuracy: 0.3500\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 286ms/step - loss: 1.0985 - accuracy: 0.3333\n",
            "Test accuracy on subset: 0.3333333432674408\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 5s 1s/step - loss: 1.3780 - accuracy: 0.3500 - val_loss: 1.2907 - val_accuracy: 0.3333\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1498 - accuracy: 0.3333 - val_loss: 1.1015 - val_accuracy: 0.3500\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 5s 4s/step - loss: 1.1004 - accuracy: 0.3667 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 892ms/step - loss: 1.0986 - accuracy: 0.3722 - val_loss: 1.0987 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0989 - accuracy: 0.3278 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 979ms/step - loss: 1.0986 - accuracy: 0.3333 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 273ms/step - loss: 1.0987 - accuracy: 0.3333\n",
            "Test accuracy on subset: 0.3333333432674408\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 5s 1s/step - loss: 1.1439 - accuracy: 0.3444 - val_loss: 1.1414 - val_accuracy: 0.4167\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 986ms/step - loss: 1.0926 - accuracy: 0.3889 - val_loss: 1.0930 - val_accuracy: 0.3500\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 983ms/step - loss: 1.0949 - accuracy: 0.4000 - val_loss: 1.0914 - val_accuracy: 0.4000\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 4s 972ms/step - loss: 1.0885 - accuracy: 0.4444 - val_loss: 1.0870 - val_accuracy: 0.3500\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 921ms/step - loss: 1.0919 - accuracy: 0.3944 - val_loss: 1.0836 - val_accuracy: 0.3000\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 943ms/step - loss: 1.0894 - accuracy: 0.3889 - val_loss: 1.0792 - val_accuracy: 0.3333\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 5s 3s/step - loss: 1.0786 - accuracy: 0.4111 - val_loss: 1.0714 - val_accuracy: 0.3833\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 3s 933ms/step - loss: 1.0511 - accuracy: 0.5111 - val_loss: 1.0689 - val_accuracy: 0.4333\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 3s 938ms/step - loss: 1.0518 - accuracy: 0.3889 - val_loss: 1.0737 - val_accuracy: 0.4167\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 4s 925ms/step - loss: 1.0831 - accuracy: 0.4111 - val_loss: 1.0667 - val_accuracy: 0.4833\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 331ms/step - loss: 1.0207 - accuracy: 0.4833\n",
            "Test accuracy on subset: 0.4833333194255829\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 1s/step - loss: 1.1791 - accuracy: 0.3111 - val_loss: 1.0876 - val_accuracy: 0.4000\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0998 - accuracy: 0.3667 - val_loss: 1.0985 - val_accuracy: 0.3333\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 1s/step - loss: 1.0967 - accuracy: 0.3778 - val_loss: 1.0989 - val_accuracy: 0.3500\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0973 - accuracy: 0.3667 - val_loss: 1.0983 - val_accuracy: 0.4000\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 281ms/step - loss: 1.0832 - accuracy: 0.3167\n",
            "Test accuracy on subset: 0.3166666626930237\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 5s 3s/step - loss: 1.2326 - accuracy: 0.3111 - val_loss: 1.1499 - val_accuracy: 0.2833\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1030 - accuracy: 0.3556 - val_loss: 1.1052 - val_accuracy: 0.3000\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 935ms/step - loss: 1.0980 - accuracy: 0.3722 - val_loss: 1.0997 - val_accuracy: 0.3833\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 5s 3s/step - loss: 1.0982 - accuracy: 0.3611 - val_loss: 1.0990 - val_accuracy: 0.2833\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 932ms/step - loss: 1.0968 - accuracy: 0.4222 - val_loss: 1.0999 - val_accuracy: 0.3333\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 922ms/step - loss: 1.0941 - accuracy: 0.3778 - val_loss: 1.1030 - val_accuracy: 0.3333\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 3s 945ms/step - loss: 1.0917 - accuracy: 0.3944 - val_loss: 1.1068 - val_accuracy: 0.3333\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 1.0970 - accuracy: 0.3667\n",
            "Test accuracy on subset: 0.36666667461395264\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 7s 4s/step - loss: 1.2334 - accuracy: 0.3667 - val_loss: 1.1491 - val_accuracy: 0.3167\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1050 - accuracy: 0.3611 - val_loss: 1.0995 - val_accuracy: 0.4000\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0964 - accuracy: 0.3111 - val_loss: 1.0955 - val_accuracy: 0.3667\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 970ms/step - loss: 1.0960 - accuracy: 0.3667 - val_loss: 1.0934 - val_accuracy: 0.4667\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 955ms/step - loss: 1.0936 - accuracy: 0.4056 - val_loss: 1.0898 - val_accuracy: 0.4000\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0911 - accuracy: 0.3889 - val_loss: 1.0854 - val_accuracy: 0.4500\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 5s 3s/step - loss: 1.0901 - accuracy: 0.4278 - val_loss: 1.0796 - val_accuracy: 0.4333\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0928 - accuracy: 0.3444 - val_loss: 1.0787 - val_accuracy: 0.4000\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0836 - accuracy: 0.4111 - val_loss: 1.0758 - val_accuracy: 0.4667\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0787 - accuracy: 0.4056 - val_loss: 1.0697 - val_accuracy: 0.3833\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 1s 531ms/step - loss: 1.0845 - accuracy: 0.3833\n",
            "Test accuracy on subset: 0.38333332538604736\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 1s/step - loss: 1.2789 - accuracy: 0.3667 - val_loss: 1.2661 - val_accuracy: 0.3500\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1441 - accuracy: 0.3222 - val_loss: 1.1016 - val_accuracy: 0.4000\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0982 - accuracy: 0.3222 - val_loss: 1.0987 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0987 - accuracy: 0.3222 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 1s/step - loss: 1.0987 - accuracy: 0.3333 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 4s 963ms/step - loss: 1.0989 - accuracy: 0.3333 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 3s 931ms/step - loss: 1.0991 - accuracy: 0.3333 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 3s 1s/step - loss: 1.0991 - accuracy: 0.3333 - val_loss: 1.0987 - val_accuracy: 0.3333\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 5s 1s/step - loss: 1.0984 - accuracy: 0.3333 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0986 - accuracy: 0.3333 - val_loss: 1.0985 - val_accuracy: 0.3333\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 1.0984 - accuracy: 0.3333\n",
            "Test accuracy on subset: 0.3333333432674408\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.3155 - accuracy: 0.3167 - val_loss: 1.1289 - val_accuracy: 0.3500\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 962ms/step - loss: 1.1235 - accuracy: 0.3444 - val_loss: 1.0978 - val_accuracy: 0.4500\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 930ms/step - loss: 1.1000 - accuracy: 0.2778 - val_loss: 1.0987 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0987 - accuracy: 0.3278 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 4s 946ms/step - loss: 1.0984 - accuracy: 0.3500 - val_loss: 1.0986 - val_accuracy: 0.3167\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 1.0994 - accuracy: 0.3667\n",
            "Test accuracy on subset: 0.36666667461395264\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.3987 - accuracy: 0.3167 - val_loss: 1.1041 - val_accuracy: 0.3500\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1145 - accuracy: 0.3333 - val_loss: 1.0973 - val_accuracy: 0.3500\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 991ms/step - loss: 1.0963 - accuracy: 0.3833 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 991ms/step - loss: 1.0982 - accuracy: 0.3333 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0981 - accuracy: 0.3333 - val_loss: 1.0981 - val_accuracy: 0.3333\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 272ms/step - loss: 1.0988 - accuracy: 0.3833\n",
            "Test accuracy on subset: 0.38333332538604736\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 1s/step - loss: 1.3657 - accuracy: 0.3667 - val_loss: 1.2861 - val_accuracy: 0.3667\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1600 - accuracy: 0.3389 - val_loss: 1.0998 - val_accuracy: 0.3500\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.0990 - accuracy: 0.3500 - val_loss: 1.0985 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 956ms/step - loss: 1.0984 - accuracy: 0.3111 - val_loss: 1.0987 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0987 - accuracy: 0.3500 - val_loss: 1.0987 - val_accuracy: 0.2833\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0987 - accuracy: 0.3056 - val_loss: 1.0987 - val_accuracy: 0.4000\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 271ms/step - loss: 1.0987 - accuracy: 0.3333\n",
            "Test accuracy on subset: 0.3333333432674408\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.3587 - accuracy: 0.3444 - val_loss: 1.1473 - val_accuracy: 0.4000\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.1022 - accuracy: 0.3667 - val_loss: 1.0975 - val_accuracy: 0.3667\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 4s 1s/step - loss: 1.0988 - accuracy: 0.3556 - val_loss: 1.0985 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0982 - accuracy: 0.3278 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0987 - accuracy: 0.3333 - val_loss: 1.0984 - val_accuracy: 0.3333\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 442ms/step - loss: 1.0958 - accuracy: 0.3667\n",
            "Test accuracy on subset: 0.36666667461395264\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 1s/step - loss: 1.1416 - accuracy: 0.2944 - val_loss: 1.1449 - val_accuracy: 0.3500\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1090 - accuracy: 0.3389 - val_loss: 1.1005 - val_accuracy: 0.3500\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.1043 - accuracy: 0.3167 - val_loss: 1.0998 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0924 - accuracy: 0.3889 - val_loss: 1.0968 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0989 - accuracy: 0.3722 - val_loss: 1.0959 - val_accuracy: 0.4333\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1005 - accuracy: 0.3500 - val_loss: 1.0990 - val_accuracy: 0.3333\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 5s 4s/step - loss: 1.0958 - accuracy: 0.3278 - val_loss: 1.0973 - val_accuracy: 0.3833\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 4s 978ms/step - loss: 1.0950 - accuracy: 0.3556 - val_loss: 1.0909 - val_accuracy: 0.3833\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 3s 975ms/step - loss: 1.0843 - accuracy: 0.3944 - val_loss: 1.0813 - val_accuracy: 0.3333\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0968 - accuracy: 0.3778 - val_loss: 1.0850 - val_accuracy: 0.3333\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 1.0832 - accuracy: 0.3167\n",
            "Test accuracy on subset: 0.3166666626930237\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 1s/step - loss: 1.2268 - accuracy: 0.3333 - val_loss: 1.1690 - val_accuracy: 0.3667\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1350 - accuracy: 0.3222 - val_loss: 1.0979 - val_accuracy: 0.4167\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0995 - accuracy: 0.3111 - val_loss: 1.0987 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 4s 1s/step - loss: 1.0986 - accuracy: 0.3333 - val_loss: 1.0987 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 1s/step - loss: 1.0989 - accuracy: 0.3333 - val_loss: 1.0987 - val_accuracy: 0.3333\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 279ms/step - loss: 1.0982 - accuracy: 0.4333\n",
            "Test accuracy on subset: 0.4333333373069763\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.1757 - accuracy: 0.3667 - val_loss: 1.2232 - val_accuracy: 0.3500\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1449 - accuracy: 0.3389 - val_loss: 1.0998 - val_accuracy: 0.3333\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0983 - accuracy: 0.3333 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0988 - accuracy: 0.3333 - val_loss: 1.0992 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.0991 - accuracy: 0.3333 - val_loss: 1.0992 - val_accuracy: 0.3333\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 917ms/step - loss: 1.0981 - accuracy: 0.3333 - val_loss: 1.0988 - val_accuracy: 0.3167\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 1.0986 - accuracy: 0.3333\n",
            "Test accuracy on subset: 0.3333333432674408\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.4112 - accuracy: 0.3778 - val_loss: 1.5081 - val_accuracy: 0.3167\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.2429 - accuracy: 0.3389 - val_loss: 1.0991 - val_accuracy: 0.3833\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.1001 - accuracy: 0.3333 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0987 - accuracy: 0.3333 - val_loss: 1.0985 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 967ms/step - loss: 1.0983 - accuracy: 0.3333 - val_loss: 1.0965 - val_accuracy: 0.3667\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0986 - accuracy: 0.3667 - val_loss: 1.0945 - val_accuracy: 0.3167\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0973 - accuracy: 0.3389 - val_loss: 1.0935 - val_accuracy: 0.3333\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 3s 931ms/step - loss: 1.0927 - accuracy: 0.3389 - val_loss: 1.0916 - val_accuracy: 0.3333\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 4s 938ms/step - loss: 1.0915 - accuracy: 0.3556 - val_loss: 1.0877 - val_accuracy: 0.3333\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0992 - accuracy: 0.3111 - val_loss: 1.0876 - val_accuracy: 0.3333\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 286ms/step - loss: 1.0925 - accuracy: 0.2833\n",
            "Test accuracy on subset: 0.28333333134651184\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.7939 - accuracy: 0.3778 - val_loss: 1.6456 - val_accuracy: 0.2833\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.1860 - accuracy: 0.3944 - val_loss: 1.1039 - val_accuracy: 0.4500\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 983ms/step - loss: 1.1034 - accuracy: 0.2833 - val_loss: 1.0988 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 944ms/step - loss: 1.0987 - accuracy: 0.3333 - val_loss: 1.0987 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0984 - accuracy: 0.3333 - val_loss: 1.0988 - val_accuracy: 0.3333\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0984 - accuracy: 0.3333 - val_loss: 1.0988 - val_accuracy: 0.3333\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0990 - accuracy: 0.3333 - val_loss: 1.0987 - val_accuracy: 0.3333\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0987 - accuracy: 0.3333 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0985 - accuracy: 0.3333 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 3s 958ms/step - loss: 1.0989 - accuracy: 0.3500 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 1.0986 - accuracy: 0.3333\n",
            "Test accuracy on subset: 0.3333333432674408\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.2861 - accuracy: 0.3889 - val_loss: 1.1348 - val_accuracy: 0.3500\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.1453 - accuracy: 0.3389 - val_loss: 1.0963 - val_accuracy: 0.3833\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.0998 - accuracy: 0.3389 - val_loss: 1.0981 - val_accuracy: 0.3667\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 997ms/step - loss: 1.0979 - accuracy: 0.3833 - val_loss: 1.0970 - val_accuracy: 0.3833\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 4s 961ms/step - loss: 1.0990 - accuracy: 0.3667 - val_loss: 1.0962 - val_accuracy: 0.3333\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0936 - accuracy: 0.3667 - val_loss: 1.0946 - val_accuracy: 0.3333\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 3s 985ms/step - loss: 1.0951 - accuracy: 0.3278 - val_loss: 1.0893 - val_accuracy: 0.3833\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0990 - accuracy: 0.3222 - val_loss: 1.0872 - val_accuracy: 0.3833\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0952 - accuracy: 0.3611 - val_loss: 1.0872 - val_accuracy: 0.4000\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0923 - accuracy: 0.3500 - val_loss: 1.0892 - val_accuracy: 0.3833\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 279ms/step - loss: 1.0944 - accuracy: 0.3833\n",
            "Test accuracy on subset: 0.38333332538604736\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.1626 - accuracy: 0.3222 - val_loss: 1.1396 - val_accuracy: 0.3333\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1133 - accuracy: 0.3500 - val_loss: 1.0982 - val_accuracy: 0.3500\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0978 - accuracy: 0.3556 - val_loss: 1.0981 - val_accuracy: 0.3667\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 4s 1s/step - loss: 1.0984 - accuracy: 0.3333 - val_loss: 1.0981 - val_accuracy: 0.3667\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 991ms/step - loss: 1.0981 - accuracy: 0.3333 - val_loss: 1.0976 - val_accuracy: 0.3833\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0965 - accuracy: 0.3889 - val_loss: 1.0972 - val_accuracy: 0.3500\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 3s 3s/step - loss: 1.0963 - accuracy: 0.3611 - val_loss: 1.0991 - val_accuracy: 0.3333\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.0899 - accuracy: 0.3889 - val_loss: 1.1044 - val_accuracy: 0.3333\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0921 - accuracy: 0.4056 - val_loss: 1.1035 - val_accuracy: 0.3333\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 279ms/step - loss: 1.0957 - accuracy: 0.3833\n",
            "Test accuracy on subset: 0.38333332538604736\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 6s 3s/step - loss: 1.3450 - accuracy: 0.3389 - val_loss: 1.2523 - val_accuracy: 0.3333\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 986ms/step - loss: 1.1855 - accuracy: 0.3500 - val_loss: 1.1021 - val_accuracy: 0.1833\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0982 - accuracy: 0.3611 - val_loss: 1.0983 - val_accuracy: 0.3000\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 1s/step - loss: 1.0980 - accuracy: 0.4167 - val_loss: 1.0976 - val_accuracy: 0.3500\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0996 - accuracy: 0.3056 - val_loss: 1.0984 - val_accuracy: 0.3833\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 953ms/step - loss: 1.0987 - accuracy: 0.3333 - val_loss: 1.0984 - val_accuracy: 0.3667\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0992 - accuracy: 0.3056 - val_loss: 1.0978 - val_accuracy: 0.3833\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 279ms/step - loss: 1.0973 - accuracy: 0.4500\n",
            "Test accuracy on subset: 0.44999998807907104\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 5s 1s/step - loss: 1.2281 - accuracy: 0.3778 - val_loss: 1.3798 - val_accuracy: 0.4000\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.2178 - accuracy: 0.2833 - val_loss: 1.0978 - val_accuracy: 0.4333\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0987 - accuracy: 0.3722 - val_loss: 1.0985 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0986 - accuracy: 0.3111 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0984 - accuracy: 0.3222 - val_loss: 1.0985 - val_accuracy: 0.3333\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 276ms/step - loss: 1.0979 - accuracy: 0.4500\n",
            "Test accuracy on subset: 0.44999998807907104\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.1096 - accuracy: 0.3000 - val_loss: 1.1134 - val_accuracy: 0.4000\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 1s/step - loss: 1.1004 - accuracy: 0.3611 - val_loss: 1.1153 - val_accuracy: 0.3333\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 984ms/step - loss: 1.1008 - accuracy: 0.3611 - val_loss: 1.1023 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0976 - accuracy: 0.3667 - val_loss: 1.0953 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 973ms/step - loss: 1.0912 - accuracy: 0.4000 - val_loss: 1.0930 - val_accuracy: 0.3667\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0925 - accuracy: 0.3333 - val_loss: 1.0869 - val_accuracy: 0.3500\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 4s 887ms/step - loss: 1.0657 - accuracy: 0.4833 - val_loss: 1.0821 - val_accuracy: 0.3333\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0578 - accuracy: 0.4667 - val_loss: 1.0799 - val_accuracy: 0.3000\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0527 - accuracy: 0.4111 - val_loss: 1.0839 - val_accuracy: 0.3500\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0727 - accuracy: 0.3722 - val_loss: 1.0801 - val_accuracy: 0.3333\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 274ms/step - loss: 1.0424 - accuracy: 0.4000\n",
            "Test accuracy on subset: 0.4000000059604645\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.3331 - accuracy: 0.3389 - val_loss: 1.2884 - val_accuracy: 0.3000\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.1856 - accuracy: 0.3667 - val_loss: 1.0993 - val_accuracy: 0.3000\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0981 - accuracy: 0.3500 - val_loss: 1.0984 - val_accuracy: 0.3167\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0989 - accuracy: 0.2611 - val_loss: 1.0985 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0984 - accuracy: 0.3556 - val_loss: 1.0985 - val_accuracy: 0.3833\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0986 - accuracy: 0.3278 - val_loss: 1.0983 - val_accuracy: 0.3167\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 3s 956ms/step - loss: 1.0989 - accuracy: 0.2833 - val_loss: 1.0979 - val_accuracy: 0.3167\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 3s 1s/step - loss: 1.0983 - accuracy: 0.3278 - val_loss: 1.0977 - val_accuracy: 0.3333\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0977 - accuracy: 0.3389 - val_loss: 1.0990 - val_accuracy: 0.3667\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0963 - accuracy: 0.3444 - val_loss: 1.1064 - val_accuracy: 0.3833\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 309ms/step - loss: 1.0935 - accuracy: 0.3333\n",
            "Test accuracy on subset: 0.3333333432674408\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 6s 4s/step - loss: 2.9533 - accuracy: 0.2722 - val_loss: 1.2577 - val_accuracy: 0.3000\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1356 - accuracy: 0.3444 - val_loss: 1.0981 - val_accuracy: 0.3333\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.1008 - accuracy: 0.3056 - val_loss: 1.0985 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0995 - accuracy: 0.3333 - val_loss: 1.0980 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 944ms/step - loss: 1.0990 - accuracy: 0.3333 - val_loss: 1.0984 - val_accuracy: 0.3333\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0982 - accuracy: 0.3333 - val_loss: 1.0981 - val_accuracy: 0.3333\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0986 - accuracy: 0.3333 - val_loss: 1.0976 - val_accuracy: 0.3333\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0975 - accuracy: 0.3333 - val_loss: 1.0941 - val_accuracy: 0.3333\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0951 - accuracy: 0.3333 - val_loss: 1.0939 - val_accuracy: 0.3333\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 3s 938ms/step - loss: 1.0999 - accuracy: 0.3333 - val_loss: 1.0946 - val_accuracy: 0.3333\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 286ms/step - loss: 1.0911 - accuracy: 0.3333\n",
            "Test accuracy on subset: 0.3333333432674408\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.4778 - accuracy: 0.3333 - val_loss: 1.3101 - val_accuracy: 0.3167\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 5s 3s/step - loss: 1.1406 - accuracy: 0.3389 - val_loss: 1.1077 - val_accuracy: 0.3167\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0999 - accuracy: 0.3389 - val_loss: 1.0998 - val_accuracy: 0.3000\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 5s 3s/step - loss: 1.0982 - accuracy: 0.3444 - val_loss: 1.0987 - val_accuracy: 0.3000\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 4s 925ms/step - loss: 1.0988 - accuracy: 0.3278 - val_loss: 1.0987 - val_accuracy: 0.3333\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 964ms/step - loss: 1.0989 - accuracy: 0.3278 - val_loss: 1.0987 - val_accuracy: 0.3333\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0986 - accuracy: 0.3333 - val_loss: 1.0987 - val_accuracy: 0.3333\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0987 - accuracy: 0.3333 - val_loss: 1.0987 - val_accuracy: 0.3333\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 3s 999ms/step - loss: 1.0987 - accuracy: 0.3333 - val_loss: 1.0987 - val_accuracy: 0.3333\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 3s 989ms/step - loss: 1.0988 - accuracy: 0.3333 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 277ms/step - loss: 1.0986 - accuracy: 0.3333\n",
            "Test accuracy on subset: 0.3333333432674408\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 6s 3s/step - loss: 1.1956 - accuracy: 0.3111 - val_loss: 1.1305 - val_accuracy: 0.3333\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 946ms/step - loss: 1.1244 - accuracy: 0.3222 - val_loss: 1.1028 - val_accuracy: 0.3000\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1003 - accuracy: 0.3111 - val_loss: 1.1010 - val_accuracy: 0.3167\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 5s 3s/step - loss: 1.0980 - accuracy: 0.3333 - val_loss: 1.1012 - val_accuracy: 0.3667\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 941ms/step - loss: 1.0948 - accuracy: 0.4000 - val_loss: 1.1036 - val_accuracy: 0.3667\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0964 - accuracy: 0.3556 - val_loss: 1.1007 - val_accuracy: 0.3333\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0940 - accuracy: 0.3389 - val_loss: 1.0966 - val_accuracy: 0.3667\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 3s 954ms/step - loss: 1.0909 - accuracy: 0.3556 - val_loss: 1.0976 - val_accuracy: 0.3333\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0893 - accuracy: 0.3889 - val_loss: 1.0881 - val_accuracy: 0.3333\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0726 - accuracy: 0.3889 - val_loss: 1.0880 - val_accuracy: 0.3667\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 459ms/step - loss: 1.0821 - accuracy: 0.3833\n",
            "Test accuracy on subset: 0.38333332538604736\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.8444 - accuracy: 0.3389 - val_loss: 1.2974 - val_accuracy: 0.3667\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.2001 - accuracy: 0.3500 - val_loss: 1.0967 - val_accuracy: 0.4000\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0963 - accuracy: 0.3389 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 961ms/step - loss: 1.0986 - accuracy: 0.3500 - val_loss: 1.0988 - val_accuracy: 0.3667\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 979ms/step - loss: 1.0988 - accuracy: 0.3500 - val_loss: 1.0990 - val_accuracy: 0.1333\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 286ms/step - loss: 1.0972 - accuracy: 0.3167\n",
            "Test accuracy on subset: 0.3166666626930237\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.2413 - accuracy: 0.3000 - val_loss: 1.1912 - val_accuracy: 0.2833\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1118 - accuracy: 0.3167 - val_loss: 1.1041 - val_accuracy: 0.2333\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 986ms/step - loss: 1.0970 - accuracy: 0.3944 - val_loss: 1.0975 - val_accuracy: 0.3167\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0950 - accuracy: 0.3778 - val_loss: 1.0964 - val_accuracy: 0.3500\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0942 - accuracy: 0.3722 - val_loss: 1.0955 - val_accuracy: 0.3167\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 960ms/step - loss: 1.0872 - accuracy: 0.4111 - val_loss: 1.0981 - val_accuracy: 0.2833\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0930 - accuracy: 0.3389 - val_loss: 1.0939 - val_accuracy: 0.2667\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 3s 978ms/step - loss: 1.0923 - accuracy: 0.3278 - val_loss: 1.0849 - val_accuracy: 0.3500\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0910 - accuracy: 0.3444 - val_loss: 1.0811 - val_accuracy: 0.4000\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0850 - accuracy: 0.4222 - val_loss: 1.0756 - val_accuracy: 0.4500\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 454ms/step - loss: 1.0880 - accuracy: 0.3500\n",
            "Test accuracy on subset: 0.3499999940395355\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 2.3858 - accuracy: 0.3222 - val_loss: 1.3070 - val_accuracy: 0.4167\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.2459 - accuracy: 0.2944 - val_loss: 1.0975 - val_accuracy: 0.3500\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.0973 - accuracy: 0.3722 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 953ms/step - loss: 1.0988 - accuracy: 0.3278 - val_loss: 1.0987 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0984 - accuracy: 0.3333 - val_loss: 1.0987 - val_accuracy: 0.3333\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 281ms/step - loss: 1.0969 - accuracy: 0.4000\n",
            "Test accuracy on subset: 0.4000000059604645\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.4324 - accuracy: 0.2833 - val_loss: 1.1023 - val_accuracy: 0.3000\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 1s/step - loss: 1.1186 - accuracy: 0.3556 - val_loss: 1.0965 - val_accuracy: 0.3500\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 986ms/step - loss: 1.0951 - accuracy: 0.4111 - val_loss: 1.0987 - val_accuracy: 0.3000\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 972ms/step - loss: 1.0982 - accuracy: 0.3500 - val_loss: 1.0987 - val_accuracy: 0.2833\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0989 - accuracy: 0.3444 - val_loss: 1.0987 - val_accuracy: 0.2333\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 1.0985 - accuracy: 0.4000\n",
            "Test accuracy on subset: 0.4000000059604645\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 1s/step - loss: 1.1690 - accuracy: 0.3278 - val_loss: 1.0949 - val_accuracy: 0.3833\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1097 - accuracy: 0.3611 - val_loss: 1.0958 - val_accuracy: 0.3667\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 932ms/step - loss: 1.1006 - accuracy: 0.3556 - val_loss: 1.0977 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 898ms/step - loss: 1.0983 - accuracy: 0.3333 - val_loss: 1.0984 - val_accuracy: 0.3333\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 1.0996 - accuracy: 0.3833\n",
            "Test accuracy on subset: 0.38333332538604736\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 6s 3s/step - loss: 1.2715 - accuracy: 0.2889 - val_loss: 1.1314 - val_accuracy: 0.3667\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.1325 - accuracy: 0.3000 - val_loss: 1.1008 - val_accuracy: 0.2833\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 942ms/step - loss: 1.0997 - accuracy: 0.3833 - val_loss: 1.0985 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0986 - accuracy: 0.3333 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 948ms/step - loss: 1.0986 - accuracy: 0.3278 - val_loss: 1.0984 - val_accuracy: 0.3167\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0981 - accuracy: 0.3778 - val_loss: 1.0969 - val_accuracy: 0.3333\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0979 - accuracy: 0.3000 - val_loss: 1.0909 - val_accuracy: 0.3000\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 4s 986ms/step - loss: 1.0973 - accuracy: 0.3889 - val_loss: 1.0871 - val_accuracy: 0.3000\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 3s 940ms/step - loss: 1.0884 - accuracy: 0.3833 - val_loss: 1.0849 - val_accuracy: 0.3167\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0923 - accuracy: 0.3833 - val_loss: 1.0820 - val_accuracy: 0.3333\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 358ms/step - loss: 1.0962 - accuracy: 0.3500\n",
            "Test accuracy on subset: 0.3499999940395355\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 1s/step - loss: 1.3913 - accuracy: 0.2722 - val_loss: 1.1070 - val_accuracy: 0.4000\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1224 - accuracy: 0.3444 - val_loss: 1.0965 - val_accuracy: 0.4000\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.1000 - accuracy: 0.3833 - val_loss: 1.0985 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0987 - accuracy: 0.3333 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0987 - accuracy: 0.3333 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 1.0981 - accuracy: 0.3000\n",
            "Test accuracy on subset: 0.30000001192092896\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.1690 - accuracy: 0.2778 - val_loss: 1.1006 - val_accuracy: 0.3167\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 950ms/step - loss: 1.1255 - accuracy: 0.3722 - val_loss: 1.0969 - val_accuracy: 0.3000\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0989 - accuracy: 0.3778 - val_loss: 1.0986 - val_accuracy: 0.3500\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0974 - accuracy: 0.3944 - val_loss: 1.0990 - val_accuracy: 0.3500\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 959ms/step - loss: 1.0956 - accuracy: 0.3389 - val_loss: 1.0972 - val_accuracy: 0.3333\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 278ms/step - loss: 1.1005 - accuracy: 0.3167\n",
            "Test accuracy on subset: 0.3166666626930237\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.1068 - accuracy: 0.3278 - val_loss: 1.1853 - val_accuracy: 0.3167\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0956 - accuracy: 0.4222 - val_loss: 1.1016 - val_accuracy: 0.3833\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 975ms/step - loss: 1.1067 - accuracy: 0.2778 - val_loss: 1.0999 - val_accuracy: 0.3167\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.1007 - accuracy: 0.3278 - val_loss: 1.1000 - val_accuracy: 0.3500\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 988ms/step - loss: 1.0914 - accuracy: 0.3556 - val_loss: 1.0996 - val_accuracy: 0.3167\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0989 - accuracy: 0.3278 - val_loss: 1.1092 - val_accuracy: 0.3167\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0783 - accuracy: 0.3611 - val_loss: 1.1079 - val_accuracy: 0.3167\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 3s 953ms/step - loss: 1.1093 - accuracy: 0.3444 - val_loss: 1.1006 - val_accuracy: 0.3167\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 274ms/step - loss: 1.0883 - accuracy: 0.3500\n",
            "Test accuracy on subset: 0.3499999940395355\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 1s/step - loss: 1.3569 - accuracy: 0.3667 - val_loss: 1.0935 - val_accuracy: 0.3500\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.1014 - accuracy: 0.3222 - val_loss: 1.1007 - val_accuracy: 0.3333\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1005 - accuracy: 0.2889 - val_loss: 1.0994 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0989 - accuracy: 0.3389 - val_loss: 1.0988 - val_accuracy: 0.3333\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 277ms/step - loss: 1.0746 - accuracy: 0.4000\n",
            "Test accuracy on subset: 0.4000000059604645\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.1563 - accuracy: 0.3500 - val_loss: 1.1869 - val_accuracy: 0.3167\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0929 - accuracy: 0.3722 - val_loss: 1.1104 - val_accuracy: 0.3167\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0834 - accuracy: 0.4333 - val_loss: 1.1030 - val_accuracy: 0.3000\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0926 - accuracy: 0.3500 - val_loss: 1.0992 - val_accuracy: 0.3167\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0801 - accuracy: 0.4500 - val_loss: 1.0952 - val_accuracy: 0.4000\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0742 - accuracy: 0.4000 - val_loss: 1.0934 - val_accuracy: 0.4500\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0754 - accuracy: 0.4333 - val_loss: 1.1011 - val_accuracy: 0.3833\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0877 - accuracy: 0.4111 - val_loss: 1.1069 - val_accuracy: 0.3167\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 3s 980ms/step - loss: 1.0465 - accuracy: 0.4778 - val_loss: 1.0857 - val_accuracy: 0.3333\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0237 - accuracy: 0.4500 - val_loss: 1.0622 - val_accuracy: 0.4500\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 275ms/step - loss: 1.0511 - accuracy: 0.4333\n",
            "Test accuracy on subset: 0.4333333373069763\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 1s/step - loss: 1.2411 - accuracy: 0.3222 - val_loss: 1.2250 - val_accuracy: 0.3500\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1787 - accuracy: 0.3500 - val_loss: 1.0967 - val_accuracy: 0.4667\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0983 - accuracy: 0.3611 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 4s 914ms/step - loss: 1.0988 - accuracy: 0.3222 - val_loss: 1.0987 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 960ms/step - loss: 1.0987 - accuracy: 0.3333 - val_loss: 1.0987 - val_accuracy: 0.3333\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 274ms/step - loss: 1.0975 - accuracy: 0.4333\n",
            "Test accuracy on subset: 0.4333333373069763\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 1s/step - loss: 1.1802 - accuracy: 0.3889 - val_loss: 1.2535 - val_accuracy: 0.2667\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 967ms/step - loss: 1.1404 - accuracy: 0.3333 - val_loss: 1.0997 - val_accuracy: 0.3167\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 977ms/step - loss: 1.0963 - accuracy: 0.3556 - val_loss: 1.0986 - val_accuracy: 0.3500\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 6s 4s/step - loss: 1.0983 - accuracy: 0.3611 - val_loss: 1.0988 - val_accuracy: 0.3167\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 945ms/step - loss: 1.0976 - accuracy: 0.3389 - val_loss: 1.0992 - val_accuracy: 0.3333\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0961 - accuracy: 0.3389 - val_loss: 1.1023 - val_accuracy: 0.3500\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 272ms/step - loss: 1.0985 - accuracy: 0.3500\n",
            "Test accuracy on subset: 0.3499999940395355\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.2950 - accuracy: 0.2889 - val_loss: 1.1046 - val_accuracy: 0.3500\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 959ms/step - loss: 1.1014 - accuracy: 0.3444 - val_loss: 1.0947 - val_accuracy: 0.3500\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0982 - accuracy: 0.3389 - val_loss: 1.0980 - val_accuracy: 0.3500\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 1s/step - loss: 1.0984 - accuracy: 0.3556 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0990 - accuracy: 0.3333 - val_loss: 1.0988 - val_accuracy: 0.3333\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 276ms/step - loss: 1.0984 - accuracy: 0.3500\n",
            "Test accuracy on subset: 0.3499999940395355\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 1s/step - loss: 1.1953 - accuracy: 0.3167 - val_loss: 1.1063 - val_accuracy: 0.3667\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 978ms/step - loss: 1.0952 - accuracy: 0.3500 - val_loss: 1.0965 - val_accuracy: 0.4333\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0979 - accuracy: 0.3556 - val_loss: 1.0976 - val_accuracy: 0.4333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 964ms/step - loss: 1.0994 - accuracy: 0.3500 - val_loss: 1.0977 - val_accuracy: 0.4167\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1006 - accuracy: 0.2833 - val_loss: 1.0983 - val_accuracy: 0.3500\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 270ms/step - loss: 1.0981 - accuracy: 0.3667\n",
            "Test accuracy on subset: 0.36666667461395264\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.2550 - accuracy: 0.2944 - val_loss: 1.1126 - val_accuracy: 0.3500\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1033 - accuracy: 0.3500 - val_loss: 1.0996 - val_accuracy: 0.3333\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 1s/step - loss: 1.0974 - accuracy: 0.3389 - val_loss: 1.0948 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.0981 - accuracy: 0.3444 - val_loss: 1.0927 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0883 - accuracy: 0.3611 - val_loss: 1.0907 - val_accuracy: 0.3500\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0926 - accuracy: 0.3333 - val_loss: 1.0978 - val_accuracy: 0.4000\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0932 - accuracy: 0.3667 - val_loss: 1.0865 - val_accuracy: 0.3000\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0870 - accuracy: 0.3778 - val_loss: 1.0854 - val_accuracy: 0.3667\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0916 - accuracy: 0.3833 - val_loss: 1.0868 - val_accuracy: 0.3500\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0869 - accuracy: 0.3500 - val_loss: 1.0867 - val_accuracy: 0.3167\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 402ms/step - loss: 1.0987 - accuracy: 0.2333\n",
            "Test accuracy on subset: 0.23333333432674408\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.5302 - accuracy: 0.3000 - val_loss: 1.2514 - val_accuracy: 0.3333\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 965ms/step - loss: 1.1788 - accuracy: 0.3667 - val_loss: 1.0999 - val_accuracy: 0.3500\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 5s 1s/step - loss: 1.0961 - accuracy: 0.3444 - val_loss: 1.0988 - val_accuracy: 0.3167\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 964ms/step - loss: 1.0988 - accuracy: 0.3111 - val_loss: 1.0987 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0985 - accuracy: 0.3278 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 999ms/step - loss: 1.0991 - accuracy: 0.2556 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0986 - accuracy: 0.3167 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 3s 1s/step - loss: 1.0984 - accuracy: 0.3333 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0982 - accuracy: 0.4444 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0985 - accuracy: 0.3556 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 384ms/step - loss: 1.0986 - accuracy: 0.3333\n",
            "Test accuracy on subset: 0.3333333432674408\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 6s 3s/step - loss: 1.7923 - accuracy: 0.2833 - val_loss: 1.4780 - val_accuracy: 0.3333\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1936 - accuracy: 0.3556 - val_loss: 1.1045 - val_accuracy: 0.3667\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0992 - accuracy: 0.3611 - val_loss: 1.0987 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0990 - accuracy: 0.3111 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 879ms/step - loss: 1.0984 - accuracy: 0.3389 - val_loss: 1.0987 - val_accuracy: 0.3333\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0988 - accuracy: 0.3278 - val_loss: 1.0987 - val_accuracy: 0.3333\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 5s 1s/step - loss: 1.0991 - accuracy: 0.3333 - val_loss: 1.0987 - val_accuracy: 0.3333\n",
            "Found 60 images belonging to 3 classes.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-b588132efc7f>\u001b[0m in \u001b[0;36m<cell line: 96>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;31m# Evaluate the model on the subset test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m     test_loss, test_acc = best_model.evaluate(\n\u001b[0m\u001b[1;32m    139\u001b[0m         test_datagen.flow_from_directory(\n\u001b[1;32m    140\u001b[0m             \u001b[0msubset_test_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   2294\u001b[0m                         ):\n\u001b[1;32m   2295\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2296\u001b[0;31m                             logs = test_function_runner.run_step(\n\u001b[0m\u001b[1;32m   2297\u001b[0m                                 \u001b[0mdataset_or_iterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2298\u001b[0m                                 \u001b[0mdata_handler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(self, dataset_or_iterator, data_handler, step, unused_shards)\u001b[0m\n\u001b[1;32m   4106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_or_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_shards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m         \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_or_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    878\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this code is not working - Hyperparmeters not rotating"
      ],
      "metadata": {
        "id": "GGoxvLs09K7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Keras Tuner\n",
        "!pip install keras-tuner --upgrade\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import keras_tuner as kt\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Function to create a small subset of the dataset\n",
        "def create_subset(original_dir, subset_dir, classes, num_images_per_class):\n",
        "    if not os.path.exists(subset_dir):\n",
        "        os.makedirs(subset_dir)\n",
        "    for class_name in classes:\n",
        "        class_dir = os.path.join(original_dir, class_name)\n",
        "        subset_class_dir = os.path.join(subset_dir, class_name)\n",
        "        if not os.path.exists(subset_class_dir):\n",
        "            os.makedirs(subset_class_dir)\n",
        "        images = os.listdir(class_dir)\n",
        "        random.shuffle(images)\n",
        "        for img in images[:num_images_per_class]:\n",
        "            shutil.copy(os.path.join(class_dir, img), os.path.join(subset_class_dir, img))\n",
        "\n",
        "# Paths to your full dataset\n",
        "full_train_dir = 'dataset-c/triple_mnist/train'\n",
        "full_val_dir = 'dataset-c/triple_mnist/val'\n",
        "full_test_dir = 'dataset-c/triple_mnist/test'\n",
        "\n",
        "# Paths to your subset dataset\n",
        "subset_train_dir = 'subset/triple_mnist/train'\n",
        "subset_val_dir = 'subset/triple_mnist/val'\n",
        "subset_test_dir = 'subset/triple_mnist/test'\n",
        "\n",
        "# Create a small subset of your data\n",
        "classes_to_use = ['000', '001', '002']  # Example classes\n",
        "num_images_per_class = 50  # Example number of images per class\n",
        "create_subset(full_train_dir, subset_train_dir, classes_to_use, num_images_per_class)\n",
        "create_subset(full_val_dir, subset_val_dir, classes_to_use, num_images_per_class)\n",
        "create_subset(full_test_dir, subset_test_dir, classes_to_use, num_images_per_class)\n",
        "\n",
        "# Data augmentation for training\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Data generators for the subset\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Hyperparameter tuning function\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(hp.Int('conv_1_filter', 32, 128, step=32), (3, 3), activation='relu', input_shape=(84, 84, 3)))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Conv2D(hp.Int('conv_2_filter', 32, 128, step=32), (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(hp.Int('dense_units', 32, 128, step=32), activation='relu'))\n",
        "    model.add(Dropout(hp.Float('dropout_rate', 0.2, 0.5, step=0.1)))\n",
        "    model.add(Dense(3, activation='softmax'))  # For subset data\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG')),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Callback for early stopping\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Define the tuner using RandomSearch\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=1,\n",
        "    directory='output',\n",
        "    project_name='digit_tuning_subset'\n",
        ")\n",
        "\n",
        "best_acc = 0\n",
        "while best_acc < 0.80:  # Run until accuracy of 80% is achieved\n",
        "    # Perform the hyperparameter search on the subset\n",
        "    tuner.search(\n",
        "        train_datagen.flow_from_directory(\n",
        "            subset_train_dir,\n",
        "            target_size=(84, 84),\n",
        "            batch_size=32,\n",
        "            class_mode='categorical'\n",
        "        ),\n",
        "        epochs=10,\n",
        "        validation_data=val_datagen.flow_from_directory(\n",
        "            subset_val_dir,\n",
        "            target_size=(84, 84),\n",
        "            batch_size=32,\n",
        "            class_mode='categorical'\n",
        "        ),\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    # Retrieve the best hyperparameters and print them\n",
        "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "    # Build and train the model with the best hyperparameters on the subset\n",
        "    best_model = tuner.hypermodel.build(best_hps)\n",
        "    history = best_model.fit(\n",
        "        train_datagen.flow_from_directory(\n",
        "            subset_train_dir,\n",
        "            target_size=(84, 84),\n",
        "            batch_size=best_hps.get('batch_size'),\n",
        "            class_mode='categorical'\n",
        "        ),\n",
        "        epochs=10,\n",
        "        validation_data=val_datagen.flow_from_directory(\n",
        "            subset_val_dir,\n",
        "            target_size=(84, 84),\n",
        "            batch_size=best_hps.get('batch_size'),\n",
        "            class_mode='categorical'\n",
        "        ),\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    # Evaluate the model on the subset test data\n",
        "    test_loss, test_acc = best_model.evaluate(\n",
        "        test_datagen.flow_from_directory(\n",
        "            subset_test_dir,\n",
        "            target_size=(84, 84),\n",
        "            batch_size=best_hps.get('batch_size'),\n",
        "            class_mode='categorical'\n",
        "        )\n",
        "    )\n",
        "    print(f\"Test accuracy on subset: {test_acc}\")\n",
        "    best_acc = test_acc\n",
        "\n",
        "    # Print the best hyperparameters\n",
        "    print(\"Best Hyperparameters:\")\n",
        "    for hp, value in best_hps.values.items():\n",
        "        print(f\"{hp}: {value}\")\n",
        "\n",
        "    # Display table of current and best hyperparameters\n",
        "    current_trial = {k: v for k, v in best_hps.values.items()}\n",
        "    current_trial['val_accuracy'] = test_acc\n",
        "\n",
        "    if 'trials' not in globals():\n",
        "        trials = []\n",
        "    trials.append(current_trial)\n",
        "\n",
        "    df_trials = pd.DataFrame(trials)\n",
        "    display(df_trials)\n",
        "\n",
        "# Plot training history for the subset\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Use the best hyperparameters for the full dataset\n",
        "train_generator_full = train_datagen.flow_from_directory(\n",
        "    full_train_dir,\n",
        "    target_size=(84, 84),\n",
        "    batch_size=best_hps.get('batch_size'),\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator_full = val_datagen.flow_from_directory(\n",
        "    full_val_dir,\n",
        "    target_size=(84, 84),\n",
        "    batch_size=best_hps.get('batch_size'),\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator_full = test_datagen.flow_from_directory(\n",
        "    full_test_dir,\n",
        "    target_size=(84, 84),\n",
        "    batch_size=best_hps.get('batch_size'),\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Build and train the model with the best hyperparameters on the full dataset\n",
        "best_model_full = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Modify the final layer to match the number of classes in the full dataset\n",
        "best_model_full.pop()  # Remove the old final layer\n",
        "best_model_full.add(Dense(train_generator_full.num_classes, activation='softmax'))  # Add a new layer with correct output size\n",
        "\n",
        "best_model_full.compile(optimizer=Adam(learning_rate=best_hps.get('learning_rate')),\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "history_full = best_model_full.fit(\n",
        "    train_generator_full,\n",
        "    epochs=20,\n",
        "    validation_data=val_generator_full,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluate the model on the full test data\n",
        "test_loss_full, test_acc_full = best_model_full.evaluate(test_generator_full)\n",
        "print(f\"Test accuracy on full dataset: {test_acc_full}\")\n",
        "\n",
        "# Plot training history for the full dataset\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_full.history['accuracy'], label='accuracy')\n",
        "plt.plot(history_full.history['val_accuracy'], label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_full.history['loss'], label='loss')\n",
        "plt.plot(history_full.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xB0oEeEBYzne",
        "outputId": "a960a00e-b7b6-4a4d-9ec9-ebb503eb3668"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.10/dist-packages (1.4.7)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.7.4)\n",
            "Reloading Tuner from output/digit_tuning_subset/tuner0.json\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 1s/step - loss: 1.1477 - accuracy: 0.3778 - val_loss: 1.1161 - val_accuracy: 0.3667\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.1283 - accuracy: 0.3778 - val_loss: 1.0955 - val_accuracy: 0.4333\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 955ms/step - loss: 1.0989 - accuracy: 0.3222 - val_loss: 1.0981 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 996ms/step - loss: 1.0983 - accuracy: 0.3333 - val_loss: 1.0978 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0988 - accuracy: 0.3556 - val_loss: 1.0973 - val_accuracy: 0.4500\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 271ms/step - loss: 1.0953 - accuracy: 0.4500\n",
            "Test accuracy on subset: 0.44999998807907104\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   conv_1_filter  conv_2_filter  dense_units  dropout_rate  learning_rate  \\\n",
              "0             64             64           32           0.4       0.003943   \n",
              "\n",
              "   batch_size  val_accuracy  \n",
              "0         128          0.45  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e50fffc7-856c-43c2-9e5b-645b90c133cf\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>conv_1_filter</th>\n",
              "      <th>conv_2_filter</th>\n",
              "      <th>dense_units</th>\n",
              "      <th>dropout_rate</th>\n",
              "      <th>learning_rate</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>val_accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.45</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e50fffc7-856c-43c2-9e5b-645b90c133cf')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e50fffc7-856c-43c2-9e5b-645b90c133cf button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e50fffc7-856c-43c2-9e5b-645b90c133cf');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_5fbac2bd-d664-43d6-836d-da5bcbab486d\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_trials')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_5fbac2bd-d664-43d6-836d-da5bcbab486d button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_trials');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_trials",
              "summary": "{\n  \"name\": \"df_trials\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"conv_1_filter\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 64,\n        \"max\": 64,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"conv_2_filter\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 64,\n        \"max\": 64,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dense_units\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 32,\n        \"max\": 32,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dropout_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.4,\n        \"max\": 0.4,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"learning_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0039426799290831925,\n        \"max\": 0.0039426799290831925,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0039426799290831925\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 128,\n        \"max\": 128,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          128\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"val_accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.44999998807907104,\n        \"max\": 0.44999998807907104,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.44999998807907104\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 1s/step - loss: 1.2123 - accuracy: 0.3333 - val_loss: 1.1383 - val_accuracy: 0.2667\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 946ms/step - loss: 1.1389 - accuracy: 0.2944 - val_loss: 1.0971 - val_accuracy: 0.3333\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 5s 1s/step - loss: 1.0984 - accuracy: 0.3500 - val_loss: 1.0980 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 937ms/step - loss: 1.0983 - accuracy: 0.3333 - val_loss: 1.0980 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 930ms/step - loss: 1.0988 - accuracy: 0.3389 - val_loss: 1.0985 - val_accuracy: 0.3333\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 1.0964 - accuracy: 0.3333\n",
            "Test accuracy on subset: 0.3333333432674408\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   conv_1_filter  conv_2_filter  dense_units  dropout_rate  learning_rate  \\\n",
              "0             64             64           32           0.4       0.003943   \n",
              "1             64             64           32           0.4       0.003943   \n",
              "\n",
              "   batch_size  val_accuracy  \n",
              "0         128      0.450000  \n",
              "1         128      0.333333  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dbd9d605-b8dc-46e8-b583-40d0c2dcd036\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>conv_1_filter</th>\n",
              "      <th>conv_2_filter</th>\n",
              "      <th>dense_units</th>\n",
              "      <th>dropout_rate</th>\n",
              "      <th>learning_rate</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>val_accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.450000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dbd9d605-b8dc-46e8-b583-40d0c2dcd036')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dbd9d605-b8dc-46e8-b583-40d0c2dcd036 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dbd9d605-b8dc-46e8-b583-40d0c2dcd036');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0292d830-e2fb-4dc0-80df-bff4790da95c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0292d830-e2fb-4dc0-80df-bff4790da95c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0292d830-e2fb-4dc0-80df-bff4790da95c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_4da09999-75d3-44c5-950b-2cbf7caebb85\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_trials')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_4da09999-75d3-44c5-950b-2cbf7caebb85 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_trials');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_trials",
              "summary": "{\n  \"name\": \"df_trials\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"conv_1_filter\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 64,\n        \"max\": 64,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"conv_2_filter\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 64,\n        \"max\": 64,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dense_units\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 32,\n        \"max\": 32,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dropout_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.4,\n        \"max\": 0.4,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"learning_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0039426799290831925,\n        \"max\": 0.0039426799290831925,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0039426799290831925\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 128,\n        \"max\": 128,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          128\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"val_accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08249577568458609,\n        \"min\": 0.3333333432674408,\n        \"max\": 0.44999998807907104,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.3333333432674408\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.1984 - accuracy: 0.3778 - val_loss: 1.2303 - val_accuracy: 0.4667\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 955ms/step - loss: 1.2224 - accuracy: 0.3556 - val_loss: 1.0949 - val_accuracy: 0.4333\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 945ms/step - loss: 1.0986 - accuracy: 0.3111 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0989 - accuracy: 0.3333 - val_loss: 1.0987 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 4s 1s/step - loss: 1.0996 - accuracy: 0.3333 - val_loss: 1.0988 - val_accuracy: 0.3333\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 1.0984 - accuracy: 0.3500\n",
            "Test accuracy on subset: 0.3499999940395355\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   conv_1_filter  conv_2_filter  dense_units  dropout_rate  learning_rate  \\\n",
              "0             64             64           32           0.4       0.003943   \n",
              "1             64             64           32           0.4       0.003943   \n",
              "2             64             64           32           0.4       0.003943   \n",
              "\n",
              "   batch_size  val_accuracy  \n",
              "0         128      0.450000  \n",
              "1         128      0.333333  \n",
              "2         128      0.350000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-834a0e0f-9a21-42d5-8faf-bb81d34ff931\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>conv_1_filter</th>\n",
              "      <th>conv_2_filter</th>\n",
              "      <th>dense_units</th>\n",
              "      <th>dropout_rate</th>\n",
              "      <th>learning_rate</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>val_accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.450000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.350000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-834a0e0f-9a21-42d5-8faf-bb81d34ff931')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-834a0e0f-9a21-42d5-8faf-bb81d34ff931 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-834a0e0f-9a21-42d5-8faf-bb81d34ff931');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4c77fefc-200f-43a4-8efb-8df6fd04cf23\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4c77fefc-200f-43a4-8efb-8df6fd04cf23')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4c77fefc-200f-43a4-8efb-8df6fd04cf23 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_a088df83-5f96-43e3-9eef-a8c1c1d215b1\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_trials')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_a088df83-5f96-43e3-9eef-a8c1c1d215b1 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_trials');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_trials",
              "summary": "{\n  \"name\": \"df_trials\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"conv_1_filter\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 64,\n        \"max\": 64,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"conv_2_filter\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 64,\n        \"max\": 64,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dense_units\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 32,\n        \"max\": 32,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dropout_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.798699777552591e-17,\n        \"min\": 0.4,\n        \"max\": 0.4,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"learning_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0039426799290831925,\n        \"max\": 0.0039426799290831925,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0039426799290831925\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 128,\n        \"max\": 128,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          128\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"val_accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06309897261111468,\n        \"min\": 0.3333333432674408,\n        \"max\": 0.44999998807907104,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.44999998807907104\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 1s/step - loss: 1.1779 - accuracy: 0.2556 - val_loss: 1.2247 - val_accuracy: 0.3333\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1136 - accuracy: 0.3167 - val_loss: 1.1032 - val_accuracy: 0.2833\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0970 - accuracy: 0.3889 - val_loss: 1.0985 - val_accuracy: 0.3833\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0997 - accuracy: 0.2833 - val_loss: 1.0974 - val_accuracy: 0.3167\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 972ms/step - loss: 1.0977 - accuracy: 0.3611 - val_loss: 1.0962 - val_accuracy: 0.3500\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0946 - accuracy: 0.4056 - val_loss: 1.0933 - val_accuracy: 0.2667\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 5s 1s/step - loss: 1.0939 - accuracy: 0.3722 - val_loss: 1.0936 - val_accuracy: 0.3167\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0919 - accuracy: 0.3722 - val_loss: 1.0949 - val_accuracy: 0.3333\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0853 - accuracy: 0.3722 - val_loss: 1.0960 - val_accuracy: 0.3500\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 1.0946 - accuracy: 0.3667\n",
            "Test accuracy on subset: 0.36666667461395264\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   conv_1_filter  conv_2_filter  dense_units  dropout_rate  learning_rate  \\\n",
              "0             64             64           32           0.4       0.003943   \n",
              "1             64             64           32           0.4       0.003943   \n",
              "2             64             64           32           0.4       0.003943   \n",
              "3             64             64           32           0.4       0.003943   \n",
              "\n",
              "   batch_size  val_accuracy  \n",
              "0         128      0.450000  \n",
              "1         128      0.333333  \n",
              "2         128      0.350000  \n",
              "3         128      0.366667  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ad1d23f2-9684-46ea-b154-67ed5ebb23b6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>conv_1_filter</th>\n",
              "      <th>conv_2_filter</th>\n",
              "      <th>dense_units</th>\n",
              "      <th>dropout_rate</th>\n",
              "      <th>learning_rate</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>val_accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.450000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.350000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.366667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ad1d23f2-9684-46ea-b154-67ed5ebb23b6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ad1d23f2-9684-46ea-b154-67ed5ebb23b6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ad1d23f2-9684-46ea-b154-67ed5ebb23b6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-37783609-c0cf-4c34-89e6-83f362e8456c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-37783609-c0cf-4c34-89e6-83f362e8456c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-37783609-c0cf-4c34-89e6-83f362e8456c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_57bab3d7-8102-4ae1-9788-6b6b5758d2cc\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_trials')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_57bab3d7-8102-4ae1-9788-6b6b5758d2cc button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_trials');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_trials",
              "summary": "{\n  \"name\": \"df_trials\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"conv_1_filter\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 64,\n        \"max\": 64,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"conv_2_filter\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 64,\n        \"max\": 64,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dense_units\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 32,\n        \"max\": 32,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dropout_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.4,\n        \"max\": 0.4,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"learning_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0039426799290831925,\n        \"max\": 0.0039426799290831925,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0039426799290831925\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 128,\n        \"max\": 128,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          128\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"val_accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05181876463580454,\n        \"min\": 0.3333333432674408,\n        \"max\": 0.44999998807907104,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.3333333432674408\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.2248 - accuracy: 0.3111 - val_loss: 1.1286 - val_accuracy: 0.3167\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 970ms/step - loss: 1.1171 - accuracy: 0.3000 - val_loss: 1.0985 - val_accuracy: 0.3667\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 975ms/step - loss: 1.0969 - accuracy: 0.3500 - val_loss: 1.0972 - val_accuracy: 0.3500\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0948 - accuracy: 0.3778 - val_loss: 1.0944 - val_accuracy: 0.3500\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 1s/step - loss: 1.0999 - accuracy: 0.3389 - val_loss: 1.0904 - val_accuracy: 0.3833\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0934 - accuracy: 0.3944 - val_loss: 1.0904 - val_accuracy: 0.4000\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0886 - accuracy: 0.4444 - val_loss: 1.0945 - val_accuracy: 0.3167\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0813 - accuracy: 0.4278 - val_loss: 1.0946 - val_accuracy: 0.3333\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0663 - accuracy: 0.4500 - val_loss: 1.1074 - val_accuracy: 0.3333\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 274ms/step - loss: 1.0780 - accuracy: 0.4667\n",
            "Test accuracy on subset: 0.46666666865348816\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   conv_1_filter  conv_2_filter  dense_units  dropout_rate  learning_rate  \\\n",
              "0             64             64           32           0.4       0.003943   \n",
              "1             64             64           32           0.4       0.003943   \n",
              "2             64             64           32           0.4       0.003943   \n",
              "3             64             64           32           0.4       0.003943   \n",
              "4             64             64           32           0.4       0.003943   \n",
              "\n",
              "   batch_size  val_accuracy  \n",
              "0         128      0.450000  \n",
              "1         128      0.333333  \n",
              "2         128      0.350000  \n",
              "3         128      0.366667  \n",
              "4         128      0.466667  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4a537e72-80e7-4115-b7c9-3d1626e79863\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>conv_1_filter</th>\n",
              "      <th>conv_2_filter</th>\n",
              "      <th>dense_units</th>\n",
              "      <th>dropout_rate</th>\n",
              "      <th>learning_rate</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>val_accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.450000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.350000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.366667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.466667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4a537e72-80e7-4115-b7c9-3d1626e79863')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4a537e72-80e7-4115-b7c9-3d1626e79863 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4a537e72-80e7-4115-b7c9-3d1626e79863');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-23ba1049-02d0-4790-9d88-63f6bf6b775d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-23ba1049-02d0-4790-9d88-63f6bf6b775d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-23ba1049-02d0-4790-9d88-63f6bf6b775d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_3d0b6e7c-a8c2-4225-ae4f-473132f0ee13\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_trials')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_3d0b6e7c-a8c2-4225-ae4f-473132f0ee13 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_trials');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_trials",
              "summary": "{\n  \"name\": \"df_trials\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"conv_1_filter\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 64,\n        \"max\": 64,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"conv_2_filter\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 64,\n        \"max\": 64,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dense_units\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 32,\n        \"max\": 32,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dropout_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.4,\n        \"max\": 0.4,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"learning_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0039426799290831925,\n        \"max\": 0.0039426799290831925,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0039426799290831925\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 128,\n        \"max\": 128,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          128\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"val_accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.060781937322623714,\n        \"min\": 0.3333333432674408,\n        \"max\": 0.46666666865348816,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.3333333432674408\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.1215 - accuracy: 0.3389 - val_loss: 1.1559 - val_accuracy: 0.2833\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0625 - accuracy: 0.4167 - val_loss: 1.1591 - val_accuracy: 0.2333\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 966ms/step - loss: 1.1124 - accuracy: 0.3500 - val_loss: 1.1054 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 970ms/step - loss: 1.0938 - accuracy: 0.4000 - val_loss: 1.1023 - val_accuracy: 0.3833\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 5s 3s/step - loss: 1.0932 - accuracy: 0.3500 - val_loss: 1.1098 - val_accuracy: 0.3500\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0802 - accuracy: 0.3778 - val_loss: 1.1282 - val_accuracy: 0.4167\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 3s 954ms/step - loss: 1.0763 - accuracy: 0.3889 - val_loss: 1.1338 - val_accuracy: 0.3667\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 279ms/step - loss: 1.0966 - accuracy: 0.3333\n",
            "Test accuracy on subset: 0.3333333432674408\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   conv_1_filter  conv_2_filter  dense_units  dropout_rate  learning_rate  \\\n",
              "0             64             64           32           0.4       0.003943   \n",
              "1             64             64           32           0.4       0.003943   \n",
              "2             64             64           32           0.4       0.003943   \n",
              "3             64             64           32           0.4       0.003943   \n",
              "4             64             64           32           0.4       0.003943   \n",
              "5             64             64           32           0.4       0.003943   \n",
              "\n",
              "   batch_size  val_accuracy  \n",
              "0         128      0.450000  \n",
              "1         128      0.333333  \n",
              "2         128      0.350000  \n",
              "3         128      0.366667  \n",
              "4         128      0.466667  \n",
              "5         128      0.333333  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e4e6b988-915e-40e5-93b9-53f468117d84\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>conv_1_filter</th>\n",
              "      <th>conv_2_filter</th>\n",
              "      <th>dense_units</th>\n",
              "      <th>dropout_rate</th>\n",
              "      <th>learning_rate</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>val_accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.450000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.350000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.366667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.466667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e4e6b988-915e-40e5-93b9-53f468117d84')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e4e6b988-915e-40e5-93b9-53f468117d84 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e4e6b988-915e-40e5-93b9-53f468117d84');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b3748fa7-0be0-4abb-ad47-c1382cfd47d9\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b3748fa7-0be0-4abb-ad47-c1382cfd47d9')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b3748fa7-0be0-4abb-ad47-c1382cfd47d9 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_b6b76f6a-24f2-420b-9a75-4ae335e32fb5\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_trials')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_b6b76f6a-24f2-420b-9a75-4ae335e32fb5 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_trials');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_trials",
              "summary": "{\n  \"name\": \"df_trials\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": \"conv_1_filter\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 64,\n        \"max\": 64,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"conv_2_filter\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 64,\n        \"max\": 64,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dense_units\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 32,\n        \"max\": 32,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dropout_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.080941944488117e-17,\n        \"min\": 0.4,\n        \"max\": 0.4,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"learning_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0039426799290831925,\n        \"max\": 0.0039426799290831925,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0039426799290831925\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 128,\n        \"max\": 128,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          128\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"val_accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05962847417986007,\n        \"min\": 0.3333333432674408,\n        \"max\": 0.46666666865348816,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.3333333432674408\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 6s 1s/step - loss: 1.1245 - accuracy: 0.3722 - val_loss: 1.1754 - val_accuracy: 0.2167\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 913ms/step - loss: 1.1481 - accuracy: 0.3556 - val_loss: 1.1001 - val_accuracy: 0.2833\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 946ms/step - loss: 1.0957 - accuracy: 0.3722 - val_loss: 1.0982 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 5s 3s/step - loss: 1.0976 - accuracy: 0.3444 - val_loss: 1.0982 - val_accuracy: 0.3667\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0990 - accuracy: 0.2944 - val_loss: 1.0980 - val_accuracy: 0.3167\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0955 - accuracy: 0.3556 - val_loss: 1.0967 - val_accuracy: 0.3333\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 3s 1s/step - loss: 1.0992 - accuracy: 0.3556 - val_loss: 1.0959 - val_accuracy: 0.3000\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 5s 986ms/step - loss: 1.0877 - accuracy: 0.3778 - val_loss: 1.0939 - val_accuracy: 0.3000\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 3s 972ms/step - loss: 1.0872 - accuracy: 0.4222 - val_loss: 1.0941 - val_accuracy: 0.3333\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0835 - accuracy: 0.3667 - val_loss: 1.0953 - val_accuracy: 0.2833\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 369ms/step - loss: 1.0727 - accuracy: 0.3333\n",
            "Test accuracy on subset: 0.3333333432674408\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   conv_1_filter  conv_2_filter  dense_units  dropout_rate  learning_rate  \\\n",
              "0             64             64           32           0.4       0.003943   \n",
              "1             64             64           32           0.4       0.003943   \n",
              "2             64             64           32           0.4       0.003943   \n",
              "3             64             64           32           0.4       0.003943   \n",
              "4             64             64           32           0.4       0.003943   \n",
              "5             64             64           32           0.4       0.003943   \n",
              "6             64             64           32           0.4       0.003943   \n",
              "\n",
              "   batch_size  val_accuracy  \n",
              "0         128      0.450000  \n",
              "1         128      0.333333  \n",
              "2         128      0.350000  \n",
              "3         128      0.366667  \n",
              "4         128      0.466667  \n",
              "5         128      0.333333  \n",
              "6         128      0.333333  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e9baf858-0b93-4559-9ab4-14dfb1b97f6e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>conv_1_filter</th>\n",
              "      <th>conv_2_filter</th>\n",
              "      <th>dense_units</th>\n",
              "      <th>dropout_rate</th>\n",
              "      <th>learning_rate</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>val_accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.450000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.350000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.366667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.466667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e9baf858-0b93-4559-9ab4-14dfb1b97f6e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e9baf858-0b93-4559-9ab4-14dfb1b97f6e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e9baf858-0b93-4559-9ab4-14dfb1b97f6e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-12bac44c-3fc9-402f-937a-0275effd9c76\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-12bac44c-3fc9-402f-937a-0275effd9c76')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-12bac44c-3fc9-402f-937a-0275effd9c76 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_289854a3-9713-4e80-ba1f-eec320af3963\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_trials')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_289854a3-9713-4e80-ba1f-eec320af3963 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_trials');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_trials",
              "summary": "{\n  \"name\": \"df_trials\",\n  \"rows\": 7,\n  \"fields\": [\n    {\n      \"column\": \"conv_1_filter\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 64,\n        \"max\": 64,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"conv_2_filter\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 64,\n        \"max\": 64,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dense_units\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 32,\n        \"max\": 32,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dropout_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.99588961666477e-17,\n        \"min\": 0.4,\n        \"max\": 0.4,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"learning_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0039426799290831925,\n        \"max\": 0.0039426799290831925,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0039426799290831925\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 128,\n        \"max\": 128,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          128\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"val_accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05762035393671297,\n        \"min\": 0.3333333432674408,\n        \"max\": 0.46666666865348816,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.3333333432674408\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 5s 1s/step - loss: 1.4030 - accuracy: 0.3389 - val_loss: 1.1601 - val_accuracy: 0.3500\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1179 - accuracy: 0.3944 - val_loss: 1.1010 - val_accuracy: 0.4167\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 953ms/step - loss: 1.0936 - accuracy: 0.3944 - val_loss: 1.0987 - val_accuracy: 0.4333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0987 - accuracy: 0.3389 - val_loss: 1.0986 - val_accuracy: 0.4333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 946ms/step - loss: 1.0979 - accuracy: 0.4222 - val_loss: 1.0985 - val_accuracy: 0.4333\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 995ms/step - loss: 1.0981 - accuracy: 0.3389 - val_loss: 1.0981 - val_accuracy: 0.4000\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0965 - accuracy: 0.3667 - val_loss: 1.0977 - val_accuracy: 0.3333\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0941 - accuracy: 0.2778 - val_loss: 1.0985 - val_accuracy: 0.3500\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0999 - accuracy: 0.4278 - val_loss: 1.1007 - val_accuracy: 0.3667\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 3s 943ms/step - loss: 1.0975 - accuracy: 0.3611 - val_loss: 1.1018 - val_accuracy: 0.3167\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 266ms/step - loss: 1.0989 - accuracy: 0.4167\n",
            "Test accuracy on subset: 0.4166666567325592\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   conv_1_filter  conv_2_filter  dense_units  dropout_rate  learning_rate  \\\n",
              "0             64             64           32           0.4       0.003943   \n",
              "1             64             64           32           0.4       0.003943   \n",
              "2             64             64           32           0.4       0.003943   \n",
              "3             64             64           32           0.4       0.003943   \n",
              "4             64             64           32           0.4       0.003943   \n",
              "5             64             64           32           0.4       0.003943   \n",
              "6             64             64           32           0.4       0.003943   \n",
              "7             64             64           32           0.4       0.003943   \n",
              "\n",
              "   batch_size  val_accuracy  \n",
              "0         128      0.450000  \n",
              "1         128      0.333333  \n",
              "2         128      0.350000  \n",
              "3         128      0.366667  \n",
              "4         128      0.466667  \n",
              "5         128      0.333333  \n",
              "6         128      0.333333  \n",
              "7         128      0.416667  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4d764af0-3d89-4411-9e8e-b812f27d3711\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>conv_1_filter</th>\n",
              "      <th>conv_2_filter</th>\n",
              "      <th>dense_units</th>\n",
              "      <th>dropout_rate</th>\n",
              "      <th>learning_rate</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>val_accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.450000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.350000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.366667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.466667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.416667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4d764af0-3d89-4411-9e8e-b812f27d3711')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4d764af0-3d89-4411-9e8e-b812f27d3711 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4d764af0-3d89-4411-9e8e-b812f27d3711');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3ccc5cac-1fdc-45c0-a484-c5579530903f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3ccc5cac-1fdc-45c0-a484-c5579530903f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3ccc5cac-1fdc-45c0-a484-c5579530903f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_2dddd9f4-4f68-4e46-a3d6-6e1300f6e656\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_trials')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_2dddd9f4-4f68-4e46-a3d6-6e1300f6e656 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_trials');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_trials",
              "summary": "{\n  \"name\": \"df_trials\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"conv_1_filter\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 64,\n        \"max\": 64,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"conv_2_filter\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 64,\n        \"max\": 64,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dense_units\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 32,\n        \"max\": 32,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dropout_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.4,\n        \"max\": 0.4,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"learning_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0039426799290831925,\n        \"max\": 0.0039426799290831925,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0039426799290831925\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 128,\n        \"max\": 128,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          128\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"val_accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05523218775311347,\n        \"min\": 0.3333333432674408,\n        \"max\": 0.46666666865348816,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.44999998807907104\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 6s 1s/step - loss: 1.2113 - accuracy: 0.3833 - val_loss: 1.3010 - val_accuracy: 0.3000\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1567 - accuracy: 0.2722 - val_loss: 1.0990 - val_accuracy: 0.3333\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 952ms/step - loss: 1.0995 - accuracy: 0.3444 - val_loss: 1.0987 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0988 - accuracy: 0.3333 - val_loss: 1.0989 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0984 - accuracy: 0.3333 - val_loss: 1.0989 - val_accuracy: 0.3333\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 972ms/step - loss: 1.0985 - accuracy: 0.3333 - val_loss: 1.0987 - val_accuracy: 0.3333\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0982 - accuracy: 0.3389 - val_loss: 1.0986 - val_accuracy: 0.3500\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0984 - accuracy: 0.3500 - val_loss: 1.0985 - val_accuracy: 0.3667\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 4s 915ms/step - loss: 1.0967 - accuracy: 0.3278 - val_loss: 1.0992 - val_accuracy: 0.3167\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0960 - accuracy: 0.3833 - val_loss: 1.0975 - val_accuracy: 0.2833\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 271ms/step - loss: 1.0933 - accuracy: 0.3000\n",
            "Test accuracy on subset: 0.30000001192092896\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   conv_1_filter  conv_2_filter  dense_units  dropout_rate  learning_rate  \\\n",
              "0             64             64           32           0.4       0.003943   \n",
              "1             64             64           32           0.4       0.003943   \n",
              "2             64             64           32           0.4       0.003943   \n",
              "3             64             64           32           0.4       0.003943   \n",
              "4             64             64           32           0.4       0.003943   \n",
              "5             64             64           32           0.4       0.003943   \n",
              "6             64             64           32           0.4       0.003943   \n",
              "7             64             64           32           0.4       0.003943   \n",
              "8             64             64           32           0.4       0.003943   \n",
              "\n",
              "   batch_size  val_accuracy  \n",
              "0         128      0.450000  \n",
              "1         128      0.333333  \n",
              "2         128      0.350000  \n",
              "3         128      0.366667  \n",
              "4         128      0.466667  \n",
              "5         128      0.333333  \n",
              "6         128      0.333333  \n",
              "7         128      0.416667  \n",
              "8         128      0.300000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f803a797-2c84-4d7e-abac-aa361fb6958d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>conv_1_filter</th>\n",
              "      <th>conv_2_filter</th>\n",
              "      <th>dense_units</th>\n",
              "      <th>dropout_rate</th>\n",
              "      <th>learning_rate</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>val_accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.450000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.350000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.366667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.466667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.416667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.300000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f803a797-2c84-4d7e-abac-aa361fb6958d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f803a797-2c84-4d7e-abac-aa361fb6958d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f803a797-2c84-4d7e-abac-aa361fb6958d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-370c7555-47b6-4513-a333-7a4ac2df5ed3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-370c7555-47b6-4513-a333-7a4ac2df5ed3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-370c7555-47b6-4513-a333-7a4ac2df5ed3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_b4209b5d-f61e-473b-af97-351197743534\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_trials')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_b4209b5d-f61e-473b-af97-351197743534 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_trials');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_trials",
              "summary": "{\n  \"name\": \"df_trials\",\n  \"rows\": 9,\n  \"fields\": [\n    {\n      \"column\": \"conv_1_filter\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 64,\n        \"max\": 64,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"conv_2_filter\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 64,\n        \"max\": 64,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dense_units\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 32,\n        \"max\": 32,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dropout_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.4,\n        \"max\": 0.4,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"learning_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0039426799290831925,\n        \"max\": 0.0039426799290831925,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0039426799290831925\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 128,\n        \"max\": 128,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          128\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"val_accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05833332666329014,\n        \"min\": 0.30000001192092896,\n        \"max\": 0.46666666865348816,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          0.44999998807907104\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.1871 - accuracy: 0.2944 - val_loss: 1.1365 - val_accuracy: 0.3500\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1311 - accuracy: 0.3556 - val_loss: 1.0943 - val_accuracy: 0.4000\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 949ms/step - loss: 1.0964 - accuracy: 0.3611 - val_loss: 1.0979 - val_accuracy: 0.3333\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-9f69f637f23f>\u001b[0m in \u001b[0;36m<cell line: 97>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;31m# Build and train the model with the best hyperparameters on the subset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_hps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     history = best_model.fit(\n\u001b[0m\u001b[1;32m    122\u001b[0m         train_datagen.flow_from_directory(\n\u001b[1;32m    123\u001b[0m             \u001b[0msubset_train_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1793\u001b[0m             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1795\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1796\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1797\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mreset_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2701\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2703\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2704\u001b[0m         \"\"\"Resets the state of all the metrics in the model.\n\u001b[1;32m   2705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this code is not working with different table format - Hyperparameters not rotating?"
      ],
      "metadata": {
        "id": "SrE77qlV8_7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Keras Tuner\n",
        "!pip install keras-tuner --upgrade\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import keras_tuner as kt\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Function to create a small subset of the dataset\n",
        "def create_subset(original_dir, subset_dir, classes, num_images_per_class):\n",
        "    if not os.path.exists(subset_dir):\n",
        "        os.makedirs(subset_dir)\n",
        "    for class_name in classes:\n",
        "        class_dir = os.path.join(original_dir, class_name)\n",
        "        subset_class_dir = os.path.join(subset_dir, class_name)\n",
        "        if not os.path.exists(subset_class_dir):\n",
        "            os.makedirs(subset_class_dir)\n",
        "        images = os.listdir(class_dir)\n",
        "        random.shuffle(images)\n",
        "        for img in images[:num_images_per_class]:\n",
        "            shutil.copy(os.path.join(class_dir, img), os.path.join(subset_class_dir, img))\n",
        "\n",
        "# Paths to your full dataset\n",
        "full_train_dir = 'dataset-c/triple_mnist/train'\n",
        "full_val_dir = 'dataset-c/triple_mnist/val'\n",
        "full_test_dir = 'dataset-c/triple_mnist/test'\n",
        "\n",
        "# Paths to your subset dataset\n",
        "subset_train_dir = 'subset/triple_mnist/train'\n",
        "subset_val_dir = 'subset/triple_mnist/val'\n",
        "subset_test_dir = 'subset/triple_mnist/test'\n",
        "\n",
        "# Create a small subset of your data\n",
        "classes_to_use = ['000', '001', '002']  # Example classes\n",
        "num_images_per_class = 50  # Example number of images per class\n",
        "create_subset(full_train_dir, subset_train_dir, classes_to_use, num_images_per_class)\n",
        "create_subset(full_val_dir, subset_val_dir, classes_to_use, num_images_per_class)\n",
        "create_subset(full_test_dir, subset_test_dir, classes_to_use, num_images_per_class)\n",
        "\n",
        "# Data augmentation for training\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Data generators for the subset\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Hyperparameter tuning function\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(hp.Int('conv_1_filter', 32, 128, step=32), (3, 3), activation='relu', input_shape=(84, 84, 3)))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Conv2D(hp.Int('conv_2_filter', 32, 128, step=32), (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(hp.Int('dense_units', 32, 128, step=32), activation='relu'))\n",
        "    model.add(Dropout(hp.Float('dropout_rate', 0.2, 0.5, step=0.1)))\n",
        "    model.add(Dense(3, activation='softmax'))  # For subset data\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG')),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Callback for early stopping\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Define the tuner using RandomSearch\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=1,\n",
        "    directory='output',\n",
        "    project_name='digit_tuning_subset'\n",
        ")\n",
        "\n",
        "best_acc = 0\n",
        "while best_acc < 0.80:  # Run until accuracy of 80% is achieved\n",
        "    # Perform the hyperparameter search on the subset\n",
        "    tuner.search(\n",
        "        train_datagen.flow_from_directory(\n",
        "            subset_train_dir,\n",
        "            target_size=(84, 84),\n",
        "            batch_size=32,\n",
        "            class_mode='categorical'\n",
        "        ),\n",
        "        epochs=10,\n",
        "        validation_data=val_datagen.flow_from_directory(\n",
        "            subset_val_dir,\n",
        "            target_size=(84, 84),\n",
        "            batch_size=32,\n",
        "            class_mode='categorical'\n",
        "        ),\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    # Retrieve the best hyperparameters and print them\n",
        "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "    # Build and train the model with the best hyperparameters on the subset\n",
        "    best_model = tuner.hypermodel.build(best_hps)\n",
        "    history = best_model.fit(\n",
        "        train_datagen.flow_from_directory(\n",
        "            subset_train_dir,\n",
        "            target_size=(84, 84),\n",
        "            batch_size=best_hps.get('batch_size'),\n",
        "            class_mode='categorical'\n",
        "        ),\n",
        "        epochs=10,\n",
        "        validation_data=val_datagen.flow_from_directory(\n",
        "            subset_val_dir,\n",
        "            target_size=(84, 84),\n",
        "            batch_size=best_hps.get('batch_size'),\n",
        "            class_mode='categorical'\n",
        "        ),\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    # Evaluate the model on the subset test data\n",
        "    test_loss, test_acc = best_model.evaluate(\n",
        "        test_datagen.flow_from_directory(\n",
        "            subset_test_dir,\n",
        "            target_size=(84, 84),\n",
        "            batch_size=best_hps.get('batch_size'),\n",
        "            class_mode='categorical'\n",
        "        )\n",
        "    )\n",
        "    print(f\"Test accuracy on subset: {test_acc}\")\n",
        "    best_acc = test_acc\n",
        "\n",
        "    # Print the best hyperparameters\n",
        "    print(\"Best Hyperparameters:\")\n",
        "    for hp, value in best_hps.values.items():\n",
        "        print(f\"{hp}: {value}\")\n",
        "\n",
        "    # Display table of current and best hyperparameters\n",
        "    current_trial = {k: v for k, v in best_hps.values.items()}\n",
        "    current_trial['val_accuracy'] = test_acc\n",
        "\n",
        "    if 'trials' not in globals():\n",
        "        trials = []\n",
        "    trials.append(current_trial)\n",
        "\n",
        "    df_trials = pd.DataFrame(trials)\n",
        "    display(df_trials)\n",
        "\n",
        "# Plot training history for the subset\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Use the best hyperparameters for the full dataset\n",
        "train_generator_full = train_datagen.flow_from_directory(\n",
        "    full_train_dir,\n",
        "    target_size=(84, 84),\n",
        "    batch_size=best_hps.get('batch_size'),\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator_full = val_datagen.flow_from_directory(\n",
        "    full_val_dir,\n",
        "    target_size=(84, 84),\n",
        "    batch_size=best_hps.get('batch_size'),\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator_full = test_datagen.flow_from_directory(\n",
        "    full_test_dir,\n",
        "    target_size=(84, 84),\n",
        "    batch_size=best_hps.get('batch_size'),\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Build and train the model with the best hyperparameters on the full dataset\n",
        "best_model_full = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Modify the final layer to match the number of classes in the full dataset\n",
        "best_model_full.pop()  # Remove the old final layer\n",
        "best_model_full.add(Dense(train_generator_full.num_classes, activation='softmax'))  # Add a new layer with correct output size\n",
        "\n",
        "best_model_full.compile(optimizer=Adam(learning_rate=best_hps.get('learning_rate')),\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "history_full = best_model_full.fit(\n",
        "    train_generator_full,\n",
        "    epochs=20,\n",
        "    validation_data=val_generator_full,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluate the model on the full test data\n",
        "test_loss_full, test_acc_full = best_model_full.evaluate(test_generator_full)\n",
        "print(f\"Test accuracy on full dataset: {test_acc_full}\")\n",
        "\n",
        "# Plot training history for the full dataset\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_full.history['accuracy'], label='accuracy')\n",
        "plt.plot(history_full.history['val_accuracy'], label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_full.history['loss'], label='loss')\n",
        "plt.plot(history_full.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "scYTM8vJaEle",
        "outputId": "c21d76fc-3591-411b-9841-c56c69303572"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.10/dist-packages (1.4.7)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.7.4)\n",
            "Reloading Tuner from output/digit_tuning_subset/tuner0.json\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.3464 - accuracy: 0.3556 - val_loss: 1.1144 - val_accuracy: 0.3667\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 975ms/step - loss: 1.1403 - accuracy: 0.3167 - val_loss: 1.0964 - val_accuracy: 0.3833\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.0981 - accuracy: 0.3444 - val_loss: 1.0984 - val_accuracy: 0.3667\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.0985 - accuracy: 0.3333 - val_loss: 1.0985 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 962ms/step - loss: 1.0987 - accuracy: 0.3444 - val_loss: 1.0983 - val_accuracy: 0.3333\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 1.0976 - accuracy: 0.4167\n",
            "Test accuracy on subset: 0.4166666567325592\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   conv_1_filter  conv_2_filter  dense_units  dropout_rate  learning_rate  \\\n",
              "0             64             64           32           0.4       0.003943   \n",
              "1             64             64           32           0.4       0.003943   \n",
              "2             64             64           32           0.4       0.003943   \n",
              "3             64             64           32           0.4       0.003943   \n",
              "4             64             64           32           0.4       0.003943   \n",
              "5             64             64           32           0.4       0.003943   \n",
              "6             64             64           32           0.4       0.003943   \n",
              "7             64             64           32           0.4       0.003943   \n",
              "8             64             64           32           0.4       0.003943   \n",
              "9             64             64           32           0.4       0.003943   \n",
              "\n",
              "   batch_size  val_accuracy  \n",
              "0         128      0.450000  \n",
              "1         128      0.333333  \n",
              "2         128      0.350000  \n",
              "3         128      0.366667  \n",
              "4         128      0.466667  \n",
              "5         128      0.333333  \n",
              "6         128      0.333333  \n",
              "7         128      0.416667  \n",
              "8         128      0.300000  \n",
              "9         128      0.416667  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c8ca96e9-8a0e-4aec-b86e-855f6794043f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>conv_1_filter</th>\n",
              "      <th>conv_2_filter</th>\n",
              "      <th>dense_units</th>\n",
              "      <th>dropout_rate</th>\n",
              "      <th>learning_rate</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>val_accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.450000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.350000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.366667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.466667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.416667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.416667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c8ca96e9-8a0e-4aec-b86e-855f6794043f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c8ca96e9-8a0e-4aec-b86e-855f6794043f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c8ca96e9-8a0e-4aec-b86e-855f6794043f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3fa5aff6-ec46-442e-9145-f3be3d1219ce\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3fa5aff6-ec46-442e-9145-f3be3d1219ce')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3fa5aff6-ec46-442e-9145-f3be3d1219ce button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_738889ed-7003-49d4-a61e-a10814fdcada\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_trials')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_738889ed-7003-49d4-a61e-a10814fdcada button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_trials');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_trials",
              "summary": "{\n  \"name\": \"df_trials\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"conv_1_filter\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 64,\n        \"max\": 64,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"conv_2_filter\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 64,\n        \"max\": 64,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dense_units\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 32,\n        \"max\": 32,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dropout_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.4,\n        \"max\": 0.4,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"learning_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9.14279549108516e-19,\n        \"min\": 0.0039426799290831925,\n        \"max\": 0.0039426799290831925,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0039426799290831925\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 128,\n        \"max\": 128,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          128\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"val_accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05676461414176444,\n        \"min\": 0.30000001192092896,\n        \"max\": 0.46666666865348816,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          0.44999998807907104\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3883 - accuracy: 0.2611 - val_loss: 1.1163 - val_accuracy: 0.3333\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 950ms/step - loss: 1.1040 - accuracy: 0.3389 - val_loss: 1.0989 - val_accuracy: 0.3667\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0981 - accuracy: 0.3722 - val_loss: 1.0973 - val_accuracy: 0.4333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0980 - accuracy: 0.3611 - val_loss: 1.0966 - val_accuracy: 0.4000\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.0964 - accuracy: 0.3611 - val_loss: 1.0963 - val_accuracy: 0.3500\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0945 - accuracy: 0.3556 - val_loss: 1.0931 - val_accuracy: 0.3500\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0861 - accuracy: 0.3944 - val_loss: 1.0891 - val_accuracy: 0.3000\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 3s 951ms/step - loss: 1.0835 - accuracy: 0.3778 - val_loss: 1.0944 - val_accuracy: 0.3333\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.1048 - accuracy: 0.3667 - val_loss: 1.0930 - val_accuracy: 0.3333\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.0799 - accuracy: 0.4278 - val_loss: 1.0900 - val_accuracy: 0.3833\n",
            "Found 60 images belonging to 3 classes.\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 1.0999 - accuracy: 0.3833\n",
            "Test accuracy on subset: 0.38333332538604736\n",
            "Best Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_2_filter: 64\n",
            "dense_units: 32\n",
            "dropout_rate: 0.4\n",
            "learning_rate: 0.0039426799290831925\n",
            "batch_size: 128\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    conv_1_filter  conv_2_filter  dense_units  dropout_rate  learning_rate  \\\n",
              "0              64             64           32           0.4       0.003943   \n",
              "1              64             64           32           0.4       0.003943   \n",
              "2              64             64           32           0.4       0.003943   \n",
              "3              64             64           32           0.4       0.003943   \n",
              "4              64             64           32           0.4       0.003943   \n",
              "5              64             64           32           0.4       0.003943   \n",
              "6              64             64           32           0.4       0.003943   \n",
              "7              64             64           32           0.4       0.003943   \n",
              "8              64             64           32           0.4       0.003943   \n",
              "9              64             64           32           0.4       0.003943   \n",
              "10             64             64           32           0.4       0.003943   \n",
              "\n",
              "    batch_size  val_accuracy  \n",
              "0          128      0.450000  \n",
              "1          128      0.333333  \n",
              "2          128      0.350000  \n",
              "3          128      0.366667  \n",
              "4          128      0.466667  \n",
              "5          128      0.333333  \n",
              "6          128      0.333333  \n",
              "7          128      0.416667  \n",
              "8          128      0.300000  \n",
              "9          128      0.416667  \n",
              "10         128      0.383333  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-18d38217-1e0a-4965-9978-4d11de62404c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>conv_1_filter</th>\n",
              "      <th>conv_2_filter</th>\n",
              "      <th>dense_units</th>\n",
              "      <th>dropout_rate</th>\n",
              "      <th>learning_rate</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>val_accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.450000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.350000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.366667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.466667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.416667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.416667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>128</td>\n",
              "      <td>0.383333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-18d38217-1e0a-4965-9978-4d11de62404c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-18d38217-1e0a-4965-9978-4d11de62404c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-18d38217-1e0a-4965-9978-4d11de62404c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a0f725a9-da68-4eeb-817b-f5a808ef5878\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a0f725a9-da68-4eeb-817b-f5a808ef5878')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a0f725a9-da68-4eeb-817b-f5a808ef5878 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_d76763d6-b115-4401-a06e-95698cb390f8\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_trials')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_d76763d6-b115-4401-a06e-95698cb390f8 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_trials');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_trials",
              "summary": "{\n  \"name\": \"df_trials\",\n  \"rows\": 11,\n  \"fields\": [\n    {\n      \"column\": \"conv_1_filter\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 64,\n        \"max\": 64,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"conv_2_filter\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 64,\n        \"max\": 64,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dense_units\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 32,\n        \"max\": 32,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dropout_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.4,\n        \"max\": 0.4,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"learning_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9.096966653664784e-19,\n        \"min\": 0.0039426799290831925,\n        \"max\": 0.0039426799290831925,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0039426799290831925\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 128,\n        \"max\": 128,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          128\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"val_accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.053889142408453786,\n        \"min\": 0.30000001192092896,\n        \"max\": 0.46666666865348816,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.3333333432674408\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 1s/step - loss: 1.1097 - accuracy: 0.3444 - val_loss: 1.0968 - val_accuracy: 0.4500\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-9f69f637f23f>\u001b[0m in \u001b[0;36m<cell line: 97>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;31m# Build and train the model with the best hyperparameters on the subset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_hps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     history = best_model.fit(\n\u001b[0m\u001b[1;32m    122\u001b[0m         train_datagen.flow_from_directory(\n\u001b[1;32m    123\u001b[0m             \u001b[0msubset_train_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "got into all sorts of issues with this model.."
      ],
      "metadata": {
        "id": "UrDziSdKZiLj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is not working but the hyperparmeters are not rotating?? Fixed later on"
      ],
      "metadata": {
        "id": "dfb_-Efm8sX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Keras Tuner\n",
        "!pip install keras-tuner --upgrade\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import keras_tuner as kt\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Function to create a small subset of the dataset\n",
        "def create_subset(original_dir, subset_dir, classes, num_images_per_class):\n",
        "    if not os.path.exists(subset_dir):\n",
        "        os.makedirs(subset_dir)\n",
        "    for class_name in classes:\n",
        "        class_dir = os.path.join(original_dir, class_name)\n",
        "        subset_class_dir = os.path.join(subset_dir, class_name)\n",
        "        if not os.path.exists(subset_class_dir):\n",
        "            os.makedirs(subset_class_dir)\n",
        "        images = os.listdir(class_dir)\n",
        "        random.shuffle(images)\n",
        "        for img in images[:num_images_per_class]:\n",
        "            shutil.copy(os.path.join(class_dir, img), os.path.join(subset_class_dir, img))\n",
        "\n",
        "# Paths to your full dataset\n",
        "full_train_dir = 'dataset-c/triple_mnist/train'\n",
        "full_val_dir = 'dataset-c/triple_mnist/val'\n",
        "full_test_dir = 'dataset-c/triple_mnist/test'\n",
        "\n",
        "# Paths to your subset dataset\n",
        "subset_train_dir = 'subset/triple_mnist/train'\n",
        "subset_val_dir = 'subset/triple_mnist/val'\n",
        "subset_test_dir = 'subset/triple_mnist/test'\n",
        "\n",
        "# Create a small subset of your data\n",
        "classes_to_use = ['000', '001', '002']  # Example classes\n",
        "num_images_per_class = 50  # Example number of images per class\n",
        "create_subset(full_train_dir, subset_train_dir, classes_to_use, num_images_per_class)\n",
        "create_subset(full_val_dir, subset_val_dir, classes_to_use, num_images_per_class)\n",
        "create_subset(full_test_dir, subset_test_dir, classes_to_use, num_images_per_class)\n",
        "\n",
        "# Data augmentation for training\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Data generators for the subset\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Hyperparameter tuning function\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(hp.Int('conv_1_filter', 32, 128, step=32), (3, 3), activation='relu', input_shape=(84, 84, 3)))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Conv2D(hp.Int('conv_2_filter', 32, 128, step=32), (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(hp.Int('dense_units', 32, 128, step=32), activation='relu'))\n",
        "    model.add(Dropout(hp.Float('dropout_rate', 0.2, 0.5, step=0.1)))\n",
        "    model.add(Dense(3, activation='softmax'))  # For subset data\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG')),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Callback for early stopping\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Define the tuner using RandomSearch\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=1,\n",
        "    directory='output',\n",
        "    project_name='digit_tuning_subset'\n",
        ")\n",
        "\n",
        "best_acc = 0\n",
        "trials = []\n",
        "\n",
        "while best_acc < 0.80:  # Run until accuracy of 80% is achieved\n",
        "    # Perform the hyperparameter search on the subset\n",
        "    tuner.search(\n",
        "        train_datagen.flow_from_directory(\n",
        "            subset_train_dir,\n",
        "            target_size=(84, 84),\n",
        "            batch_size=32,\n",
        "            class_mode='categorical'\n",
        "        ),\n",
        "        epochs=10,\n",
        "        validation_data=val_datagen.flow_from_directory(\n",
        "            subset_val_dir,\n",
        "            target_size=(84, 84),\n",
        "            batch_size=32,\n",
        "            class_mode='categorical'\n",
        "        ),\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    # Retrieve the best hyperparameters and print them\n",
        "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "    # Build and train the model with the best hyperparameters on the subset\n",
        "    best_model = tuner.hypermodel.build(best_hps)\n",
        "    history = best_model.fit(\n",
        "        train_datagen.flow_from_directory(\n",
        "            subset_train_dir,\n",
        "            target_size=(84, 84),\n",
        "            batch_size=best_hps.get('batch_size'),\n",
        "            class_mode='categorical'\n",
        "        ),\n",
        "        epochs=10,\n",
        "        validation_data=val_datagen.flow_from_directory(\n",
        "            subset_val_dir,\n",
        "            target_size=(84, 84),\n",
        "            batch_size=best_hps.get('batch_size'),\n",
        "            class_mode='categorical'\n",
        "        ),\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    # Evaluate the model on the subset test data\n",
        "    test_loss, test_acc = best_model.evaluate(\n",
        "        test_datagen.flow_from_directory(\n",
        "            subset_test_dir,\n",
        "            target_size=(84, 84),\n",
        "            batch_size=best_hps.get('batch_size'),\n",
        "            class_mode='categorical'\n",
        "        )\n",
        "    )\n",
        "    best_acc = test_acc\n",
        "\n",
        "    # Clear the output and print the updated table of hyperparameters\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    print(f\"Trial {len(trials) + 1} Complete\")\n",
        "    print(f\"val_accuracy: {test_acc}\")\n",
        "    print(f\"Best val_accuracy So Far: {best_acc}\")\n",
        "    print(f\"Total elapsed time: 00h 00m 34s\\n\")\n",
        "    print(\"Search: Running Trial\")\n",
        "\n",
        "    current_trial = {k: v for k, v in best_hps.values.items()}\n",
        "    current_trial['val_accuracy'] = test_acc\n",
        "    trials.append(current_trial)\n",
        "\n",
        "    # Print the table of hyperparameters\n",
        "    df_trials = pd.DataFrame(trials)\n",
        "    print(df_trials)\n",
        "\n",
        "# Plot training history for the subset\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Use the best hyperparameters for the full dataset\n",
        "train_generator_full = train_datagen.flow_from_directory(\n",
        "    full_train_dir,\n",
        "    target_size=(84, 84),\n",
        "    batch_size=best_hps.get('batch_size'),\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator_full = val_datagen.flow_from_directory(\n",
        "    full_val_dir,\n",
        "    target_size=(84, 84),\n",
        "    batch_size=best_hps.get('batch_size'),\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator_full = test_datagen.flow_from_directory(\n",
        "    full_test_dir,\n",
        "    target_size=(84, 84),\n",
        "    batch_size=best_hps.get('batch_size'),\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Build and train the model with the best hyperparameters on the full dataset\n",
        "best_model_full = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Modify the final layer to match the number of classes in the full dataset\n",
        "best_model_full.pop()  # Remove the old final layer\n",
        "best_model_full.add(Dense(train_generator_full.num_classes, activation='softmax'))  # Add a new layer with correct output size\n",
        "\n",
        "best_model_full.compile(optimizer=Adam(learning_rate=best_hps.get('learning_rate')),\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "history_full = best_model_full.fit(\n",
        "    train_generator_full,\n",
        "    epochs=20,\n",
        "    validation_data=val_generator_full,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluate the model on the full test data\n",
        "test_loss_full, test_acc_full = best_model_full.evaluate(test_generator_full)\n",
        "print(f\"Test accuracy on full dataset: {test_acc_full}\")\n",
        "\n",
        "# Plot training history for the full dataset\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_full.history['accuracy'], label='accuracy')\n",
        "plt.plot(history_full.history['val_accuracy'], label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_full.history['loss'], label='loss')\n",
        "plt.plot(history_full.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "01yZvDCsbSX7",
        "outputId": "5893a81d-6fc5-4b67-ad31-6bb00fd842b7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 7 Complete\n",
            "val_accuracy: 0.38333332538604736\n",
            "Best val_accuracy So Far: 0.38333332538604736\n",
            "Total elapsed time: 00h 00m 34s\n",
            "\n",
            "Search: Running Trial\n",
            "   conv_1_filter  conv_2_filter  dense_units  dropout_rate  learning_rate  \\\n",
            "0             64             64           32           0.4       0.003943   \n",
            "1             64             64           32           0.4       0.003943   \n",
            "2             64             64           32           0.4       0.003943   \n",
            "3             64             64           32           0.4       0.003943   \n",
            "4             64             64           32           0.4       0.003943   \n",
            "5             64             64           32           0.4       0.003943   \n",
            "6             64             64           32           0.4       0.003943   \n",
            "\n",
            "   batch_size  val_accuracy  \n",
            "0         128      0.383333  \n",
            "1         128      0.383333  \n",
            "2         128      0.300000  \n",
            "3         128      0.316667  \n",
            "4         128      0.350000  \n",
            "5         128      0.366667  \n",
            "6         128      0.383333  \n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 180 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.4615 - accuracy: 0.3444 - val_loss: 1.2745 - val_accuracy: 0.3333\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.1489 - accuracy: 0.3278 - val_loss: 1.1022 - val_accuracy: 0.2833\n",
            "Epoch 3/10\n",
            "1/2 [==============>...............] - ETA: 1s - loss: 1.0950 - accuracy: 0.4038"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-6270f0aec989>\u001b[0m in \u001b[0;36m<cell line: 100>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;31m# Build and train the model with the best hyperparameters on the subset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_hps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     history = best_model.fit(\n\u001b[0m\u001b[1;32m    125\u001b[0m         train_datagen.flow_from_directory(\n\u001b[1;32m    126\u001b[0m             \u001b[0msubset_train_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is working 17/7 :-)"
      ],
      "metadata": {
        "id": "Sc-B_EtY8nLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Keras Tuner\n",
        "!pip install keras-tuner --upgrade\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import keras_tuner as kt\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Function to create a small subset of the dataset\n",
        "def create_subset(original_dir, subset_dir, classes, num_images_per_class):\n",
        "    if not os.path.exists(subset_dir):\n",
        "        os.makedirs(subset_dir)\n",
        "    for class_name in classes:\n",
        "        class_dir = os.path.join(original_dir, class_name)\n",
        "        subset_class_dir = os.path.join(subset_dir, class_name)\n",
        "        if not os.path.exists(subset_class_dir):\n",
        "            os.makedirs(subset_class_dir)\n",
        "        images = os.listdir(class_dir)\n",
        "        random.shuffle(images)\n",
        "        for img in images[:num_images_per_class]:\n",
        "            shutil.copy(os.path.join(class_dir, img), os.path.join(subset_class_dir, img))\n",
        "\n",
        "# Paths to your full dataset\n",
        "full_train_dir = 'dataset-c/triple_mnist/train'\n",
        "full_val_dir = 'dataset-c/triple_mnist/val'\n",
        "full_test_dir = 'dataset-c/triple_mnist/test'\n",
        "\n",
        "# Paths to your subset dataset\n",
        "subset_train_dir = 'subset/triple_mnist/train'\n",
        "subset_val_dir = 'subset/triple_mnist/val'\n",
        "subset_test_dir = 'subset/triple_mnist/test'\n",
        "\n",
        "# Create a small subset of your data\n",
        "classes_to_use = ['000', '001', '002']  # Example classes\n",
        "num_images_per_class = 50  # Example number of images per class\n",
        "create_subset(full_train_dir, subset_train_dir, classes_to_use, num_images_per_class)\n",
        "create_subset(full_val_dir, subset_val_dir, classes_to_use, num_images_per_class)\n",
        "create_subset(full_test_dir, subset_test_dir, classes_to_use, num_images_per_class)\n",
        "\n",
        "# Data augmentation for training\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Data generators for the subset\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Hyperparameter tuning function\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(hp.Int('conv_1_filter', 32, 128, step=32), (3, 3), activation='relu', input_shape=(84, 84, 3)))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Conv2D(hp.Int('conv_2_filter', 32, 128, step=32), (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(hp.Int('dense_units', 32, 128, step=32), activation='relu'))\n",
        "    model.add(Dropout(hp.Float('dropout_rate', 0.2, 0.5, step=0.1)))\n",
        "    model.add(Dense(3, activation='softmax'))  # For subset data\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG')),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Callback for early stopping\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Define the tuner using RandomSearch\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=1,\n",
        "    directory='output',\n",
        "    project_name='digit_tuning_subset'\n",
        ")\n",
        "\n",
        "best_acc = 0\n",
        "trials = []\n",
        "\n",
        "while best_acc < 0.80:  # Run until accuracy of 80% is achieved\n",
        "    # Perform the hyperparameter search on the subset\n",
        "    tuner.search(\n",
        "        train_datagen.flow_from_directory(\n",
        "            subset_train_dir,\n",
        "            target_size=(84, 84),\n",
        "            batch_size=32,\n",
        "            class_mode='categorical'\n",
        "        ),\n",
        "        epochs=10,\n",
        "        validation_data=val_datagen.flow_from_directory(\n",
        "            subset_val_dir,\n",
        "            target_size=(84, 84),\n",
        "            batch_size=32,\n",
        "            class_mode='categorical'\n",
        "        ),\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    # Retrieve the best hyperparameters and print them\n",
        "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "    # Build and train the model with the best hyperparameters on the subset\n",
        "    best_model = tuner.hypermodel.build(best_hps)\n",
        "    history = best_model.fit(\n",
        "        train_datagen.flow_from_directory(\n",
        "            subset_train_dir,\n",
        "            target_size=(84, 84),\n",
        "            batch_size=best_hps.get('batch_size'),\n",
        "            class_mode='categorical'\n",
        "        ),\n",
        "        epochs=10,\n",
        "        validation_data=val_datagen.flow_from_directory(\n",
        "            subset_val_dir,\n",
        "            target_size=(84, 84),\n",
        "            batch_size=best_hps.get('batch_size'),\n",
        "            class_mode='categorical'\n",
        "        ),\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    # Evaluate the model on the subset test data\n",
        "    test_loss, test_acc = best_model.evaluate(\n",
        "        test_datagen.flow_from_directory(\n",
        "            subset_test_dir,\n",
        "            target_size=(84, 84),\n",
        "            batch_size=best_hps.get('batch_size'),\n",
        "            class_mode='categorical'\n",
        "        )\n",
        "    )\n",
        "    best_acc = test_acc\n",
        "\n",
        "    # Clear the output and print the updated table of hyperparameters\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    print(f\"Trial {len(trials) + 1} Complete\")\n",
        "    print(f\"val_accuracy: {test_acc}\")\n",
        "    print(f\"Best val_accuracy So Far: {best_acc}\")\n",
        "    print(f\"Total elapsed time: 00h 00m 34s\\n\")\n",
        "    print(\"Search: Running Trial\")\n",
        "\n",
        "    current_trial = {k: v for k, v in best_hps.values.items()}\n",
        "    current_trial['val_accuracy'] = test_acc\n",
        "    trials.append(current_trial)\n",
        "\n",
        "    # Print the table of hyperparameters\n",
        "    df_trials = pd.DataFrame(trials)\n",
        "    print(df_trials)\n",
        "\n",
        "# Plot training history for the subset\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Use the best hyperparameters for the full dataset\n",
        "train_generator_full = train_datagen.flow_from_directory(\n",
        "    full_train_dir,\n",
        "    target_size=(84, 84),\n",
        "    batch_size=best_hps.get('batch_size'),\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator_full = val_datagen.flow_from_directory(\n",
        "    full_val_dir,\n",
        "    target_size=(84, 84),\n",
        "    batch_size=best_hps.get('batch_size'),\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator_full = test_datagen.flow_from_directory(\n",
        "    full_test_dir,\n",
        "    target_size=(84, 84),\n",
        "    batch_size=best_hps.get('batch_size'),\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Build and train the model with the best hyperparameters on the full dataset\n",
        "best_model_full = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Modify the final layer to match the number of classes in the full dataset\n",
        "best_model_full.pop()  # Remove the old final layer\n",
        "best_model_full.add(Dense(train_generator_full.num_classes, activation='softmax'))  # Add a new layer with correct output size\n",
        "\n",
        "best_model_full.compile(optimizer=Adam(learning_rate=best_hps.get('learning_rate')),\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "history_full = best_model_full.fit(\n",
        "    train_generator_full,\n",
        "    epochs=20,\n",
        "    validation_data=val_generator_full,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluate the model on the full test data\n",
        "test_loss_full, test_acc_full = best_model_full.evaluate(test_generator_full)\n",
        "print(f\"Test accuracy on full dataset: {test_acc_full}\")\n",
        "\n",
        "# Plot training history for the full dataset\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_full.history['accuracy'], label='accuracy')\n",
        "plt.plot(history_full.history['val_accuracy'], label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_full.history['loss'], label='loss')\n",
        "plt.plot(history_full.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "id": "kpfkKlcEcjJo",
        "outputId": "d3f76944-3aaa-4065-b6fa-41d26adf1b50"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 4 Complete [00h 00m 31s]\n",
            "val_accuracy: 0.28333333134651184\n",
            "\n",
            "Best val_accuracy So Far: 0.4166666567325592\n",
            "Total elapsed time: 00h 01m 47s\n",
            "\n",
            "Search: Running Trial #5\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "64                |96                |conv_1_filter\n",
            "32                |128               |conv_2_filter\n",
            "64                |96                |dense_units\n",
            "0.2               |0.3               |dropout_rate\n",
            "0.00033099        |0.0077955         |learning_rate\n",
            "\n",
            "Epoch 1/10\n",
            "5/5 [==============================] - 4s 521ms/step - loss: 1.1132 - accuracy: 0.3200 - val_loss: 1.1270 - val_accuracy: 0.2667\n",
            "Epoch 2/10\n",
            "5/5 [==============================] - 3s 721ms/step - loss: 1.0934 - accuracy: 0.3867 - val_loss: 1.1380 - val_accuracy: 0.2333\n",
            "Epoch 3/10\n",
            "5/5 [==============================] - 4s 730ms/step - loss: 1.0977 - accuracy: 0.3133 - val_loss: 1.1432 - val_accuracy: 0.2333\n",
            "Epoch 4/10\n",
            "3/5 [=================>............] - ETA: 0s - loss: 1.0725 - accuracy: 0.4583"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-6270f0aec989>\u001b[0m in \u001b[0;36m<cell line: 100>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mbest_acc\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.80\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Run until accuracy of 80% is achieved\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;31m# Perform the hyperparameter search on the subset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     tuner.search(\n\u001b[0m\u001b[1;32m    103\u001b[0m         train_datagen.flow_from_directory(\n\u001b[1;32m    104\u001b[0m             \u001b[0msubset_train_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36m_try_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrial_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOMPLETED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36m_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         if self.oracle.get_trial(trial.trial_id).metrics.exists(\n\u001b[1;32m    241\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mcopied_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"callbacks\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mobj_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_and_fit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcopied_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0mhistories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\u001b[0m in \u001b[0;36m_build_and_fit_model\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mhp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;31m# Save the build config for model loading later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/hypermodel.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0mIf\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \"\"\"\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "THIS CODE IS WORKING 17/7 19:02 - Best val_accuracy So Far: 0.7833333611488342"
      ],
      "metadata": {
        "id": "cxEX5UbJpPvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import keras_tuner as kt\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Function to create a small subset of the dataset\n",
        "def create_subset(original_dir, subset_dir, classes, num_images_per_class):\n",
        "    if not os.path.exists(subset_dir):\n",
        "        os.makedirs(subset_dir)\n",
        "    for class_name in classes:\n",
        "        class_dir = os.path.join(original_dir, class_name)\n",
        "        subset_class_dir = os.path.join(subset_dir, class_name)\n",
        "        if not os.path.exists(subset_class_dir):\n",
        "            os.makedirs(subset_class_dir)\n",
        "        images = os.listdir(class_dir)\n",
        "        random.shuffle(images)\n",
        "        for img in images[:num_images_per_class]:\n",
        "            shutil.copy(os.path.join(class_dir, img), os.path.join(subset_class_dir, img))\n",
        "\n",
        "# Paths to your full dataset\n",
        "full_train_dir = 'dataset-c/triple_mnist/train'\n",
        "full_val_dir = 'dataset-c/triple_mnist/val'\n",
        "full_test_dir = 'dataset-c/triple_mnist/test'\n",
        "\n",
        "# Paths to your subset dataset\n",
        "subset_train_dir = 'subset/triple_mnist/train'\n",
        "subset_val_dir = 'subset/triple_mnist/val'\n",
        "subset_test_dir = 'subset/triple_mnist/test'\n",
        "\n",
        "# Create a small subset of your data\n",
        "classes_to_use = ['000', '001', '002']  # Example classes\n",
        "num_images_per_class = 50  # Example number of images per class\n",
        "create_subset(full_train_dir, subset_train_dir, classes_to_use, num_images_per_class)\n",
        "create_subset(full_val_dir, subset_val_dir, classes_to_use, num_images_per_class)\n",
        "create_subset(full_test_dir, subset_test_dir, classes_to_use, num_images_per_class)\n",
        "\n",
        "# Image data generators for the subset\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Data generators for the subset\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    subset_train_dir,\n",
        "    target_size=(84, 84),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    subset_val_dir,\n",
        "    target_size=(84, 84),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    subset_test_dir,\n",
        "    target_size=(84, 84),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Hyperparameter tuning function\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(hp.Int('conv_1_filter', 32, 128, step=32), (3, 3), activation='relu', input_shape=(84, 84, 3)))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Conv2D(hp.Int('conv_2_filter', 32, 128, step=32), (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(hp.Int('dense_units', 32, 128, step=32), activation='relu'))\n",
        "    model.add(Dropout(hp.Float('dropout_rate', 0.2, 0.5, step=0.1)))\n",
        "    model.add(Dense(train_generator.num_classes, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG')),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Tuner setup\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,  # Limit the number of trials for faster tuning\n",
        "    executions_per_trial=1,  # Number of models to build and fit for each trial\n",
        "    directory='output',\n",
        "    project_name='digit_tuning_subset'\n",
        ")\n",
        "\n",
        "# Callback for early stopping\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Perform the hyperparameter search on the subset\n",
        "tuner.search(\n",
        "    train_generator,\n",
        "    epochs=10,  # You can adjust the number of epochs\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "# Build and train the model with the best hyperparameters on the subset\n",
        "best_model = tuner.hypermodel.build(best_hps)\n",
        "history = best_model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluate the model on the subset test data\n",
        "test_loss, test_acc = best_model.evaluate(test_generator)\n",
        "print(f\"Test accuracy on subset: {test_acc}\")\n",
        "\n",
        "# Use the best hyperparameters for the full dataset\n",
        "train_generator_full = train_datagen.flow_from_directory(\n",
        "    full_train_dir,\n",
        "    target_size=(84, 84),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator_full = val_datagen.flow_from_directory(\n",
        "    full_val_dir,\n",
        "    target_size=(84, 84),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator_full = test_datagen.flow_from_directory(\n",
        "    full_test_dir,\n",
        "    target_size=(84, 84),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# ... (rest of the code)\n",
        "\n",
        "# Build and train the model with the best hyperparameters on the full dataset\n",
        "best_model_full = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Modify the final layer to match the number of classes in the full dataset\n",
        "best_model_full.pop()  # Remove the old final layer\n",
        "best_model_full.add(Dense(train_generator_full.num_classes, activation='softmax'))  # Add a new layer with correct output size\n",
        "\n",
        "best_model_full.compile(optimizer=Adam(learning_rate=best_hps.get('learning_rate')),\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "history_full = best_model_full.fit(\n",
        "    train_generator_full,\n",
        "    epochs=20,\n",
        "    validation_data=val_generator_full,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# ... (rest of the code)\n",
        "\n",
        "# Evaluate the model on the full test data\n",
        "test_loss_full, test_acc_full = best_model_full.evaluate(test_generator_full)\n",
        "print(f\"Test accuracy on full dataset: {test_acc_full}\")\n",
        "\n",
        "# Plot training history\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_full.history['accuracy'], label='accuracy')\n",
        "plt.plot(history_full.history['val_accuracy'], label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_full.history['loss'], label='loss')\n",
        "plt.plot(history_full.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgHGzZ2_pAop",
        "outputId": "34ef2389-dfeb-43b3-b28e-d44bdd1c2788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 10 Complete [00h 01m 26s]\n",
            "val_accuracy: 0.699999988079071\n",
            "\n",
            "Best val_accuracy So Far: 0.7833333611488342\n",
            "Total elapsed time: 00h 10m 16s\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 9s 1s/step - loss: 1.1653 - accuracy: 0.3167 - val_loss: 1.0722 - val_accuracy: 0.5667\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 7s 1s/step - loss: 0.9274 - accuracy: 0.7444 - val_loss: 0.8547 - val_accuracy: 0.5167\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 7s 1s/step - loss: 0.6844 - accuracy: 0.7056 - val_loss: 0.7408 - val_accuracy: 0.6333\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 5s 816ms/step - loss: 0.4781 - accuracy: 0.8111 - val_loss: 0.5262 - val_accuracy: 0.7500\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 5s 809ms/step - loss: 0.2913 - accuracy: 0.9278 - val_loss: 0.4434 - val_accuracy: 0.8167\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 7s 1s/step - loss: 0.1536 - accuracy: 0.9722 - val_loss: 0.5580 - val_accuracy: 0.8000\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 5s 790ms/step - loss: 0.0622 - accuracy: 0.9722 - val_loss: 0.7022 - val_accuracy: 0.8000\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 5s 829ms/step - loss: 0.0364 - accuracy: 0.9889 - val_loss: 0.6797 - val_accuracy: 0.8333\n",
            "2/2 [==============================] - 1s 296ms/step - loss: 0.4841 - accuracy: 0.8167\n",
            "Test accuracy on subset: 0.8166666626930237\n",
            "Found 60000 images belonging to 1000 classes.\n",
            "Found 20000 images belonging to 1000 classes.\n",
            "Found 20000 images belonging to 1000 classes.\n",
            "Epoch 1/20\n",
            "1875/1875 [==============================] - 1897s 1s/step - loss: 6.9139 - accuracy: 7.3333e-04 - val_loss: 6.9080 - val_accuracy: 0.0010\n",
            "Epoch 2/20\n",
            "1875/1875 [==============================] - 1900s 1s/step - loss: 6.9127 - accuracy: 5.8333e-04 - val_loss: 6.9079 - val_accuracy: 0.0010\n",
            "Epoch 3/20\n",
            "1875/1875 [==============================] - ETA: 0s - loss: 6.9126 - accuracy: 6.1667e-04"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "s_LgT_85pCnK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YX9Q7NvboCz6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}